Gaussian process models for
atomic and electronic
interactions

Aldo Glielmo

Department of Physics
King’s College London

This dissertation is submitted for
the degree of Doctor of Philosophy

January 2019

2

Contents
List of Figures

13

1 Introduction

15

2 Gaussian processes for energies and forces
2.1 Introduction . . . . . . . . . . . . . . . . .
2.2 Gaussian process regression . . . . . . . .
2.3 Vectorial Gaussian process regression . . .
2.4 Local energy for global energies and forces
2.5 Prior knowledge and kernel functions . . .
2.6 Summary . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.

20
20
20
24
25
28
32

.
.
.
.
.

34
34
35
41
46
49

.
.
.
.
.
.

51
51
52
54
57
65
69

5 Selecting the optimal kernel
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72
72

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

3 n-body kernels
3.1 Introduction . . . . . . . . . . . . . . . . . . . . .
3.2 Building n-body kernels I: SO(3) integration . .
3.3 Building n-body kernels II: n-body feature spaces
3.4 Tests on real systems . . . . . . . . . . . . . . . .
3.5 Summary . . . . . . . . . . . . . . . . . . . . . .
4 Covariant kernels
4.1 Introduction . . . . . . . .
4.2 Kernel covariance . . . . .
4.3 Covariant integration . . .
4.4 Building covariant Kernels
4.5 Tests on real materials . .
4.6 Summary . . . . . . . . .

.
.
.
.
.
.

.
.
.
.
.
.

3

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

5.2
5.3
5.4
5.5

Theory of Bayesian model selection
Model selecting n-body kernels . .
The advantage of low order models
Summary . . . . . . . . . . . . . .

6 Speeding up low n models
6.1 Introduction . . . . . . .
6.2 “Mapped” force fields . .
6.3 Tests on real materials .
6.4 Summary . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

7 Gaussian process wave functions
7.1 Introduction . . . . . . . . . . . . . . . . . .
7.2 Hubbard model and Variational Monte Carlo
7.3 Gaussian process wave functions . . . . . . .
7.4 Tests on the Hubbard model . . . . . . . . .
7.5 Future extensions . . . . . . . . . . . . . . .
7.6 Summary . . . . . . . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

74
76
80
82

.
.
.
.

83
83
84
87
90

.
.
.
.
.
.

92
92
93
96
100
105
106

8 Conclusions

107

A Appendix
A.1 Derivation of the predictive distribution . . . . . . . . . . .
A.2 The predictive mean provides optimal predictions . . . . . .
A.3 Kernels for multiple chemical species . . . . . . . . . . . . .
A.4 Kernel order by explicit differentiation . . . . . . . . . . . .
A.5 A one dimensional n0 -body model . . . . . . . . . . . . . . .
A.6 Databases details . . . . . . . . . . . . . . . . . . . . . . . .
A.7 Covariant integration of 2-body kernels . . . . . . . . . . . .
A.8 Proof that 2-body covariant kernels give rise to central forces
A.9 Covariant integration of 3-body kernels . . . . . . . . . . . .
A.10 One dimensional n-body toy model . . . . . . . . . . . . . .
A.11 Mapping the predictive variance . . . . . . . . . . . . . . . .
A.12 Quadratic kernel scaling for local updates . . . . . . . . . . .

108
108
109
109
111
112
113
113
116
118
124
125
125

Bibliography

.
.
.
.
.
.
.
.
.
.
.
.

127

4

List of Figures
2.1
2.2

Pictorial view of GP learning . . . . . . . . . . . . . . . . . . . .
Smoothness induced by different kernel functions . . . . . . . . .

23
29

3.1
3.2
3.3
3.4
3.5
3.6

Error and kernel orders . . . . . . . . . . . . . . . . . . . . .
Analytically and numerically obtained kernels compared . .
Haar symmetrised and directly symmetric kernels compared
Unique and non-unique interactions compared . . . . . . . .
Learning curves for Ni, Fe, C and Si . . . . . . . . . . . . .
Converged error achieved by a given kernel as a function of
kernel’s order . . . . . . . . . . . . . . . . . . . . . . . . . .

. . .
. . .
. . .
. . .
. . .
the
. . .

37
39
42
43
47

4.1
4.2
4.3
4.4
4.5
4.6
4.7

Example of covariance in 1D . . . . . . . . . . .
Covariance in 1D learning curves . . . . . . . .
Covariance in 2D learning curves . . . . . . . .
Covariance in crystalline nickel . . . . . . . . .
Density of relative error for crystalline nickel . .
Two and three body kernels compared in iron .
Two and three body kernels compared in silicon

.
.
.
.
.
.
.

58
59
60
63
65
67
69

5.1
5.2
5.3
5.4
5.5
5.6

Cartoon of Occam’s razor . . . . . . . . . . . . . . . . . . .
Cartoon of model selection . . . . . . . . . . . . . . . . . . .
Marginal likelihood as a function of training set size for a toy
Model selected order for a toy model . . . . . . . . . . . . .
Model selection for nickel systems . . . . . . . . . . . . . . .
Cartoon of configuration clusters . . . . . . . . . . . . . . .

6.1
6.2
6.3

Convergence of mapping with number of gridpoints . . . . . . . .
Computational speedup of MFFs . . . . . . . . . . . . . . . . . .
Learned energy profile of amorphous silicon . . . . . . . . . . . .

5

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

. . .
. . .
model
. . .
. . .
. . .

48

73
75
76
77
78
81
87
88
89

7.1
7.2
7.3

Extrapolative power of GP-WFs . . . . . . . . . . . . . . . . . . .
Energy per site as a function of the interaction strength U . The
GP-WF model ψ̂ was trained on 8-site data with a k4 kernel. . . .
Variational optimisation of GP wave function . . . . . . . . . . .

6

102
103
104

Summary
Data driven models for atomic energies and forces
Aldo Glielmo
King’s College London
When a brittle material is loaded to the limit of its strength, it fails by nucleation and propagation of a crack. The conditions for crack propagation are
created by the concentration of a long-range stress field at an atomically sharp
crack tip, creating a complex and strongly coupled multiscale system.

7

Preface
This dissertation describes work done between October 2015 and November
2018 at the department of physics of King’s College London under the supervision of Professor Alessandro De Vita.
I have contributed to eight publications during the course of my PhD:
• G. Csányi, G. Moras, J. R. Kermode, M. C. Payne, A. Mainwood and
A. de Vita, Multiscale Modeling of Defects in Semiconductors: A Novel
Molecular Dynamics Scheme. In D. A. Drabold and S. K. Estreicher,
editors, Theory of Defects in Semiconductors (Springer, 2007)
• G. Csányi, S. Winfield, J. R. Kermode, A. Comisso, A. de Vita, N.
Bernstein and M. C. Payne, Expressive Programming for Computational
Physics in Fortran 95+, IoP Comput. Phys. Newsletter, Spring 2007
• J. R. Kermode, T. Albaret, D. Sherman, N. Bernstein, P. Gumbsch, M.
C. Payne, G. Csányi and A. de Vita, Low speed fracture instabilities in
a brittle crystal, Submitted to Nature
• A. Glielmo, P. Sollich and A. de Vita, Accurate interatomic force fields
via machine learning with covariant kernels, Physical Review B
• A. Glielmo, C. Zeni and A. de Vita, Efficient nonparametric n-body force
fields from machine learning, Physical Review B
• A. Glielmo, C. Zeni and A. de Vita, Efficient nonparametric n-body force
fields from machine learning, Physical Review B
This dissertation is my own work and contains nothing which is the outcome
of work done in collaboration with others, except as specified in the text and
acknowledgements. It has not been submitted in whole or in part for any

8

degree or diploma at this or any other university, and does not exceed 60 000
words, including tables, footnotes, bibliography and appendices, but excluding
photographs and diagrams.

Aldo Glielmo
January 2019

9

Acknowledgements
I would like to thank my supervisor Mike Payne for.

10

Contents
List of Figures

13

1 Introduction

15

2 Gaussian processes for energies and forces
2.1 Introduction . . . . . . . . . . . . . . . . .
2.2 Gaussian process regression . . . . . . . .
2.3 Vectorial Gaussian process regression . . .
2.4 Local energy for global energies and forces
2.5 Prior knowledge and kernel functions . . .
2.6 Summary . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.

20
20
20
24
25
28
32

.
.
.
.
.

34
34
35
41
46
49

.
.
.
.
.
.

51
51
52
54
57
65
69

5 Selecting the optimal kernel
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72
72

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

3 n-body kernels
3.1 Introduction . . . . . . . . . . . . . . . . . . . . .
3.2 Building n-body kernels I: SO(3) integration . .
3.3 Building n-body kernels II: n-body feature spaces
3.4 Tests on real systems . . . . . . . . . . . . . . . .
3.5 Summary . . . . . . . . . . . . . . . . . . . . . .
4 Covariant kernels
4.1 Introduction . . . . . . . .
4.2 Kernel covariance . . . . .
4.3 Covariant integration . . .
4.4 Building covariant Kernels
4.5 Tests on real materials . .
4.6 Summary . . . . . . . . .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

CONTENTS
5.2
5.3
5.4
5.5

12

Theory of Bayesian model selection
Model selecting n-body kernels . .
The advantage of low order models
Summary . . . . . . . . . . . . . .

6 Speeding up low n models
6.1 Introduction . . . . . . .
6.2 “Mapped” force fields . .
6.3 Tests on real materials .
6.4 Summary . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

7 Gaussian process wave functions
7.1 Introduction . . . . . . . . . . . . . . . . . .
7.2 Hubbard model and Variational Monte Carlo
7.3 Gaussian process wave functions . . . . . . .
7.4 Tests on the Hubbard model . . . . . . . . .
7.5 Future extensions . . . . . . . . . . . . . . .
7.6 Summary . . . . . . . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.

.
.
.
.

74
76
80
82

.
.
.
.

83
83
84
87
90

.
.
.
.
.
.

92
92
93
96
100
105
106

8 Conclusions

107

A Appendix
A.1 Derivation of the predictive distribution . . . . . . . . . . .
A.2 The predictive mean provides optimal predictions . . . . . .
A.3 Kernels for multiple chemical species . . . . . . . . . . . . .
A.4 Kernel order by explicit differentiation . . . . . . . . . . . .
A.5 A one dimensional n0 -body model . . . . . . . . . . . . . . .
A.6 Databases details . . . . . . . . . . . . . . . . . . . . . . . .
A.7 Covariant integration of 2-body kernels . . . . . . . . . . . .
A.8 Proof that 2-body covariant kernels give rise to central forces
A.9 Covariant integration of 3-body kernels . . . . . . . . . . . .
A.10 One dimensional n-body toy model . . . . . . . . . . . . . .
A.11 Mapping the predictive variance . . . . . . . . . . . . . . . .
A.12 Quadratic kernel scaling for local updates . . . . . . . . . . .

108
108
109
109
111
112
113
113
116
118
124
125
125

Bibliography

.
.
.
.
.
.
.
.
.
.
.
.

127

List of Figures

2.1
2.2

Pictorial view of GP learning . . . . . . . . . . . . . . . . . . . .
Smoothness induced by different kernel functions . . . . . . . . .

23
29

3.1
3.2
3.3
3.4
3.5
3.6

Error and kernel orders . . . . . . . . . . . . . . . . . . . . .
Analytically and numerically obtained kernels compared . .
Haar symmetrised and directly symmetric kernels compared
Unique and non-unique interactions compared . . . . . . . .
Learning curves for Ni, Fe, C and Si . . . . . . . . . . . . .
Converged error achieved by a given kernel as a function of
kernel’s order . . . . . . . . . . . . . . . . . . . . . . . . . .

. . .
. . .
. . .
. . .
. . .
the
. . .

37
39
42
43
47

4.1
4.2
4.3
4.4
4.5
4.6
4.7

Example of covariance in 1D . . . . . . . . . . .
Covariance in 1D learning curves . . . . . . . .
Covariance in 2D learning curves . . . . . . . .
Covariance in crystalline nickel . . . . . . . . .
Density of relative error for crystalline nickel . .
Two and three body kernels compared in iron .
Two and three body kernels compared in silicon

.
.
.
.
.
.
.

58
59
60
63
65
67
69

5.1
5.2
5.3
5.4
5.5
5.6

Cartoon of Occam’s razor . . . . . . . . . . . . . . . . . . .
Cartoon of model selection . . . . . . . . . . . . . . . . . . .
Marginal likelihood as a function of training set size for a toy
Model selected order for a toy model . . . . . . . . . . . . .
Model selection for nickel systems . . . . . . . . . . . . . . .
Cartoon of configuration clusters . . . . . . . . . . . . . . .

6.1
6.2

Convergence of mapping with number of gridpoints . . . . . . . .
Computational speedup of MFFs . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

. . .
. . .
model
. . .
. . .
. . .

48

73
75
76
77
78
81
87
88

List of Figures

14

6.3

Learned energy profile of amorphous silicon . . . . . . . . . . . .

89

7.1
7.2

Extrapolative power of GP-WFs . . . . . . . . . . . . . . . . . . .
Energy per site as a function of the interaction strength U . The
GP-WF model ψ̂ was trained on 8-site data with a k4 kernel. . . .
Variational optimisation of GP wave function . . . . . . . . . . .

102

7.3

103
104

Chapter 1

Introduction
Development of quantum theory of matter
Discovery of laws
Discovery of faster computation

Quantum simulation and its limitations
The “data revolution”
Example of astonishing achievements
Common problem: curse of simensionality

Use of data for quantum simulation
Thesis objectives

A

tomistic simulation

The beginigging of a new paradigm for building fast classical models
which exploit heavy quantum calculations In the remainder of this
introductory section we briefly review the relative strengths and weaknesses of
standard parametrized (P-FFs) and machine learning force fields (ML-FFs).

Introduction
We then consider how accurate P-FFs are hard to develop but eventually
fully exploit useful knowledge on the systems, while GP-based ML-FFs offer a
general mathematical framework for handling training and validation, but are
significantly slower (Section I A). These shortcomings motivate an analysis
of how prior knowledge such as symmetry has been so far incorporated in
GP kernels (Section I B) and points to features still missing in ML kernels,
which are commonplace in the more standard, highly efficient P-FFs based
on truncated n-body expansions (Section I C). This suggests the possibility
of defining a series of n-body GP kernels (Section II B), providing a scheme
to construct them (Section II C and D) and, after the best value of n for the
given target system has been identified with appropriate testing (Section II
E), exploiting their dimensionally-reduced feature spaces to massively boost
the execution speed of force prediction (Section III).

Parametrized and machine learning force fields
Producing accurate and fully transferable force fields is a remarkably difficult
task. The traditional way to do this involves adjusting the parameters of
carefully chosen analytic functions in the hope of matching extended reference
data sets obtained from experiments or quantum calculations [97, 101]. The
descriptive restrictiveness of the parametric functions used is both a drawback
and a strength of this methodology. The main difficulty is that developing
a good parametric function requires a great deal of chemical intuition and
patient effort, guided by trial and error steps with no guarantee of success
[24]. However, for systems and processes in which the approach is fruitful, the
development effort is amply rewarded by the opportunity to provide extremely
fast and accurate force models [28, 73, 81, 105]. The identified functional forms
will in these cases contain valuable knowledge on the target system, encoded in
a compact formulation that still accurately captures the relevant physics. Such
knowledge is furthermore often transferable to novel (while similar) systems as
a “prior” piece of information, i.e., it constitutes a good working hypothesis on
how these systems will behave. When QM data on the novel system become
available, this can be simply used to fine-tune the parameters of the functional
form to a new set of best-fit values that maximise prediction accuracy.
Following a different approach, “nonparametric” ML force fields can be
constructed, whose dependence on the atomic position is not constrained to a

16

Introduction
particular analytic form. An implementation and tests exploring the feasibility
of ML to describe atomic interactions can be found, e.g., in pioneering work
by Skinner and Broughton [93] that proposed using ML models to reproduce
first-principles potential energy surfaces. More recent works implementing this
general idea have been based on Neural Networks [13], Gaussian Process (GP)
regression [9] or linear regression on properly defined bases [? ]. Current
work aims at making these learning algorithms both faster and more accurate
[22, 39, 45, 65, 77, 100].
As processing power and data communication bandwidth increase, and the
cost of data storage decreases, modeling based on ML and direct inference
promises to become an increasingly attractive option, compared with more
traditional classical force field approaches. However, although ML schemes
are general and have been shown to be remarkably accurate interpolators in
specific systems, so far they have not become as widespread as it might have
been expected. This is mainly because “standard” classical potentials are still
orders of magnitude faster than their ML counterpart [19]. Moreover, ML-FFs
also involve a more complex mathematical and algorithmic machinery than the
traditional compact functional forms of P-FFs, whose arguments are physically
descriptive features that remain easier to visualize and interpret.

Prior knowledge and GP kernels
These shortcomings provide motivation for the present work. The high computational cost of many ML models is a direct consequence of the general
inverse relation between the sought flexibility and the measured speed of any
algorithm capable of learning. Highly flexible ML algorithms by definition
assume very little or no prior knowledge of the target systems. In a Bayesian
context, this involves using a general prior kernel, typically aspiring to preserve the full universal approximator properties of e.g., the square exponential
kernel [18, 110]. The price of such a kernel choice is that the ML algorithm
will require large training databases [61], slowing down computations as the
prediction time grows linearly with the database size.
Large database sizes are not, however, unavoidable, and any data-intensive
and fully flexible scheme to potential energy fitting is suboptimal by definition,
as it exploits no prior knowledge of the system. This completely “agnostic”
approach is at odds with the general lesson from classical potential develop-

17

Introduction
ment, indicating that it is essential for efficiency to incorporate in the force
prediction model as much prior knowledge of the target system as can be made
available. In this respect, GP kernels can be tailored to bring some form of
prior knowledge to the algorithm.
For example, it is possible to include any symmetry information of the
system. This can be done by using descriptors that are independent of rotations, translations and permutations [34, 38, 65, 86]. Alternatively, one can
construct scalar-valued GP kernels that are made invariant under rotation (see
e.g., [8, 45]) or matrix-valued GP kernels made covariant under rotation ([45],
an idea that can be extended to higher-order tensors [14, 47]). Invariance or
covariance are in these cases obtained starting from a non-invariant representation by appropriate integration over the SO(3) rotation group [8, 45].
Symmetry aside, progress can be made by attempting to use kernels based
on simple, descriptive features corresponding to low-dimensional feature spaces.
Taking inspiration from parametrized force fields, these descriptors could e.g.,
be chosen to be interatomic distances taken singularly or in triplets, yielding
kernels based on 2- or 3-body interactions [45, 56, 99]. Since low-dimensional
feature spaces allow efficient learning (convergence is reached using small
databases), to the extent that simple descriptors capture the correct physics,
the GP process will be a relatively fast, while still very accurate, interpolator.

Scope of the present work
There are, however, two important aspects that have not as yet been fully explored while trying to develop efficient kernels based on dimensionally reduced
feature spaces. Both aspects will be addressed in the present work.
First, a systematic classification of rotationally invariant (or covariant, if
matrix valued) kernels, representative of the feature spaces corresponding to
n-body interactions is to date still missing. Namely, no definition or general
recipe has been proposed for constructing n-body kernels, or for identifying the
actual value (or effective interval of values) of n associated with already available kernels. This would be clearly useful, however, as the discussion above
strongly suggests that the kernel corresponding to the lowest value of n compatible with the physics of the target system will be the most informationally
efficient one for carrying out predictions: striking the right balance between
speed and accuracy.

18

Introduction
Second, for any ML approach based on a GP kernel and a fixed database,
the GP predictions for any target configuration are also fixed once and for all.
For an n-body kernel, these predictions do not need, however, to be explicitly
carried out as sums over the training dataset, as they could be approximated
with arbitrary precision by “mapping” the GP prediction on a new representation based on the underlying n-body feature space. We note that this
approximation step would make the final prediction algorithm independent of
the database size, and thus in principle as fast as any classical n-body potential based on functional forms, while still parameter free. The remainder of
this work explores these two issues, and it is structured as follows.
In the next Section II, after introducing the terminology and the notation
(II A), we provide a definition of an n-body kernel (II B) and we propose a
systematic way of constructing n-body kernels of any order n, showing how
previously proposed approaches can be reinterpreted within this scheme (II C
and D). We furthermore show, by extensive testing on a range realistic materials, how the optimal interaction order can be chosen as the lowest n compatible
with the required accuracy and the available computational power (II E). In
the following Section III we describe how the predictions of n-body GP kernels
can be recast (mapped) with arbitrary accuracy into very fast nonparametric
force fields based on machine learning (MFFs) which fully retain the n-body
character of the GP process from which they were derived. The procedure is
carried out explicitly for a 3-body kernel, and we find that evaluating atomic
forces is orders of magnitude faster than the corresponding GP calculation.

Thesis structure

19

Chapter 2

Gaussian processes for energies and
forces
2.1 Introduction
This chapter introduces the necessary background on Gaussian process (GP)
regression and the way in which it can be successfully applied to build interatomic force fields. Section 2.2 reviews the basic concepts behind standard GP
regression, while also introducing the terminology specific to learning local energy functions. Section 2.3 extends the the standard framework to its vectorial
counterpart, particularly suited to model atomic forces. Differently from forces
and from total energies, local energies are not quantum observable. However,
they still represent a useful concept for constructing GP models as they can
be learned from a datasets containing solely forces and/or total energies, and
Section 2.4 details how this can be practically done. Finally, Section 2.5 goes
through the way in which important physical properties of the energy of force
functions can be included in a GP, focussing on smoothness, symmetries and
interaction order.

2.2 Gaussian process regression
A local energy function ε(ρ) is defined as the energy ε of an atom given a
representation ρ of the set of positions of all the atoms surrounding it within
a cutoff distance rc . Such a set of positions is typically called an atomic environment or an atomic configuration, and ρ could simply be a list of the atomic

2.2. Gaussian process regression

21

species and positions expressed in Cartesian coordinates, or any suitably chosen representation of these [8, 40, 46, 65].
Let us assume that a database of reference calculations D = {(εri , ρi )}N
i=1
is available, composed by N local atomic configurations ρ = (ρ1 , . . . , ρN )T
and their corresponding energies εr = (εr1 , . . . , εrN )T . It is standard practice to
model the reference energies εri to be the result of the following process
εri = ε(ρi ) + ξi ,

(2.1)

where the true function ε(ρ) is corrupted by the independent zero-mean Gaussian noise ξi ∼ N (0, σn2 ), which can be imagined to model the combined uncertainty associated with both training data and model used. While learning
the result of a quantum calculation, the predominant source of uncertainty is
typically the locality error that results from the assumption of a finite cutoff
radius rc , outside of which atoms are treated as non-interacting. 1
Eq. (2.1) is a common starting point for many fitting approaches; the specificity and power of GP regression over standard parametric approaches lies in
the fact that ε(ρ) is not constrained to any given functional form, but it is
rather assumed to be distributed as a Gaussian stochastic process [106], typically with zero mean:
ε(ρ) ∼ GP(0, k(ρ, ρ0 )).
(2.2)
where k is the kernel function of the GP. The function k is the kernel function
of the GP, it is also called the covariance function as it is assumed to provide
the correlation
k(ρ, ρ0 ) = hε(ρ)ε(ρ0 )i,
(2.3)
where the brackets here indicate an expectation over the GP distribution. It
is important to note here that consistently with the above assumption a kernel
is required to be a positive semi-definite function. This can be seen from the
fact that for any set of real numbers {ci } the following property must always
be respected
X
X
ci k(ρi , ρj )cj = h(
ci ε(ρi ))2 i ≥ 0.
(2.4)
ij

i

The shorthand notation in Eq. (2.2) signifies that for vector of input config1

This assumption is necessary in order to define local energy functions. It typically holds
well by virtue of the “nearsightedness” of quantum mechanics [62, 78].

2.2. Gaussian process regression

22

urations ρ, the corresponding vector of local energies ε = (ε(ρ1 ), . . . , ε(ρN ))T
will be distributed according to a multivariate Gaussian distribution whose
covariance matrix is constructed through the given kernel function:
p(ε | ρ) = N (0, K)


k(ρ1 , ρ1 ) · · · k(ρ1 , ρN )


..
..
...

.
K=
.
.

k(ρN , ρ1 ) · · · k(ρN , ρN )

(2.5)

Since both ξi and ε(ρi ) are normally distributed, and since the sum of two
Gaussian random variables is also a Gaussian variable, one can write down
the distribution of the reference energies {εri } of Eq. (2.1) as a new normal
distribution whose covariant matrix is the sum of the original two:
p(εr | ρ) = N (0, C)

(2.6)

C = K + 1σn2 .

Building on this closed form (Gaussian) expression for the probability of the
reference data, it is possible to analytically obtain the predictive distribution
i.e., the probability distribution of the local energy value ε∗ associated with a
new target configuration ρ∗ , for the given dataset D = (ρ, εr ) (for details on
the derivation please refer to Appendix A.1 or Refs. [18, 110]). This is:
p(ε∗ | ρ∗ , ρ, εr ) = N (ε̂(ρ∗ ), v̂(ρ∗ ))
ε̂(ρ∗ ) = kT C−1 εr

,

(2.7)

σ̂ 2 (ρ∗ ) = k(ρ∗ , ρ∗ ) − kT C−1 k
where we defined the vector k = (k(ρ∗ , ρ1 ), . . . , k(ρ∗ , ρN ))T . Notice that the
positive semi-definiteness of the kernel function k guarantees the positive definiteness of the matrix C = K + 1σn2 , and hence also the existence of its inverse
C−1 .
The mean function ε̂(ρ) of the predictive distribution can be considered a
“best guess” for the true underlying function as it minimises the prediction
error (cf. Appendix A.2 or Ref. [18]). The mean function is often equivalently
written down as a linear combination of kernel functions evaluated over all

2.2. Gaussian process regression

2

mean
realisations

1

Energy (eV)

Energy (eV)

2

23

0

−1
−2

0.8

1.0

1.2

1.4

1.6

1.8

mean
realisations

1

0

−1
−2

0.8

Position (Å)

1.0

1.2

1.4

1.6

1.8

Position (Å)

(a)

(b)

Figure 2.1: Pictorial view of GP learning of a LJ dimer. Panel (a): mean,
standard deviation and random realisations of the prior stochastic process,
which represents our belief on the dimer interaction before any data is seen.
Panel (b): posterior process, whose mean passes through the training data
and whose standard deviation provides a measure of uncertainty.

database entries
ε̂(ρ) =

N
X

k(ρ, ρi )αi ,

(2.8)

i=1

where the coefficients are readily computed as αi = (C−1 εr )i . The posterior variance of ε∗ provides a measure of the uncertainty associated with the
prediction, normally expressed as the standard deviation σ̂(ρ).
The GP learning process can be thought of as an update of the prior
distribution Eq. (2.2) into the posterior Eq. (2.7). This update is illustrated
in Figure 2.1, in which GP regression is used to learn a simple Lennard Jones
(LJ) profile from a few dimer data. In particular, Figure 2.1(a) shows the
prior GP (Eq. (2.2)) while Figure 2.1(b) shows the posterior GP, whose mean
and variance functions are those of the predictive distribution in Eq. (2.7). By
comparing the two panels one notices that the mean function (equal to zero in
the prior process) approximates the true function (black solid line) by passing
through the reference calculations. Clearly, the posterior standard deviation
(uniform in the prior) shrinks to zero at the points where data is available, to
then increase again away from them. Three random function samples are also
shown for both prior and posterior process.

2.3. Vectorial Gaussian process regression

24

2.3 Vectorial Gaussian process regression
The force f (ρ) acting on an atom surrounded by a given environment ρ is
a vector quantity. It is hence natural to model the it with a vectorial GP
regression also referred to as “multi-output” or “multi-task” GP regression
[3, 71, 72]. This rather straightforward extension of the more standard (scalar)
method presented in the previous section is outlined in the following.
The starting assumption of the model, analogously to Eq. (2.9), is that
for any finite set of configurations {ρi } the corresponding values of the forces
{f (ρi )} taken by the vector function f (ρ) are distributed according to multivariate Gaussian distribution [110]. As a shorthand for this we write:
f (ρ) ∼ GP(0, K(ρ, ρ0 ))

(2.9)

where 0 is the vector-valued mean function, here set to zero, and K(ρ, ρ0 )
is the matrix-valued kernel function of the vectorial GP. The kernel K(ρ, ρ0 )
contains all the information about prior stochastic process as it represents
the correlation of the two vectors f (ρ) and f (ρ0 ) as a function of the two
configurations ρ and ρ0 :
K(ρ, ρ0 ) = hf (ρ)f T (ρ0 )i,

(2.10)

where the expectation is taken over the multivariate Gaussian distribution.
Similarly to the scalar case (Eq. (2.4)), any kernel K must be a positive semidefinite matrix-valued function, since for any collection of real valued vectors
{vi } one must have
X
ij

X
viT K(ρi , ρj )vj = h(
viT f (ρi ))2 i ≥ 0.

(2.11)

i

Once a training database is available, consisting in our case of atomic configurations and reference forces D = {(ρ, f r )i }N
i=1 , Bayes’ theorem [10] can be used
compute the predictive distribution (or, equivalently, the posterior GP).
Assuming a Gaussian noise process of variance σn2 , analogously to Eq. (2.1),

2.4. Local energy for global energies and forces

25

the predictive distribution can be computed analytically [3], giving
p(f ∗ | ρ∗ , ρ, {fir }) = N (f̂ (ρ∗ ), Σ̂(ρ∗ ))
∗

f̂ (ρ ) =

N
X

r
K(ρ, ρi )C−1
ij fj

,

ij

Σ̂(ρ∗ ) = K(ρ∗ , ρ∗ ) −

N
X

(2.12)

∗
KT (ρ∗ , ρi )C−1
ij K(ρ , ρi )

ij

where C = K + Iσn2 and blackboard bold characters indicate N × N block
matrices (for instance the Gram matrix K is defined as (K)ij = K(ρi , ρj )).
Similarly, C−1
ij denotes the ij-block of the inverse matrix.

2.4 Local energy for global energies and forces
The forces acting on atoms are well defined local property accessible to QM
calculations, easily computed by way of the Hellman-Feynman theorem [41].
As a consequence, the vectorial GP regression framework just described, can
be in principle be used to lean a force field directly on a database of quantum
forces (this will be the topic of Chapter 4). Local atomic energies on countrary
cannot be computed in quantum calculations, which can only provide the total
energy of the full system. However, the material presented in Section 2.2, in
addition to being of pedagogical importance, is still useful in practice since
local energy functions can be learned from observations of total energies and
forces only. Mathematically this is possible since any sum, or derivative, of
a Gaussian process is also a Gaussian process [110]. In the following, kernels
for total energies and forces will be derived starting from a simpler kernel for
local energies.

Kernel functions
Total energy kernels The total energy of a system can be modelled as a
sum of the local energies associated to each atomic environment
E({ρa }) =

Na
X
a=1

ε(ρa )

(2.13)

2.4. Local energy for global energies and forces

26

and if the local energy functions ε(ρ) in the above equation are distributed
according to a zero mean GP, then also the global energy E({ρa }) will be
GP variable with zero mean. To calculate the kernel functions k εE and k EE
providing the covariance between local and global energies and between two
global energies one simply needs to take the expectation with respect to the
GP of the given products
k εE (ρa , {ρ0b }) = hε(ρa )E({ρ0b })i

k EE ({ρa }, {ρ0b }) = hE({ρa })E({ρ0b })i

0

0

Na
X
=
hε(ρa )ε(ρ0b )i

=

=

Na
Na X
X

b=1

a=1 b=1

Na0

Na
Na X
X

X

hε(ρa )ε(ρ0b )i

0

k(ρa , ρb ).

=

k(ρa , ρb ).

a=1 b=1

b=1

(2.14)
(2.15)
Note that we have allowed the two systems to have a different number of
particles Na and Na0 and that the final covariance functions can be entirely
expressed in terms of local energy kernel functions k.
Force kernels The force f ({ρa }p ) on an atom p at position rp is defined as
the derivative
∂E({ρa }p )
.
(2.16)
f ({ρa }p ) =
∂rp
where by virtue of the existence of a finite cutoff radius of interaction, only
the set of configurations {ρa }p that contain atom p within their cutoff function
contribute to the force on p.
This quantity is also a GP [110] and the corresponding covariance between
forces and between force and local energies can be easily obtained by differentiation as described in Ref. [66, 110]. They read
kεf (ρa , {ρb }p ) =

X ∂k(ρa , ρb )
∂rT
q
q

{ρb }

Kff ({ρa }p , {ρb }q ) =

(2.17)

X X ∂ 2 kn (ρa , ρb )
.
∂rp ∂rT
q
p
q

{ρa } {ρb }

(2.18)

2.4. Local energy for global energies and forces

27

Total energy-force kernel Learning from both energies and forces simultaneously is also possible. One just needs to calculate the extra kernel kf E
comparing the two quantities in the database
0

fE

p

k ({ρa }

, {ρ0b })

=

N
XX
∂k(ρa , ρb )
{ρa

}p

b=1

∂rp

.

(2.19)

Mixed Gaussian process
To clarify how the kernels described above can be used in practice, it is instructive to look at a simple example. Imagine to have a database made up
from a single snapshot of an ab initio molecular dynamics of N − 1 atoms,
hence containing a single energy calculation and N − 1 forces.
Learning using these quantities would involve building a N × N block
matrix K containing the covariance between every pair


k EE ({ρa }, {ρb })
 fE
 k ({ρa }1 , {ρb })
K=
..

.



kEf ({ρa }, {ρb }1 ) · · · kEf ({ρa }, {ρb }N )

Kff ({ρa }1 , {ρb }1 ) · · · Kff ({ρa }1 , {ρb }N ) 
.
..
..
...

.
.


kf E ({ρa }N , {ρb }) Kff ({ρa }N , {ρb }1 ) · · · Kff ({ρa }N , {ρb }N )

(2.20)
As clear from the above equation, each block is either a scalar (the energyenrgy kernel in the top left), a 3 × 3 matrix (the force kernels) or a vector (the
energy-force kernels). The full dimension of K is hence (3N + 1) × (3N + 1).
Once such a matrix is built and the inverse C−1 = [K + Iσn2 ]−1 computed,
the predictive distribution for the value of the hidden total energy variable can
be easily written down.
For notational convenience. it is useful to define the vector {xi }N
i=1 containing all the guantities in the training database and the vector {ti }N
i=1 specifying
their type (meaning that ti is either E or f depending on the type of datapoint
contained in xi ). With this convention the predictive distribution takes the

2.5. Prior knowledge and kernel functions

28

form
p(ε∗ | D) = N (ε̂(ρ), σ̂ 2 (ρ))
X
ε̂(ρ) =
k εti (ρ, ρi )C−1
ij xj
ij

σ̂ 2 (ρ) = k(ρ, ρ) −

X

(2.21)

tj ε
k εti (ρ, ρi )C−1
ij k (ρj , ρ)

ij

−1 tj ε
where the products C−1
ij xj and Cij k (ρj , ρ) are intended to be bewteen
scalars, vectors or matrices depending on the nature of the quantities involved.

2.5 Prior knowledge and kernel functions
Choosing a Gaussian stochastic process as prior distribution over the energies
or forces rather than a parametrised functional form brings a few key advantages. A much sought advantage is that it allows grater flexibility: one can
show that in general a GP corresponds to a model with an infinite number of
parameters, and with a suitable kernel choice can act as a “universal approximator”: capable of learning any function if provided with sufficient training
data [110]. A second one is a greater ease of design: the kernel function must
encode all prior information about the local energy function, but typically contains very few free parameters (called hyperparameters) which can be tuned,
such tuning is typically straightforward and can be carried out either by trial
and error or via the more principled approaches discussed in in Chapter 5.
Third, GPs offer a coherent framework to predict the uncertainty associated
with the predicted quantities, via the posterior variance. This is typically not
possible for classical parametrised force fields.
All this said, the high flexibility associated with GPs can easily become a
drawback when examined from the point of view of computational efficiency.
Broadly, it turns out that for maximal efficiency (which unlike a simple accuracy criterion takes into account speed of learning and prediction) one should
constrain this flexibility in physically motivated ways, essentially by incorporating prior information in the kernel. In general, this will reduce the dimensionality of the problem e.g., by choosing to learn energy functions of significantly fewer variables than those featuring in the configuration ρ (3M for M
atoms in a configuration). To effectively incorporate prior knowledge into the

2.5. Prior knowledge and kernel functions

1.5

squared exp.
absolute exp.
Mat., ν = 2/3

0.8
0.6
0.4

1.0

Energy (eV)

1.0

Kernel value

29

0.2

0.5
0.0
−0.5
−1.0

0.0
0.0

0.2

0.4

0.6

−1.5

0.8

Distance (Å)

1.0

1.2

1.4

1.6

1.8

Position (Å)

(a)

(b)

Figure 2.2: Effect of various kernel functions on the smoothness of the
corresponding stochastic processes.

GP kernel it is fundamental to precisely know the the relations between important properties of the modelled energy and the corresponding kernel properties.
These are discussed in the remainder of this chapter, which considers in turn
properties of smoothness, invariance to physical symmetries, and interaction
order. The focus of the treatment is on the description scalar kernels for local
energies, and the concepts presented will be of fundamental importance in the
next chapter, dedicated to the design and use of local energy kernels that are
smooth, fully symmetric, and characterised by an adjustable interaction order
n. Similar properties apply also to the case of matrix-valued kernels for forces,
whose specific design will be the subject of Chapter 4.

Function smoothness
The relation between a given kernel and the smoothness of the random functions described by the corresponding Gaussian stochastic process has been
explored in detail [18, 110] and kernels defining functions of arbitrary differentiability have been developed. On one end of the spectrum, the so called
squared exponential kernel, defines infinitely differentiable functions:
kSE (d) = e−d

2 /2`2

.

(2.22)

The letter d here represents the distance between two points of the metric space
associated with the function to be learned (the local energy in the previous

2.5. Prior knowledge and kernel functions

30

section). At the opposite side of the spectrum, one could use the absolute
exponential kernel:
kAE (d) = e−d/` ,
(2.23)
defining everywhere continuous but nowhere differentiable target functions.
Finally, the Matérn kernel [18, 110]:
21−ν
kM,ν (d) =
Γ(ν)


√

d
2ν
`

ν


Kν

√ d
2ν
`


,

(2.24)

where Γ is the gamma function and Kν is a modified Bessel function of the
second kind, is a generalisation of the other two, and allows a controllable
differentiability depending on a parameter ν.
The relation between kernels and modelled function differentiability is illustrated by Figure 2.2, showing the three kernels mentioned above (Figure
2.2(a)) along with typical samples from the corresponding GP priors (Figure 2.2(b)). The absolute exponential kernel has been found useful to learn
atomisation energy of molecules [50, 85, 87], especially in conjunction with the
discontinuous Coulomb matrix descriptor [87]. In the context of modelling useful machine learning force fields, a relatively smooth energy or force function
is typically sought. For this reason, the absolute exponential is not appropriate and has never been used while the Matérn covariance has only found
limited applicability [27]. In fact, the squared exponential has been almost
always preferred, in conjunction with suitable representations ρ of the atomic
environment, [22, 34, 45, 111], and will also be used extensively in this thesis.

Physical symmetries
The following treatment will focus on the properties of energies forces are
analogous with the exception of rotational symmetry discussed in hcapter 4
Any energy function has to respect the symmetry properties listed below.
Translations. Physical systems are invariant upon rigid translations of all
their components. This basic property is relatively easy to enforce in any
learning algorithm via a local representation of the atomic environments. In
particular, it is customary to express a given local atomic environment as the
unordered set of M vectors {ri }M
i=1 going from the “central” atom to every

2.5. Prior knowledge and kernel functions

31

neighbour lying within a given cutoff radius [8, 40, 45, 46]. It is clear that any
representation ρ and any function learned within this space will be invariant
upon translations.
Permutations. Atoms of the same chemical species are indistinguishable,
and any permutation P of identical atoms in a configuration necessarily leaves
energy (as well as the force) invariant. Formally one can write ε(Pρ) =
ε(ρ) ∀P. This property corresponds to the kernel invariance
k(Pρ, P 0 ρ0 ) = k(ρ, ρ0 ) ∀P, P 0 .

(2.25)

Typically, the above equality has been enforced either by the use of invariant
descriptors [34, 45, 56, 65] or via an explicit invariant summation of the kernel
over the permutation group [7, 46, 111], with the latter choice being feasible
only when the symmetrisation involves a small number of atoms.
Rotations. The potential energy associated to a configuration should not
change upon any rigid rotation R of the same (i.e., formally, ε(Rρ) = ε(ρ) ∀R).
Similarly to permutation symmetry, this invariance is expressed via the kernel
property
k(Rρ, R0 ρ0 ) = k(ρ, ρ0 ) ∀R, R0 .
(2.26)
The use of rotation invariant descriptors to construct the representation ρ
immediately guarantees the above. Typical examples of such descriptors are
the symmetry functions originally proposed in the context of neural networks
[12, 13], the internal vector matrix [65], or the set of distances between groups
of atoms [34, 46, 56].
Alternatively, a “base” kernel kb can be made invariant with respect to the
rotation group via the following symmetrisation (“Haar integral” over the full
3D rotation group):
Z
k(ρ, ρ0 ) =

dR kb (ρ, Rρ0 ).

(2.27)

Such a procedure (called “transformation integration” in the ML community
[48]) was first used to build a potential energy kernel in Ref. [8]. It will be
discussed more in depth in the next chapter.

2.6. Summary

32

Interaction order
Classical parametrised force fields are sometimes expressed as a truncated
series of energy contributions of progressively higher “interaction orders” [28,
81, 97, 101]. The procedure is consistent with the intuition that, as long as the
series converges rapidly, truncating the expansion reduces the amount of data
necessary for the fitting, and enables a likely higher extrapolation power to
unseen regions of configuration space. The lowest truncation order compatible
with the target precision threshold is, in general, system dependent, as it will
typically depend on the nature of the chemical iteratomic bonds within the
system. For instance, metallic bonding in a close-packed crystalline system
might be described surprisingly well by a pairwise potential, while covalent
bonding yielding a zincblend structure can never be, and it will always require
three-body interactions terms to be present [45, 46]. Fixing the modelled
interaction order can hence be a very powerful way to the incorporation prior
knowledge into GPs, as also shown in Refs. [34, 45, 46].
The order of a kernel can be defined as the smallest integer n for which the
following property holds true:
∂ n kn (ρ, ρ0 )
=0
∂ri1 · · · ∂rin

∀ ri1 6= ri2 6= · · · =
6 rin ,

(2.28)

where ri1 , . . . , rin are the positions of any choice of a set of n different surrounding atoms. By virtue of linearity, the predicted local energy in Eq. (2.8)
will also satisfy the same property if kn does. Thus, Eq. (2.28) implies that
the central atom in a local configuration interacts with up to n − 1 other
atoms simultaneously, making the learned energy n-body. For instance, using
a 2-body kernel, the force on the central atom due to atom rj will not depend
on the position of any other atom rl6=j belonging to the target configuration
ρ({ri }). Eq. (2.28) can be used directly to check through either numeric or
symbolic differentiation if a given kernel is of order n, a fact that might be far
from obvious from its analytic form, depending on how the kernel is built.

2.6 Summary
In this chapter, the standard scalar GP regression framework was first reviewed
in Section 2.2, while Section 2.3 briefly discussed its vectorial extension. The

2.6. Summary
two approaches were later combined in Section 2.4, which explained how forces
and total energies can be used to learn a local energy function.
The importance of a careful design of the kernel - ideally encoding any
available prior information on the system studies - was stressed throughout,
and Section 2.5 contained an account of the way in which fundamental properties of the target force field, such as the interaction order, smoothness, as
well as its permutation, translation and rotation symmetry, can be included
into the kernel function used to model it.

33

Chapter 3

n-body kernels
3.1 Introduction
The previous chapter introduces the relations between fundamental properties
of energy and forces and the corresponding kernels’ properties. This chapter
exploits these relations to build kernels that define smooth and symmetric
energy functions of an arbitrary interaction order n. In Section 3.2, these
kernels are built through the sequential imposition of symmetries: from a
smooth and permutation invariant representation a range of kernels of finite
order is defined later made rotation and reflection invariant through a Haar
integration over the O(3) orthogonal group. Although analytically tractable,
the procedure yields kernel functions that are very computationally heavy to
evaluate and. Hence, to improve on this, Section 3.3 follows a different route
for the construction of symmetric kernels, which in facts can be defined directly
on invariant degrees of freedom like distances or angles between atoms. This
alternative procedure gives rise to efficient n-body kernels for low values of n,
before the cost of summing over all pairs of n-plets makes the kernel evaluation
unaffordable. However, one can inexpensively increase the order of a symmetric
kernel by raising it to an integer power - obtaining a higher finite oder kernel
- or by full exponentiation - giving rise to a fully many body kernel.
In this chapter as well as in the following ones it is assumed that atoms
are of a single chemical species, the multispecies generalisation of the main
n-body kernels proposed here is relatively straightforward and is reported in
Appendix A.3.

3.2. Building n-body kernels I: SO(3) integration

35

3.2 Building n-body kernels I: SO(3) integration
A standard translation and permutation invariant representation for an atomic
environment ρ is given by a linear sum of Gaussian functions N , each centred
on one of the M configuration atoms [8, 40, 45]. Fixing the variance of the
Gaussians to be `2 /2 for later convenience, the mentioned functional representation reads
M
X
ρ(r, {ri }) =
N (r | ri , `2 /2),
(3.1)
i=1

where r and {ri }M
i=1 are position vectors relative to the central atom of the
configuration. This representation guarantees by construction invariance with
respect to translations and permutations of atoms.
A 2-body permutation invariant kernel can be obtained as a dot product
overlap integral of two configurations [45]:
0

Z

dr ρ(r)ρ0 (r)
X
0 2
2
=L
e−kri −rj k /2` ,

k2 (ρ, ρ ) =

(3.2)

i∈ρ,j∈ρ0

where L is an unessential constant factor, omitted for convenience from now
on. Interestingly, the above kernel can also be directly defined - without ever
passing through the functional representation (3.1) - as the sum of all the
squared exponential kernels calculated on the distances between the relative
positions in ρ and those in ρ0 [52]. That the above kernel is a 2-body kernel
consistent with the definition of Eq. (2.28) can be checked straightforwardly
by explicit differentiation (see Appendix A.4), and its 2-body structure is also
readable from the fact that k2 is a sum of contributions comparing pairs of
atoms in the two configurations: the first pair located at the two ends of vector
ri in the configuration ρ, and consisting of the central atom and atom i, and
the second pair similarly represented by the vector r0j in the configuration ρ0 .
Higher order n-body kernels can be constructed as finite powers of the
2-body base kernel in Eq. (3.2):
kn (ρ, ρ0 ) = k2 (ρ, ρ0 )n−1

(3.3)

where the n-body property (Eq. (2.28)) can once more be easily checked by

3.2. Building n-body kernels I: SO(3) integration

36

explicit differentiation (see Appendix A.4). By building n-body kernels using
Eq. (3.3), one can avoid the exponential cost of summing over all n-plets that
a more naı̈ve kernel implementation would involve. This makes it in principle
possible to model any finite interaction order paying only the quadratic computational cost of computing the 2-body kernel in Eq. (3.2). Moreover, one can
obtain a fully many-body (“infinite order”) kernel by writing the squared exponential kernel on the natural distance d2 (ρ, ρ0 ) = k2 (ρ, ρ)+k2 (ρ0 , ρ0 )−2k2 (ρ, ρ0 )
induced by the (“scalar product”) k2 . This can be also written down as a formal many-body expansion:
kM B (ρ, ρ0 ) = e−d
=e

2 (ρ,ρ0 )/2`2

−k2 (ρ,ρ)−k2 (ρ0 ,ρ0 )
2`2




1
1
1
1 + 2 k2 + 4 k3 + 6 k4 + . . . .
`
2!`
3!`

(3.4)

As all powers of k2 are contained in the above series, this squared exponential kernel is fully many body. Interestingly, assuming a smooth underlying
function, the completeness of the series in Eq. (3.4) and the “universal approximator” property of the squared exponential [53, 110] can be immediately seen
to imply one another.
To check on these ideas, the proposed kernels are tested in learning the
interactions occurring in a simple one dimensional model consisting of n0 particles interacting via an ad-hoc n0 -body potential (cf. Appendix A.5 for details
on this toy model). We first let the particles interact to generate a configuration database, and then attempt to machine-learn these interactions using the
kernels just described. Very large training databases are used here to test the
intrinsic (database-independent) learnability of the interactions. Figure 3.1
illustrates the average relative prediction errors on the local energies of this
system incurred by a GP regression based on four different kernels as a function
of the interaction order n0 . It is clear from the graph that a force field that lets
the n0 particles interact simultaneously can only be learned accurately with a
(n ≥ n0 )-body kernel (Eq. (3.3)), or with the many-body squared exponential
kernel (Eq. (3.4)) containing all interaction orders.

3.2. Building n-body kernels I: SO(3) integration

Relative error on energy

101

k2
k3

100

k4
k5

37

kM B

10−1
10−2
10−3
10−4
2

3

4

5

Interaction order
Figure 3.1: GP relative error as a function of the interaction order (2- to
5-body), using n-body kernels with increasing n. Learning energies within
baseline precision (black dashed line) requires an n-body kernel with n at
least as high as the particles’ interaction order.

Rotation and reflection symmetric kernels
To construct n-body kernels useful for applications to real three dimensional
systems we need to include rotation and reflection invariance (Eq. (2.26)). As
discussed in Section 2.5 this can be done by performing an integral over the
orthogonal group. An invariant or “symmetric” n-body kernel kns can hence
be obtained as
Z
s
0
kn (ρ, ρ ) =
dQ kn (ρ, Qρ0 ).
(3.5)
O(3)

The use of this type of integral - formally known as a transformation integration
in the ML community [48] - was originally proposed in the context of potential
energy learning in Ref. [8]. A similar symmetrisation integral can be also
envisioned for the many-body base kernel kM B (Eq. (3.4))[45, 46], to define a
s
new many-body kernel kM
B invariant under all physical symmetries:
s
0
kM
B (ρ, ρ )

Z
=
O(3)

dQ kM B (ρ, Qρ0 ).

(3.6)

3.2. Building n-body kernels I: SO(3) integration

38

By virtue of the universal approximation theorem [53, 110] this kernel would be
able to learn arbitrary physical interactions with arbitrary accuracy, if provided
with sufficient data. Unfortunately, the exponential kernel (3.4) has to date
resisted all attempts to carry out the analytic integration over rotations or
reflections (3.6), leaving as the only open options numerical integration, or
discrete summation over a relevant point group of the system [45]. For a point
group G this discrete symmetrisation would take the form
0
ds
kM
B (ρ, ρ ) =

1 X
kM B (ρ, Gρ0 ).
|G| G∈G

(3.7)

On the other hand, the analytic integration of 2- or 3-body kernels has been
successfully carried out in different ways. The resulting symmetrized n-body
kernel kns will learn faster than its non-symmetrized counterpart kn , as redundant degrees of freedom have been integrated out. This is because a
non-symmetrized n-body kernel kn must learn functions of (3n − 3) variables
(translations are taken into account by the local representation based on relative position in Eq. (3.1)). After integration, the new kernel kns defines a
smaller and more physically-based space of functions of (3n − 6) variables,
which is the rotation-invariant functional domain of n interacting particles.
In Ref. [8] the Haar integration of a 3-body over the rotation group SO(3)
was carried out using appropriate functional expansions over spherical harmonics. The result was used as an intermediate step for the construction of
the widely used SOAP kernel [8, 31, 83, 99, 102]. This kernel has a full manybody character, ensured by the prescribed normalisation step [8], which made
it possible to use it e.g., to augment to full many-body the descriptive power
of a 2- and 3-body explicit kernel expansion [34]. However, the Haar integral
over rotations introduced in Ref. [8] as an intermediate kernel construction
step could also be seen, if taken on its own, as a transformation integration
procedure [48] yielding a symmetrised n-body kernel as defined in Eq. (3.5)
above, which would in turn become a higher finite-order kernel if raised to
integer powers ζ ≥ 2 (see next section).
Carrying out Haar integrals over rotations or reflections is not, in general,
an easy task. In the example above, computing a general rotation invariant
n-body kernel via the exact, suitably truncated spherical harmonics expansion
procedure of Ref. [8] becomes challenging for n > 3. Significant difficulties

3.2. Building n-body kernels I: SO(3) integration

39

n=2

n=3

n=4

n=5

Analytical integral

1.0
0.5
0.0
1.0
0.5
0.0
0.0

0.5

1.0 0.0

0.5

1.0

Numerical integral
Figure 3.2: Scatter plots showing the values of the integral in (3.9) on a
random sample of configurations, computed either by numerical integration
or via the analytic expression (Eqs. (3.10, 3.11, 3.13)). Interaction orders
from n = 2 to n = 5 are considered.

likewise arise if attempting a “covariant” integration over the orthogonal group,
the main subject of the next chapter. In this case, an exact analytic expression
was found only for 2- and 3-body matrix-valued kernels [45], with a technique
that becomes unviable for n > 3. Fortunately, the Haar integration can be
avoided altogether, following the simple route of constructing symmetric nbody kernels directly using symmetry-invariant descriptors, as we will see in
the next section. The problem of obtaining an analytic Haar integral expression
for the general n case remains, however, an interesting one, which is tackled
in the remainder of this section following a novel analytic route which fully
exploits the Gaussian nature of the configuration expansion in Eq. (3.1).
First, it is useful to express the n-body base kernel of Eq. (3.3) as an explicit
product of (n − 1) 2-body kernels. The Haar integral (3.5) can then be written
as
kns (ρ, ρ0 ) =

X

k̃i,j

(3.8)

i=(i1 ,...,in−1 )∈ρ
j=(j1 ,...,jn−1 )∈ρ0

Z
k̃i,j =
O(3)

dQ e−

kri −Qr0j k2
1
1
2`2

. . . e−

kri
−Qr0j
k2
n−1
n−1
2`2

(3.9)

3.2. Building n-body kernels I: SO(3) integration

40

where now for each of the two configurations ρ, ρ0 , the sum runs over all nplets of atoms that include the central atom (whose indices i0 and j0 are thus
omitted). Expanding the exponents as (ri − Qr0j )2 = ri2 + rj02 − 2Tr(Qr0j rT
i )
allows us to extract from the integral (3.9) a rotation independent constant
Ci,j , and to express the rotation-dependent scalar products sum as a trace of a
matrix product:
k̃i,j = Ci,j Ii,j

(3.10)

Ci,j = e
Z
Ii,j =

(3.11)

−(ri2 +rj02 +...ri2
+rj02
)/2`2
1
1
n−1
n−1

dQ eTr(QMi,j )

(3.12)

O(3)

where the matrix Mi,j is the sum of the outer products of the ordered vector
0
T
2
couples in the two configurations: Mi,j = (r0j1 rT
i1 + · · · + rjn−1 rin−1 )/` . The
integral (3.12) occurs in the context of multivariate statistics as the generating
function of the non-central Wishart distribution [4]. As shown in [58], it can
P
be expressed as a power series in the symmetric polynomials α1 = 3i µi , α2 =
P3
3
i<j µi µj and α3 = µ1 µ2 µ3 of the eigenvalues {µi }i=1 of the symmetric matrix
MT
i,j Mi,j :
Ii,j =
Ap1 p2 p3 =

X

Ap1 p2 p3 α1p1 α2p2 α3p3

p1 ,p2 ,p3

π 2−(1+2p1 +4p2 +6p3 ) (p1 + 2p2 + 4p3 )!
p1 !p2 !p3 !Γ( 23 + p1 + 2p2 + 3p3 )Γ(1 + p2 + 2p3 )
1
× 1
.
Γ( 2 + p3 )(p1 + 2p2 + 3p3 )!

(3.13)

Remarkably, in this result (whose exactness is checked numerically in Figure 3.2) the integral over the orthogonal group does not depend on the order
n of the base kernel, once the matrix Mi,j is computed. This is not the case
for previous approaches to integrating over rotations [8, 45] that need to be
reformulated with increasing and eventually prohibitive difficulty each time
the order n needs to be increased. However, the final expression given by Eqs.
(3.10-3.13) is still a relatively complex and computationally heavy function
of the atomic positions. Fortunately such complexity can be largely avoided
altogether if equally accurate kernels can be built by physical intuition at least
for the most relevant lowest n orders, as discussed in the next section.

3.3. Building n-body kernels II: n-body feature spaces

41

3.3 Building n-body kernels II: n-body feature spaces
The practical effect of the Haar integration (3.5) is the elimination of the three
spurious rotational degrees of freedom. The same result can often be achieved
by selecting a group of symmetry-invariant degrees of freedom for the system,
typically including the distances and/or bond angles found in local atomic
environments, or simple functions of these. Appropriate symmetric kernels
can then simply be obtained by defining a similarity measure directly on these
invariant quantities [34, 38, 65, 87]. To construct symmetry invariant n-body
kernels with n = 2 and n = 3 we can choose these degrees of freedom to be
just interparticle distances:
k2s (ρ, ρ0 ) =

X

0 2 /2`2

e−(ri −rj )

,

(3.14)

i∈ρ
j∈ρ0

k3s (ρ, ρ0 ) =

X

X

T −P(r 0 ,r 0 ,r 0
T 2
2
j1 j2 j1 j2 ) k /2`

e−k(ri1 ,ri2 ,ri1 i2 )

.

(3.15)

i1 >i2 ∈ρ P∈P
j1 >j2 ∈ρ0

where ri indicates the Euclidean norm of the relative position vector ri , and
the sum over all permutations of three elements P (| P |= 6) ensures the permutation invariance of the kernel (see Eq. (2.25)). Since these kernels learn
functions of low-dimensional spaces, their exact analytic form is not essential
for performance, as many well behaved functions of the relevant degrees of
freedom are likely to give equivalent converged results in the rapidly reached
large-database limit. This equivalence can be neatly observed in Figure 3.3,
which reports the performance of 2- and 3-body kernels built either directly
over the set of distances (Eqs. (3.14) and (3.15)) or via the exact Haar integral
(Eqs. (3.8-3.13)). As the test system is crystalline Silicon, 3-body kernels are
better performing. However, since convergence of the 2- and 3-body feature
space is quickly achieved (at about N = 50 and N = 100 respectively), there
is no significant performance difference between SO(3)-integrated n-body kernels and physically motivated ones. Consequently, for low interaction orders,
simple and computationally fast kernels like the ones in Eqs. (3.14, 3.15)
are always preferable to more complex (and heavier) alternatives obtained via
Haar integration (e.g., the one defined by Eqs. (3.8-3.13) or those found in
Refs. [8, 45].
An immediate generalisation of Eqs. (3.14,3.15) would be the construction

MAE on force, (eV/Å)

3.3. Building n-body kernels II: n-body feature spaces

42

k2s
k2s , Haar
k3s
k3s , Haar

0.13
0.12
0.11
0.10
0.09

101

102

103

Number of training configurations, N
Figure 3.3: Learning curves for 2 and 3-body kernels obtained either via a
Haar integration (Eqs. (3.8-3.13)), or directly specifying a similarity kernel
function of the effective degrees of freedom (Eqs. (3.14, 3.15)). The mean
absolute error (MAE) is calculated as the mean of the absolute value of the
vector difference between predicted and reference force.

of an arbitrary symmetric n-body kernel as
kns (ρ, ρ0 ) =

X

k̃n (qi1 ,...,in−1 , q0j1 ,...,jn−1 ),

(3.16)

i1 >···>in−1 ∈ρ
j1 >···>jn−1 ∈ρ0

where the components of the feature vectors q are the chosen symmetryinvariant degrees of freedom describing the n-plets of atoms. The q feature
vectors are required to be (3n − 6) dimensional for all n, except for n = 2,
where they become scalars. In practice, for n > 3 selecting a suitable set of
invariant degrees of freedom is not trivial. For instance, for n = 4 the set of
six unordered distances between four particles does not specify their relative
positions unambiguously, while for n > 4 the number of distances associated
with n atoms exceeds the target feature space dimension (3n − 6). Meanwhile,
the computational cost of evaluating the full sum in Eq. (3.16) very quickly
becomes prohibitively large as the number of elements in the sum grows exponentially with n.
Consequently, building high order kernels using (3.16) is only practical
when there are only very few atoms in atomic configurations (low M ). How-

3.3. Building n-body kernels II: n-body feature spaces
Unique

43

Non Unique

ρ

ρ

Figure 3.4: Unique interaction (left panel) associated with the 3-body kernel
k3s (3.15) compared with the non-unique 3-body interaction (right panel)
associated with the kernel k3¬u = (k2s )2 (3.17), which is a function of two
distances only (see text).

ever, the order of an already symmetric n-body kernel can be augmented with
no computational overhead by generating a derived kernel through simple exponentiation to an integer power, although this is achieved at the cost of losing
the uniqueness [8, 54, 107] of the representation. This can be easily understood
by means of an example, graphically illustrated in Figure 3.4x. Let us consider
the 2-body symmetric kernel k2s (Eq. (3.14)) which learns a function of just a
single distance, and therefore treats the ri distances between the central atom
and its neighbors independently. Its square has the form
k3¬u (ρ, ρ0 )

=

X

e

−

(ri −rj0 )2
1
1
2`2

−

e

(ri −rj0 )2
2
2
2`2

(3.17)

i1 >i2 ∈ρ
j1 >j2 ∈ρ0

Such a kernel will be able to learn functions of two distances ri1 , ri2 from the
central atom of the target configuration ρ (see Figure 3.4) and thus will be
a 3-body kernel in the sense of Eq. (2.28). However, it will not be able to
resolve angular information, as rotating the atoms in ρ around the origin by
independent, arbitrary angles will yield identical predictions.
Extending this line of reasoning, it is easy to show that squaring a symmetric 3-body kernel yields a kernel that can capture interactions up to 5-body,
although again non-uniquely. This has often been done in practice by squaring
the SOAP integral [34, 83]. Raising a 3-body “input” kernel to an arbitrary

3.3. Building n-body kernels II: n-body feature spaces

44

integer power ζ ≥ 2 yields an n-body output kernel of order 2ζ + 1:
¬u
kn=2ζ+1
= k3s (ρ, ρ0 )ζ .

(3.18)

This kernel is also non-unique as it will learn a function of only 3ζ variables,
while the total number of relevant n-body degrees of freedom (3n − 6 = 6ζ − 3)
is always larger than this. Substituting 3 with any n0 order of the symmetrized
s
0 ζ
¬u
input kernel will similarly generate a kn=(n
0 −1)ζ+1 = kn0 (ρ, ρ ) kernel of order
n = (n0 − 1)ζ + 1. A simple calculation reveals that, also in the general case,
the number of variables on which kn¬u is implicitly built is (3n0 − 6)ζ, always
smaller than the full dimension of n-body feature space (3n0 − 3)ζ − 3 (as
expected, the two become equal only for the trivial exponent ζ = 1).
In practice, the non-unicity issue appears to be a severe problem only when
the input kernel is a 2-body kernel, and as such it depends only on the radial
distances from the central atoms occurring in the two atomic configurations (cf.
Eq. (3.15) and Figure 3.4). In this case the non unique output n-body kernels
will depend on ζ-plets of radial distances, and will miss angular correlations
encoded in the training data [46]. On the contrary, a symmetric 3-body kernel
(Eq. (3.15)) contains angular information on all triplets in a configuration, and
using this kernel as input will be able to capture higher interaction orders (as
confirmed e.g., by the numerical tests performed in Ref. [8]).
None of the kernels obtained as finite powers of some symmetric lower-order
kernels is a many-body one (they will all satisfy Eq. (2.28) for some finite n).
However, an attractive immediate generalization consists of substituting any
squaring or cubing with full exponentiation similarly to what done in Eq. (3.4).
Indeed, one could build a symmetric many body kernel as a squared exponential on the invariant distance d2s (ρ, ρ0 ) = k3s (ρ, ρ) + k3s (ρ0 , ρ0 ) − 2k3s (ρ, ρ0 ),
obtaining
s
−(k3s (ρ,ρ)+k3s (ρ0 ,ρ0 )−2k3s (ρ,ρ0 ))/2`2
kM
.
(3.19)
B = e
It is clear from the infinite expansion in Eq. (3.4) that this kernel is a manybody one in the sense of Eq. (2.28), and is also fully symmetric. As is also
the case for all finite-power kernels, the computational cost of this many body
kernel will depend on the order n0 of the input kernel (3 in the present example)
as the sum in Eq. (3.16) only runs on the atomic n0 -plets (here, triplets) in ρ
and ρ0 . This new kernel is not a priori known to neglect any order of interaction

3.4. Tests on real systems
kernel
k2s
k3¬u
k3s
k5¬u
ds
kM
B

order
2
3
3
5
∞

45
name
2-body
3-body, non-unique
3-body
5-body, non-unique
many-body, discrete symm.

relevant eq.
3.14
3.17
3.15
3.18
3.7

ds was obtained by
Table 3.1: Some of the kernels proposed. The kernel kM
B
a discrete sum over the O48 crystalline group.

that might occur in a physical system and thus be encoded in a reference QM
training database.
Another way to inexpensively obtain a many body kernel is by normalisation of an explicit finite order one
kM B = p

k3s (ρ, ρ0 )
k3s (ρ, ρ)k3s (ρ0 , ρ0 )

.

(3.20)

The denominator makes this many-body in the sense of Eq. (2.28) (as is
also the case for the SOAP kernel, while no Haar integration is needed here).
Incidentally, the above can also be seen to be a squared exponential kernel
on the distance induced by the scalar product kernel given by the natural
logarithm of k3s (ρ, ρ0 ).
This section provided a definition for an n-body kernel, and proposed a
general formalism for building n-body kernels by exact Haar integration over
the orthogonal group. A class of simpler kernels based on invariant features was
defined, also n-body according to the previous definition. As both approaches
become computationally expensive for high values of n, it was pointed out that
n-body kernels can be built as powers of lower-order input n0 -body kernels,
with no additional computational overhead. While such a procedure comes at
the cost of sacrificing the unicity property of the descriptor, it also suggests
how to build, by full exponentiation, a many-body symmetric kernel. For
many applications, however, using a finite-order kernel will provide the best
option, as suggested by the numerical tests reported in the next section.

3.4. Tests on real systems

3.4 Tests on real systems
The performance of some of the kernels proposed in the last section are here
tested on a range of realistic materials described at the DFT level of accuracy,
please cf. Appendix A.6 for more details on the datasets used.
In this section, as well as in Figure 3.3 an in the rest of this thesis, the
absolute value of the vector difference |f r (ρ) − f̂ (ρ)| between reference force fir
and predicted force f̂ (ρi ) is used as a measure or error. The mean value of this
quantity across a randomly sampled “test” dataset (not containing any training
point) is defined as the mean absolute error (MAE) on force. Calculating the
MAE on different randomly sampled test sets provides the standard deviation
and hence the errors bars plotted.

The kernels considered are listed for convenience in Table 3.1, while their
performance is compared in Figure 3.5. The figure reveals some general trends.
2-body kernels can be trained very quickly, as good convergence can be attained already with ∼100 training configurations. The 2-body representation
is a very good descriptor for a few materials under specific conditions, while
their overall accuracy is ultimately limited. This will yield e.g., excellent force
accuracy for a close-packed bulk system like crystalline Nickel (inset (a)), and
reasonable accuracy for a defected α-Fe system whose bcc structure is however metastable if just pair potentials are used (inset (b)). Accuracy improves
dramatically once angular information is acquired by training 3-body kernels.
These can accurately describe forces acting on iron atoms in the bulk α-Fe
system containing a vacancy (inset (b)) and those acting on carbon atoms
in both diamond and graphite (inset (c)). However, 3-body GPs need larger
training databases. Also, atoms participate in many more triplets than simple bonds in their standard environments contained in the database, which
will make 3-body kernels slower than 2-body ones for making predictions by
GP regression. Both problems would extend, getting worse, to higher values
of n, as summing over all database configurations and all feature n-plets in
each database configuration will make GP predictions progressively slower.
However, complex materials where high-order interactions presumably play a
significant role should be expected to be well described by GPs based on a

46

3.4. Tests on real systems

47

0.4

k3s
ds
kM
B

EAM
k2s

0.3

0.2

0.1

MAE on force (eV/Å)

MAE on force (eV/Å)

0.4

0.0

0.3

0.2

0.1

0.0
10

20

50

100 200

500 1000

10

Number of training points, N

20

50

100 200

500 1000

Number of training points, N

(a)

(b)
2.0

k2s
k3s
ds
kM
B

1.5

1.0

0.5

0.0

MAE on force (eV/Å)

2.0

MAE on force (eV/Å)

ds
kM
B
k3¬u
k5¬u

EAM
k2s
k3s

k2s
k3s
ds
kM
B

1.5

1.0

0.5

0.0
10 20

50 100 200 5001000

4000

Number of training points, N
(c)

10 20

50 100 200 5001000

4000

Number of training points, N
(d)

Figure 3.5: Learning curves reporting the mean generalisation error (measured as the modulus of the difference between target and predicted force
vectors) as a function of the training set size, for different materials and
kernels of increasing order. The insets in (a) and (d) report the converged
error achieved by a given kernel as a function of the kernel’s order. The systems considered are: (a) Crystalline nickel, 500K; (b) iron with a vacancy,
500K; (c) diamond and graphite, mixed temperatures and pressures; and
(d) amorphous silicon, 650K. For extra details on the datasets and kernels
used, and on the experimental methodology, cf. Appendix A.6

48

bulk
nanocluster

0.175
0.150
0.125
0.100
0.075
0.050
2-body

3-body

Kernel order
(a)

many-body

MAE on force (eV/Å)

MAE on force (eV/Å)

3.4. Tests on real systems

0.35

bulk
amorphous

0.30
0.25
0.20
0.15
0.10
0.05
2-body

3-body

many-body

Kernel order
(b)

Figure 3.6: Converged error achieved by a given kernel as a function of
the kernel’s order. In inset (a) Crystalline nickel is compared to a nickel
nanocluster while in inset (b) crystalline silicon is compared to amorphous
silicon.

many-body kernel. This is verified here in the case of amorphous Silicon (inset
(d)).
Figure 3.5 (b) also shows the performance of some non-unique kernels. As
discussed above, these are options to increase the order of an input kernel
avoiding the need to sum over the correspondingly higher order n-plets. Our
tests indicate that the GPs generated by non-unique kernels sometimes improve appreciably on the input kernels’ performance: e.g., the error incurred
by the 2-body kernel of Eq. (3.14) in the Fe-vacancy system is higher than
that associated with its square, the non-unique 3-body kernel of Eq. (3.17).
Unfortunately, but not surprisingly, the improvement can be in other cases
modest or nearly absent, as exemplified by comparing the errors associated
with the 3-body kernel and its square -the non-unique 5-body kernel-, in the
same system.
Overall, the analysis of Figure 3.5 suggests that one way to select an optimal
kernel is by comparing the learning curves of the various n-body kernels and the
many-body kernel over the available QM database: the comparison will reveal
the simplest (most informative, lowest n) description that is still compatible
with the error level deemed acceptable in the simulation. Identifying the n
value best suited for the description of a given material system can also be
done in practice by monitoring how the converged error varies as a function of

3.5. Summary
the kernel order. Plots illustrating this behaviour are provided in Figure 3.6,
where nickel and silicon systems are considered respectively in insets (a) and
(b). In each plot the more complex system (a Ni cluster and an amorphous
Si system, respectively) display a high accuracy gain (larger negative slope)
when the kernel order is increased, while the relatively simpler cristalline Ni
and Si systems show a practically constant trend on the same scale. The issue
of choosing the best kernel order n to describe a given system can also be
tackled in the context of Bayesian theory and it is explored in greater depth
in Chapter 5.
Trading transferability for accuracy by training the kernels on a QM database
appropriately tailored for the target system (e.g., restricted to just bulk or
simply-defected system configurations sampled at the relevant temperatures
as done in the Ni and Fe-systems of Figure 3.5) will enable surprisingly good
accuracy even for low n values. This should be expected to systematically improve on the accuracy performance of classical potentials involving non-linear
parameter fitting, as exemplified by comparing the errors associated with nbody kernel models and the average errors of state-of-the-art embedded atom
model (EAM) P-FFs [69, 73] (insets (a) and (b)).

3.5 Summary
This chapter provided explicit mathematical expressions for a set of kernels
encoding smoothness and invariance properties desirable to any MD force field
and an adjustable parameter n controlling the modelled interaction order.
Section 3.2 presented a systematic way to construct such kernels by enforcing
relevant properties one after the other. Permutation invariance is encoded into
a functional representation of atomic environments based on sums of Gaussian
functions, a base kernel is then defined as an overlap integral of the product
of two environments and invariance over rotations and reflections is then imposed via a Haar integration over the orthogonal group. All the steps can be
performed analytically but the process still results into very computationally
heavy functions of the atomic positions. To improve on this problem, Section 3.3 covered an alternative way to build kernels with equivalent features.
In this parallel approach, n-body kernels are built directly on the invariant
degrees of freedom of groups of n atoms. This results in simple and computa-

49

3.5. Summary
tionally efficient kernels, whose performance is shown to be equivalent to that
achieved by the Haar-integrated kernels.
Altough this alternative procedure also becomes computationally demanding for n > 3, higher order symmetric kernels can be obtained by exponentiating lower order ones to integer powers. Kernels obtained in this way will
be “non-unique” i.e., incapable of learning an arbitrary physical interaction
of the same order. However, this issue is only a severe limitation to the final
accuracy only when the input kernel is 2-body and all the angular information
is lost.
Tests on a range of DFT materials proved the effectiveness of the proposed
kernels, all improving on state of the art parametric potentials in terms of error
on the target forces. The relative accuracy of the n-kernels was also found to
be highly dependent on the material under consideration, suggesting that the
best n should will always be system dependent

50

Chapter 4

Covariant kernels
4.1 Introduction
The concept of local energy extensively used in the last chapter, is useful but
not necessary for the construction of accurate machine learning force fields.
In facts, in learn-on-the-fly (LOTF) [30] molecular dynamics applications,
the high-accuracy target and local interpolation character of force predictions
makes it appealing to learn forces directly rather than learning a local energy
scalar field first and then deriving forces by differentiation. One way to accomplish this is by using GP regression to separately learn individual force
components [21, 22, 25, 65]. This approach might result into computationally
fast algorithms but is intrinsically limited by the impossibility of exploiting
the strong correlation existing between the three components of a force vector,
typically induced by the equivalence of forces related by a rigid rotations or reflections of the configuration space. In order to exploit this basic symmetry, it
is once again necessary to impose this constraint into the (now matrix-valued)
kernel function.
This chapter deals with the definition, construction and use of “covariant
kernels” i.e., kernels that incorporate the covariant behaviour of the function
to be learned. In addition to being surely of potential use if LOFT simulation,
covariant kernels built on the ideas introduced in this chapter have also found
great applicability in efficiently learning other physical vectors (or higher order
tensors) that are also covariant under a given symmetry transformation [47].
Section 4.1 introduces the definition of a covariant kernel, and proves that
a kernel possessing the covariance property learns vector valued functions with

4.2. Kernel covariance

52

the correct behaviour under the given symmetry operation. Section 4.3 proposes a general procedure for building covariant kernels from non covariant
ones, based on a Haar integration over the orthogonal group similar to the one
thoroughly discussed in Chapters 2 and 3. In Section 4.4 this procedure is put
into practice to build n-body and many-body covariant kernels for learning
forces in one, two and three dimensional spaces. While the lower dimensional
examples serve mainly to provide illustrative scenarios for understanding the
effect of covariance imposition, the three dimensional covariant kernels obtained can be used to accurately learn forces in realistic materials. This is
done in Section 4.5, where the performance of covariant kernels is tested in
nickel, iron and silicon systems.

4.2 Kernel covariance
The symbol Q will represent a member of the orthogonal group. In particular, Q can be either rotation (for which the symbol R will be used) or a
reflection (represented by F) acting on an atomistic configuration ρ. 1 There
are two properties of the learned GP should respect once configurations are
transformed by an operator Q (represented by a matrix Q).
Property 1 If the target configuration ρ is transformed to Qρ, the GP
predicted mean and variance must transform accordingly:
f̂ (Qρ) = Qf̂ (ρ).

(4.1)

Σ̂(Qρ) = QΣ̂(ρ)QT .

(4.2)

Property 2 The predicted mean and variance must not change if we arbitrarily transform the configurations in the database (D → Q̃ = {(Qi ρi , Qi fir )})
with any chosen set of roto-reflections {Qi }.
1

Although rotations and reflections are the focus of the present chapter, the general
theoretical results on kernel covariance developed in this section apply to the general case
of an arbitrary groups.

4.2. Kernel covariance

53

A special class of kernel functions that automatically guarantees these two
properties can be now introduced: a covariant kenrel has the defining property
K(Qρ, Q0 ρ0 ) = QK(ρ, ρ0 )Q0T .

(4.3)

That a covariant kernel imposes Property 1 follows straightforwardly from
directly from the defining equations for posterior mean and variance. For
instance, the predicted force on a rotated configuration Qρ is
f̂ (Qρ) =

N
X

r
K(Qρ, ρi )[K + Iσn2 ]−1
ij fj

ij

=

N
X

r
QK(ρ, ρi )[K + Iσn2 ]−1
ij fj

(4.4)

ij

= Qf̂ (ρ).
The correct behaviour of the predictive variance Σ̂(ρ) is also easy to check as
it follows similarly from the linearity of the relevant equation (Eq. (2.12)).
To prove Property 2, first note that if the kernel function is covariant the
transformed database D̃ has Gram matrix (K̃)ij = K(Qi ρi , Qj ρj ) = Qi K(ρi , ρj )QT
j .
If we define the block-diagonal matrix Qij = δij Qi , this can be written in
the simple block-matrix form K̃ = QKQT . Using kernel covariance again to
write K(ρ, Qi ρi ) = K(ρ, ρi )QT
ii the prediction associated with the transformed
database D̃ can be written as
f̂ (ρ | D̃) =

N
X

T
2 −1
r
K(ρ, ρi )QT
ii [QKQ + Iσn ]ij Qjj fj .

(4.5)

ij

By simple matrix manipulations it is now possible to show that in the above
expression the symmetry transformations cancel out; indeed
QT [QKQT + Iσn2 ]−1 Q = QT [Q(K + Iσn2 )QT ]−1 Q
= QT (QT )−1 [K + Iσn2 ]−1 Q−1 Q

(4.6)

= [K + Iσn2 ]−1
Eq. (4.6) along with Eq. (4.5) implies f̂ (ρ | D̃) = f̂ (ρ). Moreover, the cancellation happening in Eq. (4.6) can also be used to show that Σ̂(ρ | D̃) = Σ̂(ρ)

4.3. Covariant integration

54

to complete the proof of Property 2.
It is easy to check that the kernels proposed in Chapter 3 (e.g., the many
body squared exponential of Eq. (3.4) or the 2- and n-body in Eqs. (3.2) and
(3.3)) do not possess the covariance property (4.3). Designing, entirely by
feature engineering, a covariant kernel is in principle possible but can require
complex tuning and is likely to be highly system dependent (see e.g. Ref. [65]).
Note that non covariant kernels can be used and these difficulties be avoided,
some having been successfully implemented [21, 22]. This leaves space for
improvement as prediction efficiency will generally be enhanced by increased
exploitation of symmetry (see e.g., Figure 4.3 below for a simple test of this).

4.3 Covariant integration
This section presents a general method to transform standard matrix kernels
into covariant ones, followed by numerical tests suggesting that the resulting
kernel improves very significantly on the force-learning properties of the initial
one, its error converging with just a fraction of the training data. This proceeds along the lines of previous techniques utilised in the last chapter to build
symmetric scalar n-body kernels, namely the transformation integration procedure developed in [48] and used within the SOAP representation for learning
potential energy surfaces of atomic systems [6, 8].
Given a group Q with elements Q represented by a matrix Q and a base
kernel Kb , a covariant kernel KQ can be constructed by
Q

Z

0

K (ρ, ρ ) =
Q

b
0
dQ1 dQ2 QT
1 K (Q1 ρ, Q2 ρ )Q2

(4.7)

where dQ is the normalised Haar measure for the symmetry group we are
integrating over [68].
The covariance of KQ as given by (4.7) is easily checked as
Q

0 0

K (Qρ, Q ρ ) =

Z
Z

=

b
0 0
dQ1 dQ2 QT
1 K (Q1 Qρ, Q2 Q ρ )Q2
b
0
0T
dQ̃1 dQ̃2 QQ̃T
1 K (S̃1 ρ, Q̃2 ρ )Q̃2 Q

= QKQ (ρ, ρ0 )Q0T

(4.8)

4.3. Covariant integration

55

where the second line follows from the substitutions Q̃1 = Q1 Q and Q̃2 =
Q2 Q0 . Note that these transformations have unit Jacobian because of the
translational invariance (within the group) of any Haar measure [5, 68].
It can be shown that the positive semi-definiteness (Eq. (2.11)) of the
base kernel is preserved under the operation (4.7) of covariant integration. In
particular, a kernel is positive semi-definite if and only if it is a scalar product
in some (possibly infinite dimensional) vector space [70, 110]. Hence the base
R
0
kernel can be written as Kb (ρ, ρ0 ) = dα φα (ρ)φT
α (ρ ). It is then possible to
show that its covariant counterpart KQ (Eq. (4.7)) will also be a scalar product
in a new function space. Indeed
Q

Z

0

K (ρ, ρ ) =
Z
=
Z
=

b
0
dQ1 dQ2 QT
1 K (Q1 ρ, Q2 ρ )Q2
T
0
dα dQ1 dQ2 QT
1 φα (Q1 ρ)φα (Q2 ρ )Q2

(4.9)

0
dα ψ α (ρ)ψ T
α (ρ )

R
where the new basis vectors were defined as ψ α (ρ) = dQ QT φα (Qρ). Hence,
KQ will also be positive definite.
The completely general procedure above can be cumbersome to apply in
practice, because of the double integration over group elements in (4.7) and the
dependence on the design of the base kernel matrix Kb . As a simplification,
we assume the base kernel to be of diagonal form; assuming equivalence of all
space directions, we can then write
Kb (ρ, ρ0 ) = Ik b (ρ, ρ0 ).

(4.10)

where the scalar base kernel k b is independent on the reference frame in which
the configurations are expressed. Further requiring that
k b (Qρ, Qρ0 ) = k b (ρ, ρ0 ),

(4.11)

that is, scalar invariance of the base kernel (a property very commonly found
in standard kernels), the double integration in (4.7) reduces at this point to a

4.3. Covariant integration

56

single one
Q

Z

0

K (ρ, ρ ) =
Z
=
Z
=

b
0
dQ1 dQ2 QT
1 Q2 k (Q1 ρ, Q2 ρ )
b
−1
0
dQ1 dQ2 QT
1 Q2 k (ρ, Q1 Q2 ρ )

(4.12)

dQ Q k b (ρ, Qρ0 )

where the second line follows from property (4.11) and the third line is obtained by the substitution Q = Q−1
1 Q2 . In the next section, the analytical
integration (4.12). of some base kernels is performed. Note that incorporating
prior knowledge on the correct behaviour of forces in the kernel enables learning and predicting forces associated with any configurations, regardless of its
orientation. However, being able to do this for completely generic orientations
is not always necessary. In many systems (e.g. crystalline solids where the
orientation is known) all relevant configurations cluster around particular discrete symmetries. For these systems the relevant physics can be captured by
restricting equation (4.7) to a discrete sum over the relevant group elements:
KG (ρ, ρ0 ) =

1 X
Gk b (ρ, Gρ0 ).
|G| G∈G

(4.13)

Since there are at most 48 distinct group elements in a crystal points group (the
order of the full O48 group), the discrete covariant summation remains computationally feasible in bulk systems. In the particular case of one-dimensional
systems, where the only symmetry operation available other than the identity
is the inversion, equations (4.12) and (4.13) are formally equivalent.
The procedure described, summarised by the last line of Eq.(4.12) and by
Eq. (4.13), can be considered the vectorial counterpart of the more standard
transformation integration procedure we have used to build scalar energy kernels in the last chapter. The last chapter has also shown that such a procedure
can be avoided altogether in the case of local energy kernels as symmetric kernels can be defined directly on invariant degrees of freedom. The same line of
reasoning does not hold in the case of matrix-valued covariant kernel as designing suitable covariant descriptors is arguably harder than finding invariant
ones. For this reason, the automatic procedure to build covariant descriptors
just described can be imagined to be relatively more useful.

4.4. Building covariant Kernels

57

In facts, covariant kernels build as detailed above have recently been also
used recently to learn other physical tensors that are also covariant upon rotations [14, 47].

4.4 Building covariant Kernels
In the previous chapter translation and permutation invariant kernels were
successfully developed (see e.g., the 2-body kernel (3.2), its n-body generalisation (3.3) and the many-body kernel (3.4)). These kernel will be used here
as base kernels, and the technique developed in the previous section will be
exploited to make those kernels covariant over the orthogonal group.
Systems with dimensions d = 1, 2, 3 are considered in the following three
subsections. The first two provide a useful a conceptual playground where the
features of “covariant learning” can be more easily visualised. The third one
benchmarks the method in real physical systems, simulated at the DFT level
of accuracy.

1D systems
The only relevant symmetry transformation in one dimension is the reflection
of a configuration through its centre. The covariant symmetrisation discussed
in the previous section (Eq. (4.13)) hence takes a very simple form
1
k D1 (ρ, ρ0 ) = (k b (ρ, ρ0 ) − k b (ρ, Fρ0 )).
2

(4.14)

where here and in the following Cn will denote the cyclic group of order n
and Dn the dihedral group (containing also reflections) of order 2n (C1 hence
indicating the trivial group). Note that k D1 is identically zero for inversionsymmetric configurations ρ or ρ0 whose associated forces must vanish.
A key feature of covariant kernels is the ability to enable “learning” of the
entire set of configurations that are equivalent by symmetry to those actually
provided in the database. For instance, the force acting on the central atom
at the origin of configuration ρ can be predicted even if only configurations ρ0
of different symmetry are contained in the database. In the simplest possible
system, a dimer, the only symmetry transformation maps configurations where
the central atom has a right neighbour (i.e. those for which the central atom

4.4. Building covariant Kernels

58

4

non cov.
cov.

Force (eV/Å)

3
2
1
0

−1
−2
−3
−4
−2.0

−1.5

−1.0 1.0

Position (Å)

1.5

2.0

Figure 4.1: Learning the force profile of a 1D LJ dimer using data (blue
circle) coming from one atom only. It is seen that a non covariant GP
(solid red line) does not learn the symmetrically equivalent force acting
on the other atom and it thus predict a zero force and maximum error.
If covariance is imposed to the kernel via Eq. (4.14) (dashed blue line),
then the correct equivalent (inverted) profile is recovered. Shaded regions
represent the predicted standard deviation interval in the two cases.

is the left atom in the dimer) onto configurations where the central atom has
a left neighbour.
The force field associated with a 1D Lennard Jones dimer is plotted in
Figure 4.1 (black solid line) as a function of a single signed number - the 1D
vector going from the central atom to its neighbour. The figure also shows
the predictions of an unsymmetrised 2-body base kernel using training data
coming from configurations centred on the left atom only (red solid curve).
This closely reproduces the true LJ forces in the region where the data are
available, and predicts the pure prior mean (i.e. zero) in the symmetry related
region, i.e. the left half of the figure. Meanwhile, because of the covariant
constraint (prior information) the GP based on the covariant kernel learns the
left part of the field by just reflecting the right part appropriately.
To further check the performance improvements of the covariant symmetrisation (4.14) the above comparison is now extended to the prediction of forces
associated with a 1D Lennard Jones 50-atom chain system, in periodic bound-

4.4. Building covariant Kernels

Figure 4.2: Learning Curves for a 1D chain of LJ atoms. The covariant
kernel (D1 ) learns twice as fast as the base one (C1 ).

ary conditions. A database of training configurations and an independent test
set of local configurations and forces were sampled from a constant temperature molecular dynamics simulation using a Langevin thermostat.
Figure 4.2 reports the average relative force error made by the GP process
on the test set as a function of training set size. It is immediately apparent
that the covariant kernel performance is comparable to that of the base kernel
with double the amount of data points for training. We will observe the same
effect also in 2 and 3 dimensions: symmetrising over a relevant finite group
of order |G| gives rise to an error drop approximately equivalent to a |G|-fold
increase in the number of training points. Since the computational complexity
of training GP is O(N 3 ), this can obviously lead to significant computer time
savings.

59

4.4. Building covariant Kernels

60

Figure 4.3: Learning curves for 2D triangular grid of LJ atoms. The larger
the symmetry group used to construct the kernel, the faster the learning,
provided that the lattice symmetry is captured.

2D systems
In two dimensions all rotations and reflections, as well as any combination of
these, are elements of O(2). This group can be represented by the following
set of matrices
O(2) = {R(θ) | θ ∈ (0, 2π]} ∪ {R(θ)F | θ ∈ (0, 2π]}
!
cos(θ) sin(θ)
R(θ) =
− sin(θ) cos(θ)

(4.15)

with F being any 2 × 2 reflection matrix.
The above decomposition makes the covariant integration (4.12) over O(2)
trivial once the matrix elements resulting from the integration over SO(2) have
been calculated. We next carry out the integration for the 2-body base kernel
of Eq. (3.2). This can be expressed as a sum of pair contributions, where the
first atom of the pair belongs to ρ and the second to ρ0 :
SO(2)

K

0

(ρ, ρ ) =

XZ
ij

SO(2)

−

dR R e

kri −Rr0j k2
2`2

.

(4.16)

4.4. Building covariant Kernels

61

Consistent with Eq. (4.12), only one atom of the pair is rotated during the
integration. The pairwise integrals in (4.16) are calculated in two steps. We
first define Rij to be the rotation matrix which aligns r0j onto ri , and then
−1
perform the change of variable R̃ = RRT
ij (and equivalently, R̃ = RRij )
yielding

kri −R̃Rij r0j k2
X Z
−
SO(2)
0
2`2
K
(ρ, ρ ) =
dR̃ R̃ e
Rij .
(4.17)
SO(2)

ij

Since the two vectors ri and Rij r0j are now aligned, each integral of Eq. (4.17)
can only depend on the two moduli ri and rj0 . The final result takes a very
simple analytic form (cf. Appendix A.7):
SO(2)

K

0

(ρ, ρ ) =

X

−

e

(ri2 +rj02 )
2`2


I1

ij

ri rj0
`2


Rij

(4.18)

where I1 is a modified Bessel function of the first kind. The kernel in (4.18) is
rotation-covariant by construction as can be seen immediately by comparison
with Eq. (4.3).
By exploiting the mentioned internal structure of the orthogonal group
(Eq. (4.15)), it is straightforward to show that the roto-reflection covariant
kernel is given by

1
KSO(2) (ρ, ρ0 ) + KSO(2) (ρ, Fρ0 )F ,
2

KO(2) (ρ, ρ0 ) =

(4.19)

which is the two-dimensional analog of equation (4.14). Interestingly, the
resulting kernel can be also cast in the more intuitive form (cf. Appendix A.8)
O(2)

K

0

(ρ, ρ ) =

X
ij

−

e

(ri2 +rj02 )
2`2


I1

ri rj0
`2



r̂i r̂0T
j ,

(4.20)

where the hat denotes a normalised vector. Equation (4.20) implies that the
predicted force on an atom at the centre of a configuration ρ will be a sum
of pairwise forces oriented along the directions r̂i connecting the central atom
with all its neighbours (while each neighbour will experience a corresponding reaction force). The modulus of these forces will be a function of the
interatomic distance completely determined by the training database, whose
integral can be thought of as a pairwise energy potential. Clearly then, the
resulting force field will be conservative: for any fixed database, the forces pre-

4.4. Building covariant Kernels
dicted by GP inference using this kernel will do zero work if integrated along
any closed trajectory loop in configuration space.
Covariant integration of a 3-body kernel can also be performed analytically,
using a change of variable similar to that of Eq. (4.17) (cf. Appendix for this
calculation). In this case, the resulting covariant 3-body kernel does not give
rise to conservative force fields but it can give rise to much more accurate
forces, which will be approximately conservative as this property is inherited
by the reference quantum model.
The relative performance of the 2-body covariant kernels is tested against
training and test databases sampled from a two-dimensional, 48-particle triangular lattice in periodic boundary conditions with Lennard-Jones interactions,
kept at constant temperature via a Langevin thermostat. As the chosen lattice
has three-fold and six-fold symmetry, we can also examine the performance of
covariant kernels that obey the two properties described above restricted to
appropriate finite groups; these kernels are constructed as in equation (4.13).
So that we can monitor how imposing a progressively higher degree of symmetry on the kernel changes the rate at which forces in this system can be
learned.
These results are reported in Figure 4.3. As anticipated, the discrete covariant summation over the elements of a group G is observed to be approximately
equivalent to a |G|- fold increase of the number of data points. This can be
seen e.g. from the results for the C3 kernel (3-fold rotations) and the C6 kernel
(6-fold rotations), by comparing the error incurred in the two cases using 20
and 10 datapoints, respectively. More generally, we observe that the larger
the group, the faster the learning. Note, however, that for the covariant summation (4.13) to extract content from the database that is actually useful for
predicting forces in the test configurations at hand, the group used must describe a true underlying point symmetry of the system. Hence, for instance,
the C4 kernel gives rise to much slower learning than the C3 kernel for the
2D triangular lattice examined. Consistently, for this lattice the full point
group D6 performs almost as well as the continuous symmetry kernels, suggesting that not much more is to be gained once the symmetry of the full main
(finite-group) symmetry of a system has been captured. This finding enables
accurate force prediction in crystalline system when base kernels are used for
which the covariant integration cannot be performed analytically, because the

62

4.4. Building covariant Kernels

Figure 4.4: Learning Curves for crystalline nickel at two target temperatures. The SO(3) covariant kernel (full lines) outperforms the base
one(dashed lines).

summation over a discrete symmetry group is available as a viable alternative.

3D systems
As in the two dimensional case, the covariant integration of the 2-body base
kernel is performed analytically. After expressing the integration as a sum of
pairwise integrals, the position vectors ri and r0j of two atoms in each pair
are aligned onto each other. A convenient way to achieve this is by making
both vectors parallel to the z-axis with appropriate rotations Rzi and Rzj . As
before, the covariant integration will yield a matrix whose elements are scalar
functions of the radii ri and rj0 only. The integration over the rotation group
(SO(3) in three dimensions) can be carried out analytically over the standard
three Euler angle variables (cf. Appendix A.7 for further details). Due to the
z-axis orientation, the elements turn out to be all null except for the zz one.

63

4.4. Building covariant Kernels

64

The result reads


0
0
0
X


KSO(3) (ρ, ρ0 ) =
RzT
0  Rzj ,
i 0 0
ij
0 0 φ(ri , rj0 )
φ(ri , rj ) =

e−βij
(γij cosh γij − sinh γij )
γij2

(4.21)

ri2 + rj02
,
2`2
ri rj0
γij = 2
`

βij =

As in the two dimensional case, the covariant kernel matrix can be rewritten
only in terms of the unit vectors r̂i and r̂0j associated with the atoms of the
configurations ρ, ρ0 as
KSO(3) (ρ, ρ0 ) =

X

φ(ri , rj0 )r̂i r̂0T
j ,

(4.22)

ij

making it apparent that the kernel models a pairwise conservative force field
(the steps from (4.21) to (4.21) are provided in Appendix A.8). However, while
in two dimensions we needed to impose the full roto-reflection symmetry in
order to obtain equation (4.20), rotations alone are sufficient to arrive at the
fully covariant kernel in (4.22). This is a consequence of the fact that, in 3D,
the covariant integral over rotations already imposes that the predicted force
any atom will exert on any other is aligned along the vector connecting the
pair: by symmetry there can be no preferred direction for an orthogonal force
component after integrating over all rotations around the connecting vector,
so that KcO(3) = KSO(3) . This is not the case in 2D where covariant integration
is over rotations around the z-axis orthogonal to all connecting vectors lying
in the xy plane, so that non-aligned predicted force components associated
with a non-zero torque are not forbidden by symmetry in KSO(2) , and only
the fully symmetrised kernel (4.19) will reduce to the pairwise form (4.20).
More generally we may conjecture that the rotationally covariant kernel KcO(d)
derived from a linear base kernel predicts pairwise central forces, and hence is
conservative, in any dimension d.
Note that energy conserving kernels can be obtained as double derivatives

4.5. Tests on real materials

Figure 4.5: Density of relative error made by the GP algorithm (N =
320) for bulk nickel at 500K. The inset shows the scatter plot of real vs.
predicted component for the same data.

(Hessian matrices) of scalar energy kernels as described in Chapter 2 Section
2.4. This more standard method was originally described in [42, 66] and was
first used for atomistic systems in [6] to learn energies (later also used in [27]
to learn forces). However, no analytic energy kernel forms exist that would
yield our O(d) energy conserving kernels through this route, since the double
integration of the kernel expressions (4.14,4.20,4.22) cannot be carried out
analytically.

4.5 Tests on real materials
In this section the accuracy of covariant kernels is benchmarked in predicting
DFT forces in three-dimensional bulk metal systems. The test database was
constructed by performing DFT-accurate dynamical simulation with exchange
and correlation energy modelled via the PBE/GGA approximation [76]. The
systems considered were 4 × 4 × 4 supercells of fcc nickel and bcc iron in
periodic boundary conditions. A weakly coupled Langevin thermostat was
used to control the temperature. We first examine bulk nickel at the target
temperatures of 500K and 1700K i.e., for an intermediate temperature where

65

4.5. Tests on real materials
anharmonic behaviour is already significant, and at a temperature close to the
melting point where the strong thermal fluctuations make the system explore
a more complex target configuration space.
Figure 4.4 illustrates the performance of the kernel in Eq. (4.21) on this
system. The effect of adding symmetry information on the learning curve is
very significant for both temperatures. In particular, the O(3) covariant kernel
achieves a force error average lower than the 0.1eV/Å threshold using remarkably few training points: 10 and 80 for the lower and higher temperatures
in this test, respectively. The errors of the most accurate models (achieved
with a N = 320 database) are particularly low: 0.0435(±0.0006)eV/Å and
0.095(±0.003)eV/Å respectively. 2
Figure 4.5 allows inspecting the accuracy of the GP predictions in a complementary way: here we plot the probability distribution of the atomic forces as
a function of the force modulus (blue histogram) and the associated relative er|
p(f ),
ror density (grey histogram). The latter is here defined as RED(f ) = |∆f
f
which is normalised to 0.055, reflecting the 5.5% average relative error incurred
by force prediction. The fact that RED(f ) is everywhere a small fraction of
p(f ) demonstrates that a reasonable accuracy is achieved for the whole range
of forces predicted.
The results presented so far indicate that fully exploiting symmetry significantly improves the accuracy of force prediction. Covariance is thus always
used in the following analysis, where we compare the performance of different
symmetric kernels. We start by choosing iron systems for these tests as many
properties of iron-based systems remain out of modelling reach. This is mostly
due to technical limitations. On the one hand full DFT calculations on large
systems are too computationally expensive and even hybrid quantum-classical
(“QM/MM”) simulations of iron systems are typically overwhelmingly costly,
as they require large QM-zone buffered clusters to fully converge the forces
[17][Bianchini, Glielmo, Kermode]. On the other hand, in many situations
even the best available, state of the art classical force fields may not guarantee
accurate force prediction, as they may incur systematic errors [16, 17], or may
2

Note that the error on each force component (often reported in the literature,
and
√
different from the error on the full force vector used here) will be lower by a factor 3. This
yields errors of 0.025eV/Å and 0.052eV/Å in the two cases, the former comparing well with
the 0.09eV/Å value obtained by using a state of the art Embedded Atom Model (EAM)
interatomic potential for nickel [17, 73].

66

4.5. Tests on real materials

Figure 4.6: Learning curves associated with force prediction by the linear
(L, dashed lines) and squared exponential (SE, solid lines) covariant kernels
in bulk iron systems. Red and blue colours indicate undefected systems and
model systems containing a vacancy, respectively.

be hard to extend to complex chemical compositions [108], so that a technique
that can indefinitely re-use all computed QM forces via GP inference and produce results that are traceably aligned with DFT-accurate forces could be very
useful [65, 99].
Two bcc iron systems are here considered - both kept at constant temperature (500K) with a Langevin thermostat: a 64-atom crystalline system
and a 63-atom system derived from this and containing a single a vacancy.
In the latter, only the atoms within the first two neighbour shells of the vacancy were used to test the algorithm, to better resolve the performance of
our kernels in a defective system. Figure 4.6 shows the learning curves for the
two symmetrised kernels: the 2-body kernel covariant over O(3) (4.20) and
the many-body squared exponential kernel (3.4) made covariant over the full
cubic point-group of the crystal using Eq. (4.13). The figure also reports the
performance of a high-quality EAM potential [69].
The trends of this figure are very similar to those already discussed in
Chaprer 3. Both kernels perform better than the EAM potentials in this test.
However, the error rate of the 2-body kernel (dashed lines) levels off to some

67

4.5. Tests on real materials
constant non-zero value that might or might not be satisfactory (depending
on the application), and will generally depend on the system being examined.
In bulk iron the error floor value is about 0.09eV/Å while in the vicinity of
a vacancy it is considerably higher (0.15eV/Å), suggesting that in spite of its
many attractive properties (e.g. fast evaluation, fast convergence, energy conservation), 2-body kernels of the form (4.22) are by no means complete, that
is, they often cannot capture and reproduce the entirety of the reference QM
physical interaction. In many situations, kernels capable of reproducing higher
order interactions could be needed to reach the target accuracy. This is exemplified by the much better performance of the many-body squared exponential
kernel (full lines in the figure) which yields higher accuracy, particularly for
the more complex vacancy system (about 0.05eV/Å and 0.075eV/Å for atoms
in the bulk and near the vacancy respectively). It is worth noting here that,
in general, conserving energy exactly by construction provides no guarantee of
higher force accuracy. For instance, in the case above, the squared exponential kernel delivers much more precise forces even though it conserves energy
only approximately. As the approximation will in any case improve with the
accuracy of the predicted forces, while no O(3)-invariant energy conserving
equivalent of this kernel has been proposed or appears viable, whether it is
preferable to use this kernel or a less accurate but energy conserving alternative one, will generally depend on both the target system and the application
at hand.
For target systems with no clear point symmetry, a full covariant integration would always be desirable. This cannot be carried out analytically for
the squared exponential kernel, to symmetrise which a discrete summation is
the only option. However, interactions beyond pairwise can be still captured
by e.g., the 3-body kernel. In contrast to the squared exponential kernel,
this is analytically tractable and our analysis reveals that a matrix-valued 3body kernel covariant over O(3) can be derived analytically (details of the
calculation can be found in Appendix A.7). The resulting model generates a
roto-reflection symmetric 3-body force field that can be expected to properly
describe non close-packed bonding, such as found e.g., in covalent systems.
Figure 4.7 illustrates the errors incurred by 2- and 3-body kernel while
attempting to reproduce the forces obtained during Langevin dynamics of
a 64-atom crystalline silicon system using Density Functional Tight Binding

68

4.6. Summary

Figure 4.7: Learning curves obtained for crystalline silicon using the linear
kernel (dashed lines) or the quadratic kernel (solid lines). Different colours
indicate different temperatures.

(DFTB) [36]. Both 2- and 3-body kernel are significantly more accurate than
a classical Stillinger Weber (SW) potential [97] fitted to reproduce the DFTB
lattice parameter and bulk modulus [65]. As already observed in Chapter 3,
due to its more restricted associated function space the 2-body kernel is the
one that learns faster, and would be the more accurate if only very restricted
databases had to be used. However, the 3-body kernel eventually performs
much better than the 2-body one for both investigated temperatures, 500K
and 1000K, in this covalent system, with errors of 0.05eV/Å and 0.1eV/Å in
the two cases, respectively, or approximately 4% and 6% of the mean force in
the two cases. These are very close to the minimum baseline locality error of
this system, associated with the use of a finite cutoff radius rc .

4.6 Summary
This chapter described a novel method to learn quantum forces on local configurations. This method is based on a vectorial GP and on the inclusion of
prior knowledge in a matrix valued kernel function. Section 4.2 showed how
to include rotation and reflection symmetry of the force in the GP process

69

4.6. Summary
via the notion of covariant kernels. Section 4.3 provided a general recipe to
impose this property on otherwise non-symmetric kernels. The essence of this
recipe lies in a special integration step, which we call covariant integration,
over the full roto-reflection group associated with the relevant number of system dimensions. In Section 4.4, this calculation was performed analytically
starting from a 2-body or a 3-body base kernel. When 2-body kernels are
made covariant, the resulting O(d) covariant kernels can be shown to generate
conservative force fields.
We furthermore tested covariant kernels on standard physical systems in
one, two and three dimensions. The one- and two-dimensional scenarios served
as playgrounds to better understand and illustrate the essential features of
such learning. The 3D systems (discussed in Section 4.5) allowed some practical benchmarking of the methodology in real systems. In agreement with
what physical intuition would suggest, incorporating symmetry consistently
gave rise to more efficient learning. In particular, if both database and target configurations belong to a system with a definite underlying symmetry,
restricting kernel covariance to the corresponding finite symmetry group delivers the full speed-up of error convergence with respect to database size. At
the same time this approach lifts the requirement of analytical integrability
over the full O(d) manifold, as the restricted integration becomes a simple
discrete sum over the relevant finite set of group elements. Testing on nickel,
silicon and iron (the latter both pure and defective) reveals that the present
recipes can improve significantly on available classical potentials.
SOMEWHERE BEFORE, IN A DISCUSSION SECTION: In general, kernels of order n > 2 may be needed for accurate force predictions in the presence of complicated interactions, e.g. in the study of plasticity or embrittlement/fracture behaviour of covalent or metallic systems. In particular, our
tests suggest that a fully O(3) covariant 3-body kernel can be used successfully
to improve the accuracy of force prediction in covalent materials. The results
presented reveal that force covariance is achievable without imposing energy
conservation to the kernel form. While both are desirable properties, lifting the
exact energy conservation constraint can sometimes yield higher force accuracy. For instance, no invariant local energy based kernel has been proposed for
the squared exponential (“universal approximator”) kernel, since the analytic
integration over O(3) is not viable. However, we find that covariance limited

70

4.6. Summary
to the O48 point group is very effective for force predictions in crystalline Fe
systems using this kernel (see Fig. 4.6). In general, while predicting forces
with high accuracy is the main motivation for machine learning-based work
in this field, the best compromise between accuracy, energy conservation and
covariance will depend on the specific target application. For instance, kernels
built from a covariant integration (or summation) that do not conserve energy
exactly should not be used as substitutes for conventional interatomic potentials to perform long NVE simulations, since they might in principle lead to
non-negligible spurious energy drifts. This is not a problem in NVT simulations, where a thermostat exchanges energy with the system to achieve and
conserve the target temperature, which will be able to compensate for any
such drift if appropriately chosen [60]. Furthermore, the same kernels will be
particularly suited for schemes that are in all cases incompatible with strict
energy conservation. These include the LOTF approach and any online learning scheme similarly involving a dynamically updated force model. They also
include any highly accurate and transferable scheme based on a fixed, very
large database where, to maximise efficiency, each force prediction only uses
its corresponding most relevant database subset. On the other hand, any usage style is possible for covariant kernels conserving energy exactly, such as
the covariant linear kernels of Eqs. (??), (??), and (??). In fact, the conservative pairwise interaction forces generated by these covariant linear kernels
can be easily integrated to provide effective “optimal” standard pairwise potentials for any application needing a total energy expression. We also note
that while the pair interaction form would still ensure very fast evaluation of
the predicted forces, its accuracy for complex systems could be improved by
dropping the transferability requirement of a single pairwise function. In such
a scheme, different system regions could conceivably be modelled by locally
optimised forces/potentials, where the local tuning could be simply achieved
by restricting the inference process to subsets of the database pertinent to each
target region.

71

Chapter 5

Selecting the optimal kernel
5.1 Introduction
Through the previous chapters, this thesis has introduced several GP models
to infer a classical force field starting from a dataset of quantum calculations.
These differed in the type of kernel function used which, to give some examples,
could be one of the n-body energy kernels proposed in Chapter 3 or one of
the covariant matrix-valued kernels of Chapter 4. Many more energy of force
kernels have been proposed in the literature, some of the most notable ones
being the SOAP kernel [8], the coulomb matrix kernel [87] or the bag of bonds
[49]. Moving out of the realm of GP regression, other fitting algorithms have
been proposed based on neural networks [13], generalised linear models [92],
not to mention the many available classical parametrised models. With the
above list of methods being surely incomplete, the problem of selecting a single
model both interesting and unavoidable.
The No Free Lunch (NFL) theorems proven by D. H. Wolpert in 1996 state
that no learning algorithm can be consider better any other (and than random
guessing) in a general sense [29]. This remarkable result seems to suggest that
the best among competing models has to be chosen in relation to the particular
system studied and and the dataset available. An attractive approach lies in
the long-standing Occam’s razor principle and select the simplest model that
is still able to provide a satisfactory explanation [43, 59, 79]. Hence, in the
context of force field learning, one should incorporate as much prior knowledge
as is available on the function to be learned and the particular system at hand.
When prior knowledge is not enough to decide among competing models, these

5.1. Introduction

73

1.75

simple model
complex model

1.50

y

1.25
1.00
0.75
0.50
0.00

0.25

0.50

0.75

1.00

1.25

x
Figure 5.1: A simple linear model (blue solid line) and a complex GP model
(green dashed line) are fitted to some data points. In this situation, if we
have prior knowledge that a linear trend underpins the data, we should
enforce the blue model a priori; otherwise we should select the blue model
by Occam’s razor after the data becomes available, since it is the simplest
one. The advantages of this choice lie in the greater interpretability and
extrapolation power of the simpler model.

should all be trained and tested, after which the simplest one that is still
compatible with the desired target accuracy should be selected. This approach
is illustrated in Figure 5.1, where two competing models are considered for a
one dimensional data set.
Such a principle has driven the heuristic based method of model selection
suggested by the analysis of Chapter 3 (Section 3.4), where the best model
could be considered to be the simplest kernel compatible with a given target
accuracy. However, the mentioned method is not theoretically sound and fails
when the set of kernels (or more generally the set of models) considered cannot
be clearly ranked in terms of their complexity.
This chapter details how the theory of Bayesian model selection embodies
the Occam’s razor principle and how it can provide a rigorous approach to
the selection of force fields models, specifically GP-based ones. Section 5.2
deals with the general theory of Bayesian model selection and its relation to
the Occam’s razor principle. Section 5.3 details the specific way in which the

5.2. Theory of Bayesian model selection

74

general theory can be applied in practice for selecting the order n of an n-body
kernel. The analysis, carried out in either one or three dimensional systems,
reveals that low order kernels are advantageous to use not only for physical
systems of low complexity but for any system when the available database is
small or moderate in size. The advantages of choosing a low order model also
include more physically driven reasons, these are illustrated in Section 5.4,
where some ideas for circumventing the representation power limitations of a
low order force field are also presented.

5.2 Theory of Bayesian model selection
Let us assume that we want to select a single model out of the set {Mθn } (each
e.g., defined by a kernel function of given order n). Each model is equipped
with a vector of hyperparameters θ, (in our context this will be associated
with the covariance lengthscale `, the data noise level σn , and similar). The
model one should select in a Bayesian framework is that with the largest
posterior probability, conditioned on a given set of reference calculations D =
{(εri , ρi )}N
i=1 . This can be formally written down using Bayes’ theorem as
p(Mθn | ρ, εr ) =

p(εr | ρ, Mθn )p(Mθn )
.
p(εr | ρ)

(5.1)

However, often little a priori information is available on the candidate models
and their hyperparameters (or it is simply interesting to operate a selection
unbiased by priors, and “let the data speak”). In such a case, the prior p(Mθn )
can be ignored as being flat and uninformative, and maximising the posterior
becomes equivalent to maximising the marginal likelihood p(εr | ρ, Mθn ) (here
equivalent to the model evidence 1 ), and the optimal selection tuple (n, θ) can
be hence chosen as
(n̂, θ̂) = argmax p(εr | ρ, Mθn ).

(5.2)

(n,θ)

The marginal likelihood is an analytically computable in the case of GP models,
being the normalised multivariate distribution given by Eq. (2.6).
1

The model evidence is conventionally defined as the integral over the hyperparameter
space of the marginal likelihood times the hyperprior (cf. [110]). We here simplify the

Marg. lik., p(r | ρ, Mn )

5.2. Theory of Bayesian model selection

n̂ = 3

75

M2
M3
M4

r0

Possible reference energies, r
Figure 5.2: Cartoon of the marginal likelihood profile of three models of
increasing complexity. More complex models can fit very different datasets
εr , this is illustrated by the fact that their marginal likelihood is non-zero
for a broader region of the dataset space (here pictorially one dimensional).

The maximisation in Eq. (5.2) can be thought of as a Bayesian formalisation of the Occam’s razor principle. This is illustrated in Figure 5.2, which
contains a cartoon of the marginal likelihood of three models of increasing
complexity/flexibility (a useful analogy is to think of polynomials Pn (x) of increasing order n, the likelihood representing how well these would fit a set of
measurements εr of an unknown function ε(x)). By definition, the most complex model in the figure is the green one, as it assigns a non-zero probability
to the largest domain of possible outcomes, and would thus be able to explain
the widest range of datasets. Consistently, the simplest model is the red one,
which is instead restricted to the smallest dataset range (in our analogy, a
straight line will be able to fit well fewer data sets than a fourth order polynomial). Once a reference database εr0 is collected, it is immediately clear that
the M3 model with highest likelihood p(εr | ρ, Mθn ) at εr = εr0 is the simplest
that is still able to explain it (the blue one in Figure 5.2). Indeed, the even
simpler model M2 is not likely to explain the data, the more complex model
M4 can explain more than is necessary for compatibility with the εr0 data at
hand, and thus produces a lower likelihood value, due to normalisation.
analysis by jointly considering the model and its hyperparameters.

2

n=2
n=3
n=4

0

−2
10

20

50 100 200

500 1000

Number of training points, N

(a) nt = 2

76

ln maxθ p( | ρ, Mθn )/N

4

ln maxθ p( | ρ, Mθn )/N

ln maxθ p( | ρ, Mθn )/N

5.3. Model selecting n-body kernels

n=2
n=3
n=4

4

2

0

−2

n=2
n=3
n=4

4

2

0

−2

10

20

50 100 200

500 1000

Number of training points, N

(b) nt = 3

10

20

50 100 200

500 1000

Number of training points, N

(c) nt = 4

Figure 5.3: Scaled log maximal marginal likelihood as a function of the
number of training points for different kernel models n and true interaction
orders nt .

5.3 Model selecting n-body kernels
The model selection methodology just described could be in principle used to
select the best within any set of GP kernel, which might contain indistinctly
scalar n-body kernels, matrix-valued covariant kernels as any other kernel not
treated in this thesis [8, 22, 49, 65, 87]. This section however focuses on the
restricted but representative class of scalar n-body kernels and tests Bayesian
model selection on the problem of selecting the order n given a set of target
calculations. It is instructive to first test analyse a simple one dimensional system with controllable interaction order, while real three dimensional materials
are analysed afterwords.

1D systems
The system considered here is a one dimensional chain of atoms interacting
via an ad hoc potential of order nt (t standing for “true”) (see Appendix
A.10 for more details on this model). For each value of nt , a database was
generated by random sampling of N configurations and associated energies
and the corresponding optimal lengthscale parameter `ˆ and interaction order
n̂ of the n-kernel in Eq. (3.3) were found by solving the maximisation problem
of Eq. (5.2). This procedure was repeated 10 times to obtain statistically
significant conclusions, the results were however found to be very robust in
as much as they they were found not to depend significantly on the specific
realisation of the training dataset.

4

N = 10
N = 100
N = 1000

3

2
2

3

True order, n
(a)

77

Model selected order, n̂

Model selected order, n̂

5.3. Model selecting n-body kernels

4
t

4

nt = 2
nt = 3
nt = 4

3

2
5

10

20

50

100 200

500 1000

Number of training points, N
(b)

Figure 5.4: Model selected order n̂ as a function of the true order nt (left)
and as a function of the number of training data points N (right).

The results are reported in Figure 5.3, which contains the logarithm of
the marginal likelihood maximised over the hyperparameter `, divided by the
number of training points N , as a function of N for different combinations
of true orders nt and kernel order n. The model selected in each case is the
one corresponding to the line achieving the maximum value of this quantity.
It is interesting to notice that, when the kernels order is lower than the true
order (i.e., for n < nt ), the maximal marginal likelihood can be observed to
decreases as a function of N (as e.g., the red and blue lines in Figure 5.3(c)).
This makes the gap between the true model and the other models increase
substantially as N becomes sufficiently large.
Figure 5.4 summarises the results of model selection. In particular, Figure
5.4(a) illustrates the model-selected order n̂ as a function of the true order nt ,
for different training set sizes N . The graph reveals that, when the dataset is
large enough (N = 1000 in this example) maximising the marginal likelihood
always yields the true interaction order (green line). On the contrary, for
smaller database sizes, a lower interaction order value n is selected (blue and
red lines). This is consistent with the intuitive notion that smaller databases
may simply not contain enough information to justify the selection of a complex

5.3. Model selecting n-body kernels

Scaled log marg. lik.

78

0
−10

bulk, N = 50
bulk, N = 200
cluster, N = 50
cluster, N = 200

−20
−30
−40

2-body

3-body

5-body

Kernel order
(a)

(b)

Figure 5.5: Panel (a): the two nickel systems used in this section as examples, with bulk fcc nickel in periodic boundary conditions on the left (red)
and a nickel nanocluster containing 19 atoms on the right (blue). Panel (b):
maximum log marginal likelihood for the 2-, 3- and 5-body kernels in the
bulk Ni (red) and Ni nanocluster (blue) systems, using 50 (dashed lines)
and 200 (full lines) training configurations.

model, so that a simpler one should be chosen. More insight can be obtained
by observing Figure 5.4(b), reporting the model selected order as a function
of the training dataset size for different true interaction orders. While the
order of a simple 2-body model is always recovered (red line), to identify as
optimal a higher order interaction model a minimum number of training points
is needed, and this number grows with the system complexity.

3D systems

The maximal marginal likelihood principled is here used to select the optimal kernel model for two nickel systems. The first is an undefected fcc crystal
in periodic boundary conditions (Figure 5.5(a) left) kept at a temperature
of 500K via a Langevin thermostat. The second is a nanocluster of 19 atoms
(Figure 5.5(a) right) simulated at 300K. More details on the datasets used are
available in Appendix A.6). Differently from the one dimensional toy model
just discussed, here there is no notion of “true” interaction order as a quantum
interaction is theoretically always a many body one. However, there certainly

5.3. Model selecting n-body kernels
is a notion of complexity of the physical interactions occurring, and more complex systems will require a model of higher interaction order. One can expect
that, in spite of the higher temperature fluctuations of the crystal, the greater
surface effects of the nanocluster could make the latter still the most complex
system and consequently the one for which a higher order is selected.
The set of models considered for this test {Mθ2 , Mθ3 , Mθ5 } comprises the
two body kernel (3.15), the 3-body kernel (3.15), and the 5 body kernel (3.18).
The hyperparameter vectors comprise prior noise σn and lenghtscale ` for each
candidate model θ = (σn , `). Similarly to what done for the one dimensional
system, in this experiment the noise hyperarameter was kept fixed to what
was a priori believed to be the intrinsic locality error of the forces while the
lengthscale parameter was separately optimised for each kernel and training
set. Figure 5.5(b) shows the results obtained in the form of a graph of the
maximal marginal likelihood as a function of kernel order, for the two materials
and for training sets of either 50 or 200 configurations. For the crystalline nickel
system (red lines) the marginal likelihood has a maximum at interaction order
n = 2 for both N = 50 (dahsed line) and for N = 200 (solid line). This
points at the optimality of a 2-body kernel for closed packed undefected nickel
system. Clearly, being able to reproduce forces in in the crystalline phase does
not guarantee accuracy for the same material in other circumstances. In facts,
in the case of the nickel nanocluster (blue curves) the value of the maximal
marginal likelihood of a 3-body kernel is slightly higher than the 5-body one
and substantially higher 2-body one, pointing at the inappropriateness of 2body modelling for such a system and at the optimality of a 3-body kernel.
The results presented so far suggest that lower order (simpler) models are
often selected over more complex ones. This happens not only when the underlying physical interaction is effectively of low order, but also when there is
not sufficient data available to resolve a more complex model. Although not
immediately obvious, choosing a simpler model under these circumstances also
leads to smaller prediction errors on unseen configurations, since overfitting is
ultimately prevented as clear from e.g., the numerical tests shown in Chapter
3 (Figure 3.5).

79

5.4. The advantage of low order models

5.4 The advantage of low order models
The picture emerging from the observations made so far is one in which, although the quantum interactions occurring in atomistic systems will in principle involve all atoms in the system, there is never going to be sufficient data
to select/justify the use of interaction models beyond the first few terms of the
many-body expansion (or any similar expansion based on prior physical knowledge). Furthermore, using low order models presents strong advantages which
cannot be ignored even when very large datasets are available. Indeed, putting
aside their greater interpretability (which however can be very useful for physically based validation), low order models very hardly undergo overfitting and
they hence naturally generalise better in unexplored regions of configuration
space (see e.g., the discussion in [34].
At the same time, in many likely scenarios, a realistic target threshold for
the average error on atomic forces (typically of the order of 0.1eV/A) will be
met by truncating the series at a complexity order that is still computationally manageable. Hence, in practice a small finite order model will always be
optimal.
This is in stark contrast with the original hope of finding a single manybody “universal approximator” model to be used in every context, which has
been driving a lot of interest in the early days of the ML-FF research field, producing for instance the reference methods [9, 13]. Furthermore, the observation
that it may be possible to use models of finite-order complexity without ever
recurring to universal approximators suggests alternative routes for increasing
the accuracy of GP models without increasing the kernels’ complexity. These
are worth a small digression.
Imagine a situation as the one depicted in Figure 5.6, where we have an
heterogeneous dataset composed of configurations that cluster into groups.
This could be the case, for instance, if we imagine collecting a database which
includes several relevant phases of a given material. Given the large amount
of data and the complexity of the physical interactions within (and between)
several phases, we can imagine the model selected when training on the full
dataset to be a relatively complex one. On the other hand, each of the small
datasets representative of a given phase may be well described by a model of
much lower complexity. As a consequence, one could choose to train several

80

Collective variable 2

5.4. The advantage of low order models

81

Nanocluster

Defected crystal
Liquid

Gas

Amorphous solid
Perfect crystal

Collective variable 1
Figure 5.6: A an illustrative representation of an heterogeneous database
composed of configurations which “cluster” around specific centroids in an
arbitrary two dimensional space. The different clusters can be imagined to
be different phases of the same material.

GPs, one for each of the phases, as well as a gating function p(c | ρ) deciding,
during an MD run, which of the clusters c to call at any given time. These GPs
learners will effectively specialise on each particular phase of the material. This
model can be considered a type of mixture of experts model [57, 80], and heavily relies on a viable partitioning of the configuration space into clusters that
will comprise similar entries. This subdivision is far from trivially obtained in
typical systems, and in fact obtaining “atlases” for real materials or molecules
similar the one in Figure 5.6 is an active area of research [31, 32, 44, 67].
However, another simpler technique to combine multiple learner could be that
of bootstrap aggregating (“Bagging”) [23]. Indeed, in our particular case this
could involve training multiple GPs on random subsections of the data and
then averaging them to obtain a final prediction. While it should not be expected that the latter combination method will perform better than a GP
trained on the full dataset, the approach can be very advantageous from a
computational perspective since, similar to the mixture of experts model, it
circumvents the O(N 3 ) computational bottleneck of inverting the kernel matrix in Eq. (2.7) by distributing the training data to multiple GP learners. The
algorithms described above and in general ML algorithms based on the use of
multiples learners belong to a broader class of ensemble learning algorithms
[88, 91].

5.5. Summary

5.5 Summary
Motivated by the need of choosing among competing models - including e.g,
those developed in Chapter 3 and 4 as well as among other potential candidates
from the the literature - this chapter was dedicated to the theory of Bayesian
model selection and its application to choosing an optimal kernel within a
set of potential candidates. After the necessary background was introduced,
leading to the principle of maximum marginal likelihood within GP regression,
model selection was carried out over the set of n-body kernels introduced in
Chapter 3. The procedure was first exemplified in a one dimensional toy model
of controllable interaction order and later applied to realistic DFT systems of
nickel atoms. In both cases, it was found that low order models were selected
not only when the systems’ interaction was effectively of low order, but also
when datasets of small or moderate sizes are available.
Choosing a low order (“simple”) model guarantees that the corresponding
GP does not overfit to the data and generalises well to unexplored regions of
configuration space, not present in the training database, making the resulting force field more robust and transferable. Furthermore, simple models are
more easily interpretable, in as mush as the trained force field can be readily
visualised and inspectioned.
In spite of the many attractive features of low order models, they suffer from
obvious limitations regarding their flexibility when compared to e.g., universal
approximators. To circumvent this problem, some recipes based on ensemble
learning ideas where suggested.

82

Chapter 6

Speeding up low n models
6.1 Introduction
Perhaps contrary to expectations, but perfectly in line with the lesson learned
for parametrised force fields, this thesis has shown that models of low interaction oder are often optimal both in the Bayesian sense of having maximal
evidence (Chapter 5) and in the practical sense of providing the lowest error
on unseen configurations (Chapters 3, 4 and 5). The reasons for their success
were identified in the greater transferability and interpretability that low order
models offer when compared to higher or infinite order ones.
This chapter will further show that these models provide a very substantial
advantage also in terms of evaluation time. Indeed, in spite of being orders of
magnitude faster than DFT calculations, GPs remain considerably slower than
standard parametrised force fields. This does not need to be the case when low
n kernels are used and this chapter explains how this computational gap can
be bridged. In particular, a “mapping” procedure is introduced and tested,
consisting in the storage and local interpolation the learned energy profile on
a grid of points on the relevant degrees of freedom of the n-body interaction.
Section 6.2 illustrates the idea behind mapping taking the first non-trivial
interaction order n = 3 as an example. In Section 6.3 the procedure is tested
iron and silicon systems, confirming that very substantial computational gains
can be achieved.

6.2. “Mapped” force fields

84

6.2 “Mapped” force fields
It is clear that once a GP kernel is recognised as being n-body, it automatically defines an n-body force field corresponding to it, for any given choice
of training set. This will be an n-body function of atomic positions, whose
values can be computed by GP regression sums over the training set as done
by standard ML-FF implementations, but do not have to be computed this
way. In particular, the execution speed of a machine learning-derived n-body
force field might be expected to depend on its order n (e.g., it will involve sums
over all atomic triplets, like any 3-body parametrised force fields, if n = 3),
but should otherwise be independent of the training set size.
It is therefore possible to construct a “mapping” procedure yielding a machine learning-derived, nonparametric force field (here called “MFF”) that
allows a very significant speed-up over calculating forces by direct GP regression. For convenience, the first non-trivial interaction order n = 3 is here
considered. It is first shown that a 3-body GP exactly corresponds to a classical 3-body MFF, and later explained how the mapping yielding the MFF can
be carried out, in this case using a 3D-spline approximator.
To explicitly reveal the the 3-body force field defined by GP predictions
using a 3-body kernel, we start by writing the GP predictive mean as
ε̂(ρ) =

N
X
X

k̃3 (qi1 ,i2 , qdj1 ,j2 )αd ,

(6.1)

d=1 i1 >i2 ∈ρ
j1 >j2 ∈ρd

where the general form of a 3-body kernel (Eq. (3.16)) was used here. Then,
by inverting the order of the sums over the database and atoms in the target
configurations we obtain the explicit expression for the otherwise implicit 3body potential:
ε̂(ρ) =

X

ε̃(qi1 ,i2 )

i1 >i2 ∈ρ

ε̃(qi1 ,i2 ) =

N
X
X

(6.2)
k̃3 (qi1 ,i2 , qdj1 ,j2 )αd .

d=1 j1 >j2 ∈ρd

Eq. (6.2) reveals that the GP defines the local energy of a configuration as a
sum over all triplets containing the central atom, where the function ε̃(qi1 i2 )

6.2. “Mapped” force fields
represents the energy associated with each triplet qi1 i2 = (ri1 , ri2 , ri1 i2 )T . The
triplet energy is calculated by three nested sums, one over the N database
entries and two running over the M atoms of each database configuration
(M may slightly vary over configurations, but can be assumed to be constant
for the present purpose). The computational cost of a single evaluation of
the triplet energy ε̃ in Eq. (6.2) scales consequently as O(N M 2 ). Clearly,
improving the GP prediction accuracy by increasing N and M will make the
prediction slower.
However, such a computational burden can be avoided, bringing the complexity of calculating the triplet energy ε̃ in Eq. (6.2) to O(1). Since the triplet
energy ε̃ is a function of just three variables (the effective symmetry-invariant
degrees of freedom associated with three particles in three dimensions), we can
calculate and store its values on an appropriately distributed grid of points
within its domain.
This procedure effectively maps the GP predictions on the relevant 3-body
feature space: once completed, the value of the triplet energy at any new target
point can be calculated via a local interpolation, using just a subset of nearest
tabulated grid points. If the number of grid points Ng is made sufficiently
high, the mapped function will be essentially identical to the original one but,
by virtue of the locality of the interpolation, the cost of evaluating it will not
depend on Ng .
In practice, a spline interpolation of the so-tabulated potential can be very
easily used to predict any ε̂(ρ) or its negative gradient f̂ (ρ) (analytically computed, to allow for a constant of motion in MD runs). The interpolation approximates the GP predictions with arbitrary accuracy, which increases with
the number of points Ng in the grid of tabulated values, as illustrated in next
section.
The formal generalisation of the developed technique to any finite order
n is straightforward provided that a good interpolator can be identified and
implemented. The computational speed of the resulting MFF can similarly be
expected to be independent of the number of training points N and to depend
linearly on the number of distinct atomic n-plets present in a typical atomic
environment ρ including M atoms plus the central one (this is the number of

M
= M !/(n − 1)!(M − n + 1)!, yielding e.g., M pairs and
combinations n−1

M
M (M − 1)/2 triplets). This would result in an overall N n−1
speedup factor

85

6.2. “Mapped” force fields
against a GP using an n-body kernel given by Eq. (3.16), as this would instead
scale linearly with N and quadratically with the number of n-plets present in
an environment.
In practice however, there are two major limitations to using this approach
for n > 3. Firstly, while mapping 2-body or 3-body predictions on a one
or three dimensional spline is straightforward, the number of values to store
grows exponentially with n, consistent with the rapidly growing dimensionality
associated with atomic n-plets. This makes the procedure very challenging
for higher n values which would require (3n-6)-dimensional mapping grids
and interpolation splines. Secondly, one should note that the evaluation time
of non-unique n-body kernels obtained as powers of an n0 -body input kernel
do not scale as the number of n-plets but only with the number of n0 -plets,
independently on n (as described in Section 3.3). This means that in practice,
high order GP kernels can be built as powers of a 3-body kernel as done
in Section 3.3 and the corresponding high order MFF would quickly become
slower to evaluate than the original GP.
On a brighter note, flexible 3-body force fields were shown to capture most
of the features for a variety of materials [34, 46, 100, 111]. Increasing the order
of the kernel function beyond 3 might be unnecessary for many systems (and it
could actually be advantageous to use a low-n model as discussed in Chapters
3,4 and 5). Hence, building extremely fast yet flexible and accurate 3-body
force fields could represent a “sweet spot” for many practical applications.
Moreover, GP predictions allow for a natural intrinsic measure of uncertainty
- the GP predicted variance σ̂ 2 (ρ) in Eq. (2.7 - and the same mapping procedure
used for the former can also be applied to the latter. Thus, like their GP
counterparts, and unlike parametric force fields, MFFs potentially offer a tool
that could be used to monitor whether any extrapolation is taking place that
might involve large prediction errors. However, since the predictive variance
depends on couples of n-plets its exact mapping is rather cumbersome already
for n > 2 and approximations need to be put in place that make it viable for
n = 3 (the interested reader is referred to Appendix A.11).

86

87

Fit (3.8Ng−0.76 )
Data

0.8
0.6
0.4
0.2
0.0
103

104

105

106

Number of grid points, Ng
(a)

MAE on GP force (eV/Å)

MAE on GP force (eV/Å)

6.3. Tests on real materials

100

10−1

10−2

10−3

Fit (3.8Ng−0.76 )
Data
103

104

105

106

Number of grid points, Ng
(b)

Figure 6.1: Error made by the MFF on GP forces predicted using the 3-body
GP kernel Eq. (3.15) as a function of the number of grid points used for the
spline interpolation (Ng ). The MFF was constructed on distances between
1.5Å and 4.5Å. Panels (a) and (b) show the same data on a linear-log or
log-log scale respectively.

6.3 Tests on real materials
The production and use of MFFs is here tested for two materials: Crystalline
iron in the presence of a vacancy and a cell of 64 atoms of amorphous silicon in
periodic boundary conditions (cf. Appendix A.6 for details on the datasets).
Figure 6.1 shows the convergence the mapped forces derived from the 3body kernel in Eq. (3.15), for the iron database. The interpolation is carried
out using a three dimensional cubic spline for different mesh sizes. Comparison
with the reference forces produced by the GP allows to calculate, for each
mesh size Ng , the mean error the MFF makes on the GP predicted forces.
This error is observed to diminish with a power law as a function of Ng . A
negligible accuracy loss with the respect to the original GP model is achieved
for Ng ∼ 106 corresponding to about 100 grid points for each spline dimension.
Since the potential was saved as a function of three distances, all going from a
minimum of 1.5Å to the maximum cutoff distance of 4.5Å, this gives a point
to point spacing of 0.03Å.
Depending on the specific reference implementation, the speed-up in calculating the local energy (Eq. (6.2)) provided by the mapping procedure can vary
widely, while it will always grow linearly with N and quadratically with M
(see Figure 6.2), and it will be always substantial: in typical testing scenarios

6.3. Tests on real materials

88

GP
MFF
∝ M4
∝ M2

4
3

Prediction time (s)

Prediction time (s)

5

2
1

GP
MFF
∝N
const.

400
300
200
100
0

0
10

15

20

25

Number of atoms, M
(a)

30

0

100

200

300

400

500

Number of training points, N
(b)

Figure 6.2: Computational cost of evaluating the 3-body energy (Eq. (6.2))
as a function of the database size N and the number of atoms M located
within the cutoff radius. Panel (a): time taken for a single energy prediction
using the GP (red solid line) and the mapped potential (blue dashed line),
as a function of M , for a training set of N = 5 configurations. Panel (b):
scaling of the same quantities as a function of N , for M = 24.

The system considered for this test is amorphous silicon.
we found this to be of the order of 103 − 104 .
Note that, while the procedure detailed here is based on the existence of a
previously trained GP model, one might envision to skip this intermediate step
and directly learn a parametric 3-body model on a grid points similarly arranged. This alternative approach is surely an attractive option which is worth
exploring further, but the following difficulties should be kept in mind. Firstly,
from the tests shown in Figure 6.1 it was found that with a local basis like the
three dimensional spline used here the number of points needed to match the
accuracy of the nonparametric GP is of the order of 105 . Any linear model
having that number of parameter would be very heavy to train (cubic cost
in the number of parameters) and even in the prediction phase (linear cost).
Secondly, while it is likely that an appropriately chosen basis could potentially
circumvent the first problem by requiring many fewer grid points, the problem
of finding such a basis is not a trivial one and needs to be addressed. The
MFF approach presented here, on the other hand, does not need to be based
on any given linear expansion. The current spline implementation is however
particularly advantageous since the locality of the interpolation guarantees an
extremely fast evaluation as only the 64 points closest to a given triplet need

6.3. Tests on real materials

89

0.25

Energy (eV)

0.00

2-body

−0.25
2.0

2.5

3.0

3.5

4.0

4.5

Distance (Å)
0.2

3-body, r1 = r2 = 2.4

0.0
80

100

120

140

Angle (deg)
Figure 6.3: Energy profiles of the 2- and 3-body MFF, trained for the aSi system at 650K. Upper panel: 2-body interaction term. Lower panel:
3-body interaction energy for an atomic triplet, angular dependence when
the two distances from the central atoms are both equal to 2.4Å.

to be taken into account for any energy or force calculation.
Figure 6.3 shows the MFF obtained for a system of amorphous Silicon.
As the 3-body kernel used for this test included also modelled 2-body contributions (being the sum of the two kernels in Eqs. (3.14) and (3.14)), the
corresponding MFF includes both types of interaction. As the energy profile is
not prescribed by any particular functional form, it is free to optimally adapt
to the information contained in the QM training set, to best reproduce the
quantum interactions that produced it. The potential contains some expected
features e.g., a radial minimum at about r ' 2.4Å in the 2-body section (upper
panel), the corresponding angular minimum at θ0 ' 110◦ (lower panel), which
is approximately equal to the sp3 hybridization angle of 109.47◦ , and rapid
growth for small radii (upper panel) and angles (lower panel). Less intuitive
features are also visible, which however contribute to the best representation
of the bulk system’s interactions that a 3-body expansion can achieve for the
given database. An example is the shallow maximum in the 2-body section
at r ' 3.1Å, which would of course disappear if we fitted our model on QM
forces calculated for a Si dimer, that do not contain a hump. The resulting
Si force field, appropriate for a Si dimer, would however inevitably reproduce
the QM bulk interactions less accurately. More generally, training on the

6.4. Summary
aggregate dataset could be a sensible compromise, producing a more transferable, but locally less accurate force field. Alternatively, an efficient strategy
for simulating complex systems with space- and time-varying bonding nature
might involve using (concurrently across the system, and at any given time)
the locally optimal choice of low-order MFF, similarly to “mixture of experts”
strategy suggested in Section 5.4 of the last chapter.

6.4 Summary
This chapter presented a simple and very effective method to substantially
speed up GPs’ evaluation time when using low order (practically 2- or 3body) kernels. The procedure is made possible by first revealing the n-body
nature of the GP. Once this is done, it is natural to recognise the n-body GP
predictions and store them on a grid of points of a 3n − 6 dimensional space
(one dimensional for n = 2) corresponding to the effective degrees of freedom
of n atoms. Subsequent calls to the force field can be then calculated by a
local interpolation of the stored grid points (e.g., using a cubic spline). GP
predictions computed this way are identical to the original ones but are much
faster as they do not involve lengthy sums over database entries or expensive
kernel evaluations.
The described method was here named “mapping” (and the resulting force
field was named “MFF”), as one can imagine GP predictions to be mapped
onto the effective degrees of freedom of n atoms. Building MFFs becomes
computationally impractical for n > 3 as the number of degrees of freedom
quickly grows with the number n of atoms considered. Fortunately, many
systems can be expected to be described very well by flexible 3-body force fields
and in this case the speedup achieved by building MFFs is very substantial.
To me more precise, the speedup in this case was predicted (and observed)
to be linear with number of training configurations N and quadratic in the
number of configuration atoms M . Within the reference implementation used
and assuming typical values of N and M , MFFs were found to be faster than
the corresponding GPs by factors of 103 − 104 , reaching the speed of very fast
standard parametrised force fields. Differently from parametrised force fields,
however, MFFs can be considered nonparametric and they can flexibly adapt to
the shape of the force field that best reproduces the quantum calculations they

90

6.4. Summary
are trained on. A particular MFF trained on amorphous silicon was shown, and
its inspection revealed expected as well as unexpected features, both enhancing
the final accuracy of the model. MFFs could be further improved by mapping
the variance of the GP (providing this way a measure of uncertainty associated
to their predictions), as well as by ensemble learning techniques consisting e.g.,
of an array of trained MFF each specialised on a given material phase.

91

Chapter 7

Gaussian process wave functions
7.1 Introduction
This chapter takes a step back away from the problems discussed in the rest
of the thesis as it does not deal with the GP modelling of energies or forces.
It can hence be read without a detailed knowledge of the other chapters, with
the exception of Chapter 2, which contains the essential background material
on GPs. Gaussian processes are here utilised to model the wave functions of
electrons. Electrons are fully quantum particles and, differently from classical ones, their state at any given time is described by a linear combination
of all possible configurations available to the system. The total number of
configurations grows exponentially with number of particles and this poses a
great challenge in modelling quantum system. This challenge is often called
the “quantum many body problem”, and the present chapter explores the possibility of facing it by representing the state of a quantum system compactly
with a Gaussian process.
Sections 7.2 contains the necessary background material quantum many
body physics and some of the methods available to tackle it. To ease the read,
the information provided is very minimal and the reader is directed to external
references for a more comprehensive exposition [2, 11]. The Hubbard model
is first described, a prototypical for strongly interacting electrons Later, the
Variational Monte Carlo method is outlined, a standard modelling approach
to quantum many body systems based on a parametric Ansatz for the target
wave function.
The Slater-Jastrow wave function is a particularly relevant Ansatz and

7.2. Hubbard model and Variational Monte Carlo

93

it is also briefly discussed. Finally, the an Ansatz based on a log-GP prior
is proposed in Section 7.3, along with a range of specifically designed kernel
functions. This is tested for the Hubbard model in Section 7.4. The results
are promising but many possible improvements can and should be pursued,
some of them are listed in Section 7.5.

7.2 Hubbard model and Variational Monte Carlo
The Hubbard Hamiltonian
Given an Hamiltonian operator Ĥ, the lowest energy state of the system |φ0 i
can be found by solving the time independent Schrödinger equation.
Ĥ|φi i = Ei |φi i

(7.1)

and selecting the lowest energy eigenstate.
Clearly the above eigenproblem can be expressed in an arbitrary basis as
long is that basis is complete. It is very convenient to work within the occupation number representation, in which bases are defined by the occupancies of
the available states of the system. For a one dimensional chain of electrons of
length L, basis states are defined by the vector |ni = |n1↑ n1↓ n2↑ n2↓ . . . nL↑ nL↓ i
It is a fundamental principle of quantum mechanics that fermionic states
need to be anty-symmetric upon the exchange of any two electrons and this
property is take into account by building these bases through the action of
creation operators c†is (s ∈ {↑, ↓}) on a vacuum state |0i where all lattice sites
are empty. These operators are defined by their action
c†is |0i = |nis i, c†js0 |nis i = |njs0 nis i

(7.2)

and by the commutation relation they must respect
{c†is , c†js0 } = 0, {cis , cjs0 } = 0, {cis , c†js0 } = δij δss0 .

(7.3)

where {a, b} is the anticommutator between two operators {a, b} = ab+ba, and
cis is the “annihilation operator”, conjugate transpose of the creation operator
cis = (c†is )† . It is useful to also define the “number operator” n̂i s = c†is cis

7.2. Hubbard model and Variational Monte Carlo

94

simply counting the number of electrons at site i with spin s (obviously either
zero or one by the Pauli exclusion).
The Hubbard Hamiltonian can be conveniently written in terms the the
operators just defined. Its simplest one dimensional form considered here reads
[37, 55]
L X
L
X
X
†
†
Ĥ = −t
(ci,s ci+1,s + ci,s ci−1,s ) + U
n̂i↑ n̂i↓
(7.4)
i=1

s

i=1

where L is the length of the one dimensional lattice and the indices of the
operators are intended to have modulo L for periodic boundary conditions to
apply.
The first term of the Hamiltonian, characterised by the parameter t > 0,
defines a simple non-interacting tight binding model where the hopping of an
electron to a neighbouring site is favoured by a factor t. The second one instead
introduces a repulsion between two electrons, this is done in a very essential
way by means of an on-site repulsion of strength U > 0. Both terms, taken on
their own, lead to simple Hamiltonians that can be treated analytically in the
thermodynamic limit. The one Hubbard model can also be treated analytically
in one dimension by means of the Bethe Ansatz [37], and this exact result will
be very useful for benchmarking. In general, the interplay of the two terms
in the Hubbard model in higher dimensions leads to a very rich, and still not
well understood, behaviour that is an active topic of research [33, 64, 82].

Variational Monte Carlo
A generic state can be defined as a linear combination of all possible state
vectors |ni = |n1↑ n1↓ n2↑ n2↓ . . . nL↑ nL↓ i . For later notational convenience it is
useful to define xi to be the tuple xi = (ni↑ , ni↓ ), which allows the bases to
be equivalently written as |xi = |x1 x2 . . . xL i. It is simple to see that for a
Hubbard system of L sites and with Ne↑ (Ne↓ ) spin up (down) electrons there


are NL↑ · NL↓ possible basis states {x} and a generic state can be written as
|ψi =

X

ψ(x)|xi,

(7.5)

{x}

where the expansion coefficient ψ(x) will be here called the wave function of
the state.

7.2. Hubbard model and Variational Monte Carlo

95

It is a basic principle of quantum mechanics that the energy of any state
E = hψ|Ĥ|ψi cannot be smaller than the ground state energy of the Hamiltonian E0 , and it can only be equal to it if the state is the ground state
|ψi = |φ0 i (Appendix/Citation?). This is called the variational principle of
quantum mechanics and it allows the calculation of the energy of a system
using the so called variational method summarised in the following. First a
suitable Ansatz is chosen for the given system, typically taking the form of
a wave function ψ η (x) providing the expansion coefficient for each term in
Eq. (7.5) and depending only on the few parameters contained in the vector η.
These parameters are then optimised by minimising the corresponding energy
and the lower is the optimal energy the better is the Ansatz chosen.
Optimising the energy would be impossible if that had to be calculated
exactly by summing over all states
E=

X
{x}{x0 }

=

X

ψ ∗ (x)ψ(x0 )hx|Ĥ|x0 i
ψ ∗ (x)Ĥxx0 ψ(x0 )

(7.6)

{x}{x0 }

Luckily, the total energy of the Hubbard Hamiltonian can be efficiently calculated by Monte Carlo sampling. This can be done since the above expression
can be rewritten as a classical average over the probability distribution given
by the square of the wave function amplitude p(x) = |ψ(x)|2
E=

X

ψ ∗ (x)Ĥxx0 ψ(x0 )

{x}{x0 }

=

X

ψ ∗ (x)

{x}{x0 }

=

X
{x}

|ψ(x)|2

ψ(x)
Ĥxx0 ψ(x0 )
ψ(x)

(7.7)

X Ĥxx0 ψ(x0 )
{x0 }

ψ(x)

= hEL (x)i|ψ(x)|2 .
In the last step of Eq.(7.7) the local energy has been defined as EL (x) =
P
Ĥxx0 ψ(x0 )
and it is immediately obvious that the total energy can be
0
{x }
ψ(x)
computed as a classical average of this quantity. The local energy can be calculated linear time with the size of the system. This can be seen from the fact

7.3. Gaussian process wave functions

96

that, although EL (x) formally involves a sum over the entire set {x0 }, only a
linear number of terms will be non-zero as Ĥxx0 will vanish everywhere else.
A Markov Chain [51] can easily be set to sample in linear time the probability function |ψ(x)|2 so that an estimate for the total energy can typically be
obtained in time proportional to L2 . In Variational Monte Carlo [96] the total
energy along with gradient information are calculated as given above and this
allows the optimisation of a parametric wave function ψ η (x).

Slater-Jastrow wave function
The Slater-Jastrow wave function is one of the earliest and most used wave
function Ansatzes [26, 35, 74, 104]. It is based on the idea that a simple
but nontrivial approximation to the exact ground state wave function can be
improved by explicitly accounting for important neglected interactions. In
the case of the Hubbard model, the Slater-Jastrow wave function corrects the
Slater determinant ψS (x) providing the solution to the tight binding Hamiltonian (first term of Eq. (7.4)), through an exponential function of the distance
between any two electrons (the Jastrow factor )
ψSJ (x) = e

P

ijs

ηij hx|n̂is n̂js |xi

ψS (x)

(7.8)

where the parameters in ηij are chosen in order to minimise the energy.

7.3 Gaussian process wave functions
A log GP Ansatz
The very same approach adopted by the Slater-Jastrow Ansatz and expand a
“GP wave function” (GP-WF) on top of the Slater determinant ψS (x) that
solves the tight binding Hamiltonian, with the expectation that the latter
will contain most of the sign information. However, countrary to standard
VMC Ansatzes, the modelled wave function ψ(x) will not depend on a set
of parameters but on a training data set. The starting assumption of the
model developed here is that the wave function is distributed according to a

7.3. Gaussian process wave functions

97

log Gaussian process, i.e.
ψ(x) = eλ(x) ψS (x)
λ(x) ∼ GP(0, k(x, x0 )).

(7.9)

This practically means that log wave function λ(x) can be modelled with the
standard GP regression techniques detailed in Chapter 2.
The model expressed in Eq. (7.9) can be trained on a database of reference
configurations-coefficient pairs D = {(xd , ψdr )}N
d=1 and the predictions coming
from the trained model take the form:
P

ψ̂(x) = e d k(x,xd )αd ψS (x)
X
αd =
(K + 1σn2 )−1
dd0 log
d0

ψdr0
.
ψS (xd0 )

(7.10)

The GP predicted wave function ψ̂(x) looks very similar to the parametric
Ansatz given in Eq. (7.8), with the key difference that the sum over parameters in the exponential function has been substituted by a sum over database
entries. This allows to model a much larger set of interactions since the computational cost of evaluating the wave function will not depend on their number.
As detailed next, a proper design of the kernel function allows modelling all
possible interactions occurring in the system in polynomial time.

Kernel functions
Plaquette kernels A very simple decomposition for the log wave function
λ can be imagined to be
X
λ̃1 (xi )
(7.11)
λ1 (x) =
i

where λ̃(xi ) is a GP that now depends on the state of a single site. The kernel
function corresponding to this one-site decomposition is trivially obtained as
k1 (x, x0 ) =

X
hλ̃1 (xi )λ̃1 (x0i )i
ij

=

X
ij

(7.12)
δxi x0j

7.3. Gaussian process wave functions

98

where the choice of a delta correlation hλ̃1 (xi )λ̃1 (x0i )i = δxi x0j is very sensible in
the discrete space considered here. The above reasoning can be generalised to
model interactions involving an arbitrary number n of nearest neighbour sites.
The decomposition of the log wave function in this case takes the form
λn (x) =

X

λ̃n (xi , xi+1 , . . . , xi+n−1 )

(7.13)

i

and a the corresponding n-site kernel is simply written as
kn (x, x0 ) =

X

δxi x0j δxi+1 x0j+1 . . . δxi+n−1 x0j+n−1

(7.14)

ij

The n-sites kernels can be understood - and extended to higher spatial dimensions - in terms of plaquettes. The kernel in Eq. (7.14) can indeed be imagined
to scan through the configurations x and x0 with with a “plaquette” of size
n and a given shape (a line in one dimension) counting the number of times
identical plaquettes are found.
Distance dependent kernels It is apparent that a n-site kernel with n = L
would be able to represent any interaction occurring in the system. Such a
description would not be, however, a very useful one as the resulting GP will
simply look for the coefficient with matching configuration in the database and
predict that, with no hope of interpolation or extrapolation. A better model
should allow for the possibility of selecting physically relevant interactions.
For instance, the interaction of two sites at a given distance might be more
important than that of four consecutive sites. This objective could be achieved
by relaxing the nearest neighbour constraint in the decomposition (7.13) and
choosing plaquettes of arbitrary, physically based shapes. Albeit interesting
and worth exploring, this approach suffers from the obvious drawback of having
to choose “by hand” the relevant interactions modelled. A different route is
here proposed.
In the spirit of the Slater-Jastrow wave function, the following 2-site decomposition for the log coefficient can be envisioned
λd2 (x) =

X
i1 i2

λ̃d2 (xi1 , xi2 , ∆i1 i2 )

(7.15)

7.3. Gaussian process wave functions

99

where ∆i1 i2 is the relative position (signed distance in one dimension) of site
i2 with respect to i1 . Using a standard squared exponential kernel to learn
functions on this distance, the kernel corresponding to the above decomposition
can be written down as
k2d (x, x0 ) =

2

X

δxi1 x0j δxi2 x0j e−(∆i1 i2 −∆j1 j2 )/2` .
1

2

(7.16)

i1 i2 j1 j2

The cost of evaluating the kernel in Eq. (7.16) scales as L4 , which can quickly
become computationally heavy. However, this cost does not increase when
increasing the interaction order further. For instance, the next term in the
series k3d can also be brought down from L6 to L4 by implicitly representing
higher 3-site interactions as the square of 2-sites ones
!
k3d (x, x0 ) =

X

X

δxi1 x0j

1

X

δxi2 x0j

2

i1 j1

i2 j2

X

X

δxi3 x0j e−(∆i1 i2 −∆j1 j2

)/2`2

3

e−(∆i1 i3 −∆j1 j3

)/2`2

i3 j3

!2
=

δxi1 x0j

1

i1 j1

δxi2 x0j e−(∆i1 i2 −∆j1 j2

)/2`2

2

.

i2 j2

(7.17)
The above kernel gives rise to the decomposition
λ(x) =

X

λ3 (xi1 , xi2 , xi3 , ∆i1 i2 , ∆i1 i3 )

(7.18)

i1 i2 i3

and, since ∆i1 i2 and ∆i1 i3 are signed distances, the function λ̃3 is a unique
function of the states of three sites and their relative position. It is now an
immediate step to define a generic n-site distance dependent kernel as
!n−1
knd (x, x0 ) =

X

δxi1 x0j

X

1

i1 j1

δxi2 x0j e−(∆i1 i2 −∆j1 j2
2

)/2`2

(7.19)

i2 j2

The scaling L4 evaluation cost of distance dependent kernels can be reduce by exploiting the discontinuous nature of the relative positions ∆ij in
the lattice. This allows the use of a delta correlation instead of a squared
exponential one supposedly without a significant accuracy loss, which in turn

7.4. Tests on the Hubbard model

100

allows a reduction of the computational cost to L3 by writing
!2
knd (x, x0 ) =

X

δxi1 x0j

X

1

δxi2 x0j δ∆i1 i2 ∆j1 j2
2

i1 j1

i2 j2

X

X

(7.20)

!2
=

δxi1 x0j

i1 j1

δxi1 +∆ x0j

1 +∆

1

∆

where from the first to the second line the delta function constraint over the
relative position is enforced by summing directly over the displacements. [Cite
the quadratic scaling achievable for local updates, cite Appendix]
Complete kernel A “complete” kernel, modelling the interaction of any
number of sites at any distance, can at this point be defined as
!L−1
kcθ (x, x0 ) =

X

δxi1 x0j

1

i1 j1

1 + e−1/θo

X

δxi1 +∆ x0j

1 +∆

e

−∆2 /2θd2

.

(7.21)

∆

One can better visualise the interaction modelled by this kernel by imagining
to expand the binomial in brackets. In facts, using the standard formula
Pp p i
(1 + a)p =
i=1 i a it is clear that all n-site distance dependent kernels
knd are present, with n going from one up to the full length L of the lattice.
Interaction involving i+1 sites are super-exponentially suppressed by the factor

L−1 −i/θo
, and the hyperparameter θo can be used to control this dumping.
e
i
Eq. (7.21) also includes a Gaussian dumping on the site to site distance ∆,
controlled by the hyperparameter θd .

7.4 Tests on the Hubbard model
Testing size extensitivity
The kernels defined so fat, combined with the log linear model of Eq. (7.9)
have the property of inferring wave functions that are extensive with the configuration size. This means that in principle one could train a GP on a small
system, for which exact results can be easily calculated, and use that to model
a much larger - near thermodynamic - system. The amount of residual fi-

7.4. Tests on the Hubbard model
nite size effects can be imagined to depend both on the particular parameters
chosen and on the quality of the GP representation.
The following experiments were run to test on this ideas. The exact wave
functions of a 6-site and a 8-site systems were first obtained by exact diagonalisation [98] and GP-WF models were fitted on them as given by Eq. (7.10).
These trained model were then used to predict the energy of systems of up to
50 sites by the Monte Carlo sampling explained in Section 7.2. Since we look
at one dimensional lattices, exact results are also available for benchmarking.
In particular, the thermodynamic energy can be calculated analytically using
the Bethe Ansatz [37] while the energy for finite size systems can be computed
in polynomial time using the Density Matrix Renormalisation Group (DMRG)
[75, 89, 109] method.
Figure 7.1 reports the results of these experiments by graphing the achieved
energy per site as a function of the size of the lattice for U = 2 and for U = 8.
All the tests presented here were run with t = 1 and at half-filling, meaning
that the total number of electrons equals the lattice size and no net magnetisation is present. The figure shows that GP-WFs fitted on small systems are
able to represent surprisingly well the the wave function of larger systems as
they can capture most of the correlation energy (defined as the difference between the mean field and the exact one). In particular, as clear from the first
column (panels (a) and (c)) virtually all GP models very significantly improve
on the baseline Ansatz ψS (solid brown curve). A more detailed picture can
be captured from the second column (panels (b) and (d)), in which the relative error on the correlation energy (difference between baseline and exact) is
plotted. The following trends can be observed.
Firstly, the error initially increases with system size before plateauing at
some generally small value. As one would expect, the final error achieved
is smaller for larger plaquette sizes when using simple kn kernels. Perhaps
surprisingly, the k3 kernel trained on 6-site data does as well as the more
complex complete kernel kc for this system. This is probably due to the fact
that only L/2 sites are completely explored in an L-site system at half filling
(going from all empty to the all occupied sistes) so that not much is to be
gained by going to larger plaquette sizes. The kc has however the important
property of being able of exactly reproducing the wave function on the original
training data (zero error on the 6-site system), property that is absent in kn

101

102

Relative error on corr. en.

Energy per site, E/L

7.4. Tests on the Hubbard model

−0.775
−0.800
−0.825
−0.850

ψS
k1

−0.875
−0.900
10

20

k2
k3
30

kc
k4
40

0.125
0.100
0.075
0.050

k1
k2

0.025

10

Relative error on corr. en.

Energy per site, E/L

0.4

k2
k3

kc
k4

0.0
−0.2
20

30

Sistem size, L
(c) U = 8

30

40

50

(b) U = 2

0.6

10

20

Sistem size, L

(a) U = 2

0.2

k4

0.000

50

Sistem size, L

ψS
k1

k3
kc

40

50

0.150

k1
k2

0.125

k3
kc

k4

0.100
0.075
0.050
0.025
0.000
10

20

30

40

Sistem size, L
(d) U = 8

Figure 7.1: The left column (insets (a) and(c)) reports the energy per site
obtained by the different models as a function of L, the energy of the baseline
Ansatz (ψS ) and the that of DMRG (black, dashed-dotted line) are also
shown. The right column (insets (b) and (d)) reports the relative errors on
the correlation energy. All the models were fitted on 6-site data apart from
the k4 kernel (orange dashed line), which was trained on 8-site data.

50

Bethe Ansatz
ψS

1.5
1.0

ψ̂

0.5
0.0
−0.5
−1.0
0

5

103

Energy per site, E/L

Energy per site, E/L

7.4. Tests on the Hubbard model

10

−0.2
−0.4
−0.6
−0.8

Bethe Ansatz
ψS

−1.0

ψ̂

−1.2
0

Interaction strength, U
(a)

5

10

Interaction strength, U
(b)

Figure 7.2: Energy per site as a function of the interaction strength U . The
GP-WF model ψ̂ was trained on 8-site data with a k4 kernel.

kernels. Finally, training on a larger initial yields a lower final energy as finite
size effects are obviously less prominent.
The quality of the GP Ansatz with an n-site kernel is benchmarked in
Figure 7.2 for several values of the interaction strength U . The figure reports
the energy per site of a 32-site system achieved by a 4-site kernel GP trained
on the 8-site wave function, as a function of U . The exact thermodynamic
energy and the baseline one are also reported. The comparison shows clearly
that the GP correction to the mean field function is able to capture most of
the interaction energy for a very wide range of interaction strengths.

Variational optimisation of database entries
The results shown so far are very promising and suggest that GP-based Ansatzes
for wave functions could represent a new route to the description of interacting
system. In spite of this, there is an obvious pitfall in the approach presented
so far that needs to be addressed: training on small systems will always give
rise to non-negligible finite size effects in the learned wave function.
The variational principle can help in tackling this problem. For instance,
after training a GP-WF on a reference dataset D = {(xi , ψir )}N
i=1 belonging
to a small system, the learned wave function could be optimised by adjusting the wave function entries belonging to training dataset {ψir }N
i=1 in order
to minimise the Monte Carlo sampled energy of the larger system studied.

−0.24

104

k1 from 2
k2 from 4
k3 from 6

−0.26
−0.28
−0.30
−0.32
0

250

500

Iteration
(a)

750

1000

Energy per site, E/L

Energy per site, E/L

7.4. Tests on the Hubbard model

−0.29
−0.30
−0.31

Initial ψ̂
Optimised ψ̂

−0.32

0

1

2

3

Epochs (1500 iterations)
(b)

Figure 7.3: Energy per site as a function of gradient descent iteration in an
8-site system (a) and a 32-site system (b).

The simplest algorithm for performing this minimisation would be a steepest
descent on the stochastic gradient obtained by Monte Carlo sampling i.e., a
stochastic gradient gradient descent [84].
This idea is tested in Figure 7.3, which shows the energy per site of GPWFs as a function of the iteration number of the stochastic gradient descent.
In the left panel, Figure 7.3(a), a GP-WF is trained on a 2-, 4- or 6-site system
and the learned wave function is optimised to minimise the energy of an 8-site
system.
In the right panel, Figure 7.3(b), a 6-site GP-WF is instead optimise for a
system of 32-sites. From the results shown, it is apparent that the proposed
optimisation is able to significantly improved on the original fit. The improvement is however harder to achieve when larger training system are employed.
This is presumably a consequence of the larger number of database points
that need to be optimised. In facts, database points here play the role of the
variational parameters in a standard VMC calculation and it is well known
that VMC optimisations become very challenging with increasing number of
parameters. Luckily, effective algorithms exist that can help in this direction
and their proper implementation for the present scope is crucial to fully explore
the power of this approach.

7.5. Future extensions

7.5 Future extensions
The ideas and results presented in this chapter are very preliminary and many
interesting directions of improvement can be identified. For instance, an obvious drawback of GP-WFs come from the cubic bottleneck in training a GP
regression (obvious from Eq. (7.10)). This is a well known issue in the ML
community and several algorithms for “sparse GPs” have been proposed to
mitigate it including e.g., the Informative Vector Machines (IVM) [63], the
Relevance Vector Machines [103] or active learning approaches [94]. It would
certainly be useful to compare their performance for the present scopes, in
some preliminary tests available in Appendix ? IVM have been succesfully
trained on a wave functions of up to a million entries.
The variational optimisation could also be significantly improved. In facts,
while here a simple stochastic gradient descent algorithm has been used, more
sophisticated optimisation algorithm like the stochastic reconfiguration [95,
96], the Adagrad or Adam algorithms [84, 90] should be implemented and
tested. Ideally, a more complete variational minimisation would also have to
optimise the kernels hyperparameters. Another interesting route to explore
would be expressing the GP Ansatz on a different basis. Obvious candidates
to try would be the occupation number bases both in real and in momentum
space or some combination of the two.
Finally, it is fundamental to test the proposed framework on more challenging systems. The perfect candidate would be the two dimensional Hubbard
model as many feature of this system remaoin not well understood at the presr
This system, differently from the one dimensional analogue, presents substantial modelling challenges [64]. Perhaps the major difficulty is the nontrivial
sign structure of the wave function (absent in one dimension). The GP-WF
approach presented here relies on the Slater determinant sign but this could
be augmented or substituted by a separate machine learning algorithm specifically designed to learn it from data, good candidates for this task could be
Gaussian process classification [110] or a support vector machine [18]. Alternatively, sign information could be included in the present model by the use
of complex Gaussian processes [20].

105

7.6. Summary

7.6 Summary
This chapter introduced a novel method to model wave functions of strongly
interacting electronic systems. The method is centred on the idea of compactly
representing the sought wave function not in terms of a few parameters as typically done in variational Monte Carlo approaches - but in terms of
a data set of calculations. A log-GP Ansatz was proposed and a range of
kernel functions encompassing specific sets of physically based interaction was
proposed. These kernels then tested on the one dimensional Hubbard model
and they were shown to represent surprisingly well the wave function of large
lattice systems (∼ 40 sites) even when trained on a very small lattices (∼ 6
sites). Obviously finite size effects are always present in the learned wave
function when only training on small systems. To overcome this limitation,
a variational scheme was proposed. This involves optimising entries of the
training database in order to minimise the variational energy of the target
system calculated via Monte Carlo sampling. The optimisation was shown
to significantly reduce the finite size effects of the learned wave function, but
the improvements were found to be harder to obtain as the training system
size increases. This is a common problem of stochastic optimisations and it
could be tackled by faster and more sophisticated routines and and a parallel
implementation. The results presented here are very promising and make
GP wave functions a potentially very efficient candidate for the description of
qauntum many body systems. However, more research is surely necessary to
fully explore their capability and some extension of the work presented here
were given in the final section of this chapter.

106

Chapter 8

Conclusions

Appendix A

Appendix
A.1 Derivation of the predictive distribution
In this appendix we give a sketch of the procedure by which Eq. (??) is obtained, which substantially relies on the properties of multivariate Gaussian
distributions. For full details on this one can consult the excellent Refs. [18]
and [110].
In the main text we computed the probability distribution p(εr | ρ) (i.e.
the marginal likelihood of the data) in closed form (Eq. (2.6)). To calculate the
predictive distribution for the new pair (ρ∗ , ε∗ ) (i.e. p(ε∗ | ρ∗ , ε, ρ) ), one first
write down the probability of the original dataset augmented with the new
pair. This is simply found by adapting Eq. (2.6) to the augmented database:



p(ε∗ , εr | ρ∗ , ρ) = N (0, Ca )




C k


Ca
=


T

k
c
where c = k(ρ∗ , ρ∗ ).
The two variables ε∗ and εr are hence jointly Gaussian. Importantly, it two
variables are distributed according to a multivariate normal distribution, then
the conditional distribution of one conditioned on the other is also normal.
The corresponding mean and variance of the normal conditional distribution
p(ε∗ | ρ∗ , εr , ρ) can be found by appropriately completing the square in the
argument of the exponential of the joint distribution [18]. Doing so results in
the predictive distribution in Eq. (2.7).

A.2. The predictive mean provides optimal predictions

A.2 The predictive mean provides optimal predictions
The predictive distribution p(ε | ρ, D) in Eq. (2.7) (D = (εr , ρ)), completely
specifies our knowledge about the local energy function ε associated to a given
configuration ρ. Once such a configuration is encountered during an MD run
one is typically faced with the problem of deciding a single estimate ε̄(ρ) for
its energy in order to proceed to the subsequent time step.
This can be formally seen as the problem of making a decision under conditions of uncertainty. This is a standard problem in decision theory [15, 18] and
a typical solution consists in choosing the estimate which minimises a given
expected loss under the known distribution
hLi =

Z

dε p(ε | ρ, D)L(ε̄(ρ), ε).

The loss function L can be taken to be, for instance, the squared error
between the estimate and the measured value of the local energy L = (ε̄(ρ) −
ε)2 . Minimising the expected squared error with respect to the function ε̄(ρ)
can be done analytically by equating the relative functional derivative to zero
δhLi
=2
δ ε̄(ρ)

Z

dε p(ε | ρ, D)(ε̄(ρ) − ε)

= 2(ε̄(ρ) − hεi) = 0,
where from the last line it is clear that the optimal estimate is the mean
ε̂(ρ) of the predictive distribution in Eq. (??)(or, equivalently, the mean of
the posterior GP). One can show that using the absolute difference loss function L = |ε̄(ρ) − ε| makes the mode of the predictive distribution the optimal
estimate, this however coincides with the mean in the case of Gaussian distributions.

A.3 Kernels for multiple chemical species
This appendix develops the basic theory to construct kernels for multispecies
systems and provides specific expressions for the case of 2- and 3-body kernels.
What presented is based on the concepts described in Chapters 2 and 3 and
hence they should be read before this Appendix.

109

A.3. Kernels for multiple chemical species

110

It is convenient to show the reasoning behind multispecies kernel construction starting from a simple example. Defining by sj the chemical species of
atom j, a generic 2-body decomposition of the local energy of an atom i surrounded by the configuration ρi takes the form
ε(ρi ) =

X

φsi sj (rij ).

j

where a different pairwise function φsi sj (rij ) is assumed to provide the energy
of each couple of atoms i and j. These pairwise energy functions should be
invariant upon re-indexing of the atoms i.e., φsi sj (rij ) = φsj si (rji ). The kernel
for the function ε(ρi ) then takes the form
k2s (ρi , ρ0l ) = hε(ρi )ε(ρ0l )i
X
0 0
0
=
hφsi sj (rij )φsl sm (rlm
)i
jm

=

X

0 0

0
k̃ si sj sl sm (rij , rlm
)

jm

The design problem is at this point reduced to that of choosing a suitable
kernel k̃ comparing couples of atoms. An obvious choice for this would include
a simple squared exponential for the radial dependence and a delta correlation
0
). This kernel
for the species dependence, giving rise to δsi s0l δsj s0m kSE (rij , rlm
is however still not symmetric upon the exchange of two atoms and it would
hence not impose the required property φsi sj (rij ) = φsj si (rji ) on the learned
pairwise potential. Permutation invariance can be simply enforced by a direct
sum over the permutation group, in this case simply an exchange of the two
atoms l and m in the second configuration. The resulting 2-body multispecies
kernel reads
k2s (ρi , ρ0l ) =

X

0

(δsi s0l δsj s0m + δsi s0m δsj s0l )e(rij −rlm )

2 /2`2

j∈ρ
m∈ρ0

This can be considered the natural generalisation of the single species 2-body
kernel in Eq. (3.14) A very similar sequence of steps can be followed for the
3-body case. By defining the vector containing the chemical species of an
ordered triplet as sijk = (si sj sk )T as well as the vector containing the corresponding three distances rijk = (rij rjk rki )T , a multispecies 3-body kernel can

A.4. Kernel order by explicit differentiation

111

be compactly written down as
k3s (ρi , ρ0l ) =

X

X

T

0

2 /2`2

δsijk ,Ps0lmn e−krijk −Prlmn k

,

(A.1)

j>k∈ρi P∈P
m>n∈ρ0l

where the group P contains six permutations of three elements, represented
by the matrices P. The above can be considered the direct generalisation of
the 3-body kernel in Eq. (3.15).
It is simple to see how the reasoning generalises to an arbitrary n-body
kernel, although the sum over indices and permutations become quickly unaffordable from a computational point of view.

A.4 Kernel order by explicit differentiation
We first prove that the kernel given in Eq. (3.2) is 2-body in the sense of
Eq. (2.28). For this it is sufficient to show that its second derivative with
respect to the relative position of two different atoms of the target configuration
ρ always vanishes. The first derivative is
∂k2 (ρ, ρ0 ) X ∂ −kri −r0j k2 /2`2
=
e
∂ri1
∂ri1
ij
=

X

=

X

0 2 /2`2

e−kri −rj k

ij

(ri − r0j )
δii1
`2

0 2 /2`2

e−kri1 −rj k

j

(ri1 − r0j )
.
`2

This depends only on the atom located at ri1 of the configuration ρ. Thus,
differentiating with respect to the relative position ri2 of any other atom of
the configuration gives the relation in Eq. (2.28) for 2-body kernels:
∂ 2 k2 (ρ, ρ0 )
= 0.
∂ri1 ∂ri2
We next show that the kernel defined in Eq. (3.3) is an n-body in the sense
of Eq. (2.28). This follows naturally from the result above, given that kn is

A.5. A one dimensional n0 -body model

112

defined as kn = k2n−1 . We can thus write down its first derivative as
∂k2
∂kn
= (n − 1)k2n−2
.
∂ri1
∂ri1
Since the second derivative of k2 is null, the second derivative of kn is simply
∂ 2 kn
∂k2 ∂k2
= (n − 2)(n − 1)k2n−3
∂ri1 ∂ri2
∂ri1 ∂ri2
and after n − 1 derivations we similarly obtain
∂ 2 k2n−1
∂k2
∂k2
= (n − 1)! k20
...
.
∂ri1 · · · ∂rin
∂ri1
∂rin−1

Since k20 = 1, the final derivative with respect to the nth particle position rin
is zero as required by Eq. (2.28).

A.5 A one dimensional n0-body model
To test the ideas behind the n-body kernels, we used a 1D n0 -particle model
reference system where a (“central”) particle is kept fixed at the coordinate
axis origin (consistent with the local configuration convention of Eq. (3.1)).
The energy of the central particle is defined as
f=

X
i1 ...in0 −1

0

J xi1 . . . xin0 −1

−1
where {xip }np=1
are the relative positions of n0 − 1 particles, and J is an interaction constant.
To generate Figure 3.1 a large set of configurations was generated by uniformly and independently sampling each relative position xip within the range
(−0.5, 0.5). The energy of the central particle of each configuration was then
given by the above equation, with the interaction constant J set to 0.5. In order to analyse the converged properties of the n-body kernels presented, large
training sets (N = 1000) were used.

A.6. Databases details

113

A.6 Databases details
The bulk Ni and Fe databases were obtained from simulations using a 4 × 4 × 4
periodically repeated unit cell, modelling the electronic exchange and correlation interactions via the PBE/GGA approximation [76], and controlling the
temperature (set at 500K) by means of a weakly-coupled Langevin thermostat (the DFT trajectories are available from the KCL research data management system at the link http://doi.org/10.18742/RDM01-92). The C database
comprises bulk diamond and AB- and ABC-stacked graphene layer structures.
These structures were obtained from DFT simulations at varying temperatures and pressures, using a fixed 3×3×2 periodic cell geometry for graphite,
and simulation cells ranging from 1×1×1 to 2×2×2 unitary cells for diamond,
the relative DFT trajectories can be found in the “libAtoms” data repository
via the following link http://www.libatoms.org/Home/DataRepository. Crystalline and amorphous Si database was obtained from a microcanonical DFTB
64-atom simulation carried out in periodic boundaries, with average kinetic
energy corresponding to a temperature of T = 650K. The results presented
for the Ni cluster are reported from Ref. [111] and correspond to constant
temperature DFT MD runs (T = 300K) of a particular Ni19 geometry (named
“4HCP” in the article). The radial cutoffs used to create the local environments for the four elements considered are: 4.0 (Ni), 4.45 (Fe), 3.7 (C) and
4.5 (Si).

A.7 Covariant integration of 2-body kernels
The integral we wish to evaluate, repeated here for convenience, is

Z
1X
0 2
2
dR Re−(ri −Rrj ) /2`
K (ρ, ρ ) =
L ij
1X
=
Iij .
L ij
c

0

A.7. Covariant integration of 2-body kernels

114

First of all it is convenient to separate the radial part from the angular one as
the first of these does not depend on rotations:
Iij = e

−(ri2 +rj02 )/2`2

Z

Z

0

T

2

dR R eri Rrj /`
0

T

2

dR R eri Rrj /` .

= Cij

2D systems
If we define Rij to be the rotation matrix that brings the vector r0j onto ri ,
then we can perform the change of variable R̃ = RRT
ij
Z

dR̃ R̃ eri R̃Rij rj /` Rij

Z

dR̃ R̃ eri R̃r̃j /` Rij .

0

T

Iij = Cij

T

= Cij

0

2

2

where the two vectors ri and r̃j are now aligned with each other. By parametrising all rotations by a single angle θ we can rewrite the above integration as
Z

2π

dθ
T
0
2
R(θ)eri R(θ)r̃j /` Rij
2π

0Z 2π
dθ
ri rj0 cos θ/`2
R(θ)e
Rij .
= Cij
2π
0

Iij = Cij

The integral in brackets can now be given an analytic form. The rotation
matrix R(θ) is composed by cos θ on the diagonal and {sin θ, − sin θ} off the
diagonal. Evaluating the above integration for such terms one finds that
R
 2π
0
R 2π
0

dθ
2π
dθ
2π

cos θ e
sin θ e

ri rj0 cos θ/`2

ri rj0

cos θ/`2

= I1



ri rj0
`2



=0

where I1 (·) is a modified Bessel function of the first kind. The second line follows because we are integrating an odd function over an even domain. The first
line, on the other hand, results from a definition of modified Bessel functions
of the first kind In (z) for integer values of n ([1] p. 376), i.e.
1
In (z) =
π

Z
0

π

ez cos θ cos(nθ)dθ.

A.7. Covariant integration of 2-body kernels

115

Hence the final integral reads

Iij = Cij I1

ri rj0
`2


Rij .

3D systems
In three dimensions, it is first of all convenient to cast the integral in the
following form
Z

dR Re−(ri −Rrj )

Z

dR R k p (ri , Rr0j ).

Iij =
=

0 2
/2`2

Now we can use the global invariance of the base pairwise kernels k p , that
is k p (r, r0 ) = k p (Rr, Rr0 ), in order to align ri onto the z-axis. We call the
rotation that does so Rzi and we have
Z

dR R k p (Rzi ri , Rzi Rr0j )

Z

dR R k p (r̃i , Rzi Rr0j ).

Iij =
=

where we defined r̃i = Rzi ri . At this point we find the matrix Rzj that brings
also rj parallel to the z-axis. We then insert it in front of r0j in the form of the
z
identity RzT
j Rj :
Z
Iij =
Z
=

z 0
dR R k p (r̃i , Rzi RRzT
j Rj r j )
0
dR R k p (r̃i , Rzi RRzT
j r̃j )

where we again used the tilde notation to define the vector now aligned to the
z-azis. Finally we perform the change of variables R̃ = Rzi RRzT
j to obtain
Iij =

RzT
i

Z

dR̃ R̃ k p (r̃i , R̃r̃0j )Rzj

z
= RzT
i Rij Rj .

A.8. Proof that 2-body covariant kernels give rise to central forces
The central integral yielding Rij remains to be performed. Its evaluation is
considerably simpler than the original problem since now both vectors r̃i , r̃0j are
along the z-axis. Hence, by parametrising all rotations by Euler angles α, β, γ
around the z, y, z axes respectively, we find by geometric reasoning that the
argument of the exponential has to be invariant upon rotations of angles α
and γ around the z-axis. In fact, we have that
Z
Rij = Cij

dαdβdγ sin β
ri rj0 cos β/`2
R(α,
β,
γ)e
8π 2

where we made use of the normalised Haar measure dαdβdγ sin β/8π 3 . The
rotation matrix to be averaged reads



cα cγ − cβ sα sγ −cγ cβ sα − cα sγ sα sβ


R(α, β, γ) = cγ sα + cα cβ sγ cα cγ cβ − sα sγ −cα sβ  .
sγ sβ
cγ s β
cβ
All the elements of the above matrix apart from the zz element vanish since
there is always either a sine or a cosine integrated over an entire period. By
defining γij = ri rj0 /`2 , the only non trivial integral reads
Z

π

0

Z

π

dβ sin(2β) γij cos β
e
2
2
0
 γij cos β
π
e
(1 − γij cos β)
=
2γij2
0
γij cosh γij − sinh γij
.
=
γij2

dβ sin β
0
2
cos β eri rj cos β/` =
2

A.8 Proof that 2-body covariant kernels give rise to
central forces
2D systems
Exploiting the decomposition of the O(2) given in Eq. (4.15), the kernel KO(2)
can be written in terms of the already calculated KSO(2) kernel simply as
KO(2) (ρ, ρ0 ) =


1  SO(2)
K
(ρ, ρ0 ) + KSO(2) (ρ, Fρ0 )F
2

116

A.8. Proof that 2-body covariant kernels give rise to central forces

117

To show that the kernel the above is actually equivalent to the form given in
Eq. (4.20) it is convenient to first write the rotation matrix Rij bringing the
unit vector r̂i onto r̂j as
0T T
Rij = r̂i r̂0T
j + Rx r̂i r̂j Rx

where Rx is a 90-degree rotation matrix. Substituting the above into the
explicit expression for the KSO(2) kernel (4.18), the covariant kernel KO(2) 4.19
reads
KO(2) (ρ, ρ0 ) =



1X
0T T
0T T
0T T T
φ(ri , rj0 ) (r̂i r̂0T
j + Rx r̂i r̂j Rx ) + (r̂i r̂j F + Rx r̂i r̂j F Rx )F
2 ij

Finally, by grouping together the radial terms and using the fact that RT
x + Rx = 0
one obtains the desired result


1X
T
0T
T
T T
φ(ri , rj0 ) r̂i r̂0T
j (1 + FF ) + Rx r̂i r̂j (Rx + F Rx F)
2 ij


1X
0T
T
φ(ri , rj0 ) 2 r̂i r̂0T
=
j + Rx r̂i r̂j (Rx + Rx )
2 ij
X
=
φ(ri , rj0 ) r̂i r̂0T
j

KO(2) (ρ, ρ0 ) =

ij

3D systems
To show that the KSO(3) covariant kernel in Eq. (4.21) is equivalent to the
KO(3) in Eq. (4.22) one just needs to be able to see that the outer product of
the ẑ unit vector is


0 0 0


ẑẑT = 0 0 0
0 0 1
so that Eq. (4.21) can be rewritten as
KSO(3) (ρ, ρ0 ) =

X
ij

T z
φ(ri , rj0 )RzT
i ẑẑ Rj .

A.9. Covariant integration of 3-body kernels

118

At this point one can exploit the definitions Rzi r̂i = ẑ and Rzj r̂j = ẑ to find
z T
T
that RzT
i ẑ = r̂i and (Rj ẑ) = r̂j , obtaining this way the final result
KSO(3) (ρ, ρ0 ) =

X

φ(ri , rj0 )r̂i r̂T
j .

ij

A.9 Covariant integration of 3-body kernels
The integral we wish to evaluate is

KcS

=

XXZ
ij

=

0

e−(rl −Rrm )

2 /4σ 2

lm

XX
ij

0 2
/4σ 2

dR Re−(ri −Rrj )

Iijlm

lm

First of all it is convenient to separate the radial part from the angular one as
the first does not depend on rotations:
Iijlm = e

02 )/4σ 2
−(ri2 +rj02 +rl2 +rm

Z
= C̃ijlm

1

Z
T

T

0

2

T

0

dR R eri Rrj /2σ erl Rrm /2σ
0

T

2

0

dR R e 2σ2 [ri Rrj +rl Rrm ]

2D systems
In two dimensions this can be written as
Z
1
0
0
dθ
Iijlm = C̃ijlm
R(θ) e 2σ2 [ri rj cos(θij +θ)+rl rm cos(θlm +θ)]
2π
Z
dθ
= C̃ijlm
R(θ) e[Aij cos(θij +θ)+Alm cos(θlm +θ)] .
2π
Now it is possible to simplify the argument of the exponential by recurring to
standard trigonometric identities. By using

A cos(θ + θ)
= Aij cos θij cos θ − Aij sin θij sin θ
ij
ij
A cos(θ + θ) = A cos θ cos θ − A sin θ sin θ
lm
lm
lm
lm
lm
lm

A.9. Covariant integration of 3-body kernels

119

the sum of the two terms above reads
(Aij cos θij +Alm cos θlm ) cos θ−(Aij sin θij +Alm sin θlm ) sin θ = Cijlm cos θ−Sijlm sin θ.
The two amplitude parameters in the last expression can be cast into an amplitude and a phase as follows
Cijlm cos θ − Sijlm sin θ =

q

2
2
Cijlm
+ Sijlm
cos(θ + tan−1 (

Sijlm
))
Cijlm

= Aijlm cos(θ + θijlm ).
In this form, the integral can finally be performed
Z

dθ
R(θ) eAijlm cos(θ+θijlm )
2π
Z
dθ
R(θ) eAijlm cos(θ)
= C̃ijlm R(θijlm )
2π
= C̃ijlm R(θijlm )I1 (Aijlm )

Iijlm = C̃ijlm

where from the first to the second line the change of variables θ + θijlm → θ
was performed and I1 (·) is a modified Bessel function of the first kind.
Hence, the final result for the second order covariant kernel in two dimension is
KcS =

X

C̃ijlm I1 (Aijlm )R(θijlm )

ijlm
2

02

2

02

2

C̃ijlm = e−(ri +rj +rl +rm )/4σ
 0

0
sin θlm
ri rj sin θij + rl rm
−1
θijlm = tan
0 cos θ
ri rj0 cos θij + rl rm
lm
q
0 cos θ )2 + (r r 0 sin θ + r r 0 sin θ )2 /2σ 2
(ri rj0 cos θij + rl rm
Aijlm =
lm
i j
ij
l m
lm
Notice that the argument of the Bessel function can be rewritten in terms
of the angles “within” each configuration (i.e. θil ∧ i, l ∈ ρ and θjm ∧ j, m ∈ ρ0 )
as
q
Aijlm = (ri rj )2 + (rl rm )2 + 2ri rj rl rm cos(θil − θjm )/2σ 2 .

A.9. Covariant integration of 3-body kernels

120

3D systems
In three dimensions, it is first of all convenient to cast the integral in the
following form
Z

dR Re−(ri −Rrj )

Z

dR R k p (ri , Rr0j )k p (rl , Rr0m ).

Iijlm =
=

0 2
/4σ 2

0

2 /4σ 2

e−(rl −Rrm )

Now we can use the global invariance of the pairwise kernels k p , that is
k p (r, r0 ) = k p (Rr, Rr0 ), in order to align the first couple “ri , rl ” in such a
way that ri is parallel to the z-axis and rl lies in the xz plane. We call the
rotation that does so Ril and we have
Z
dR R k p (Ril ri , Ril Rr0j )k p (Ril rl , Ril Rr0m )
Iijlm =
Z
=
dR R k p (r̃i , Ril Rr0j )k p (r̃l , Ril Rr0m ).
At this point we find the matrix Rjm that bring rj parallel to the z-axis and
rm onto the xz plane. We write
Z
Iijlm =
Z
=

0
p
T
0
dR R k p (r̃i , Ril RRT
jm Rjm rj )k (r̃l , Ril RRjm Rjm rm )
0
p
T 0
dR R k p (r̃i , Ril RRT
jm r̃j )k (r̃l , Ril RRjm r̃m ).

Finally we perform the change of variables R̃ = Ril RRT
jm to obtain
Iijlm =

RT
il

Z

dR R̃ k p (r̃i , R̃r̃0j )k p (r̃l , R̃r̃0m )Rjm

= RT
il Rijlm Rjm .
The central integral yielding Rijlm remains to be performed. Analytical solution for Rijlm is hardly obtainable. However, we can find a reasonable approximation to it. We start by noticing that once the vectors are oriented as
explained above, the maximum of the integrand will certainly lie within the
xz plane. This means that, by choosing the Tait–Bryan angles α, β, γ to be
given by the sequence of rotations around y − z − x respectively the maximum
will be at α0 = θijlm , β0 = 0, γ0 = 0. Moreover, given the above mentioned

A.9. Covariant integration of 3-body kernels

121

alignments, we have that θij = 0 and the alignment angle α0 will be given by
−1

α0 = tan



0
rl rm
sin θlm
0
0 cos θ
ri rj + rl rm
lm


.

As we found a maximum, we can Taylor expand the angular part in the exponential around that point to second order. For convenience let us define
θ ≡ (α, β, γ)T the angular part to be expanded reads
f (θ) ≡

1 T
0
[r̃ R̃(α, β, γ)r̃0j + r̃T
l R̃(α, β, γ)r̃m ]
2σ 2 i

so that



1
∂ 2f
T
f (θ) ≈ f (θ 0 ) − (θ − θ 0 ) −
(θ − θ 0 ).
2
∂θ∂θ θ0
h 2 i
∂ f
The Calculation of the (negative) Hessian H = − ∂θ∂θ
proceeds as follows.
θ0

First of all let us write down explicitly the analytic dependence of f (θ) on the
TB angles. The rotation matrix around the y − z − x angles is written down
as


cα cβ
−sβ
cβ s α


Ryzx (α, β, γ) = cα cγ sβ + sα sγ cβ cγ cγ sα sβ − cα sγ  .
cα s β s γ − cγ s α cβ s γ cα cγ + s α s β s γ
From it, it is straightforward to write down the sum quadratic forms

 
 x 
0
rl




1 




yzx
yxz
f (α, β, γ) =
 0 0 riz R  0  + rlx 0 rlz R  0  ,
2
2σ
rj0z
rj0z
which reads
f (α, β, γ) =

1
0x
0x
0z
[cα (rlz rm
sβ sγ + rlx rm
cβ + riz rj0z cγ + rlz rm
cγ )
2
2σ
0z
0x
0z
cγ )].
cβ − rlz rm
+ sα (sβ sγ (riz rj0z + rlz rm
) + rlx rm

A.9. Covariant integration of 3-body kernels

122

It is then a tedious exercise to calculate, one by one, all the double derivatives

T
∂2
and
evaluate
them
at
the
point
θ
=
. The result reads
α
0
0
0
0
∂θi ∂θj



Hαα 0
0


H = −  0 Hββ Hβγ 
0 Hβγ Hγγ
0x
0z
0x
0z
Hαα = [(rlz rm
− rlx rm
)sα0 − (riz rj0z + rlx rm
+ rlz rm
)cα0 ]/2σ 2
0x
0z
Hββ = [−rlx (rm
cα0 + rm
sα0 )]/2σ 2

0x
0z
Hγγ = [rlz rm
sα0 − (riz rj0z + rlz rm
)cα0 ]/2σ 2

0x
0z
Hβγ = [rlz rm
cα0 + (riz rj0z + rlz rm
)sα0 ]/2σ 2

From the above expression one can see that the approximation we are evaluating increases in accuracy as σ → 0 as in this limit indeed the maximum
becomes more and more peaked. This statement can be made more precise by
decomposing the determinant of the negative Hessian H as follows


| H |= 



...
...
...

 1
 2
2σ


...

= 
...





...

1
2σ 2

3

Since we are expanding around a maximum the Hessian will be positive definite
and its determinant will hence be strictly positive. The determinant of the
negative Hessian will consequently be strictly negative from the last expression
given we will have
lim |H| = −∞.
σ→0

To allow the algorithm to effectively distinguish between similar configurations,
typical values of σ are always chosen to be relatively small (generally roughly a
fourth of the nearest neighbour distance). Hence we expect the approximation
to work reasonably well.
After the explained manipulations, we managed to transform the integral
over all rotations into the expected value of the rotation matrix over a multi-

A.9. Covariant integration of 3-body kernels

123

variate normal distribution:
Rijlm = C̃ijlm e

f (θ 0 )

Z

1

T H(θ−θ

dθ R(θ) e− 2 (θ−θ0 )

0)

= C̃ijlm efijlm Zijlm hR(θ)iN (θ0 ,H−1 )
where Z is the normalisation of the probability distribution, given by
s
Zijlm =

| Hijlm |
(2π)3

while the quadratic form fijlm is written explicitly as
fijlm =

1
0z
0z
0x
[cα (rx r0x + riz rj0z + rlz rm
) + sα0 (rlx rm
− rlz rm
)].
2σ 2 0 l m

The expected value of the rotation matrix is taken element-wise. The
structure of all the integrals is similar, hence, all 9 of them are evaluated in
the same way. For instance, the xx element of Ryzx (α, β, γ) reads cos α cos β.
First of all, using prostapheresis formulas we are able to rewrite this term as
(cos(α − β) − cos(α + β))/2. At this point, the random variable of interest
is sum or the difference of correlated gaussian RVs (α and β). We know that
θ ∼ N (θ 0 , H−1 ) hence the variable x = α ± β is also normally distributed
x ∼ N (µx , σx2 ). We are hence left with the calculations of hcos(x)i. This is
easily obtained using Euler’s formula and the characteristic function of the
normal distribution as follows:
heix i = hcos(x)i + ihsin xi
1

2

= eiµx − 2 σx

1

2

= (cos µx + i sin µx )e− 2 σx
⇓

hcos xi = e− 12 σx2 cos µ
x
1
hsin xi = e− 2 σx sin µ
x

To calculate the expected value of the exponential we used the characteristic
1 2 2
function of the normal distribution heitx i = eitµx − 2 t σx evaluated at t = 1.
In the following we report the result of the 9 integrals (all evaluated as

A.10. One dimensional n-body toy model

124

explained above) yielding the expected value of the whole rotation matrix
hR(θ)iN (θ0 ,H−1 ) ≡ R̂ijlm


Rxx 0 Rxz


=  0 Ryy 0 
Rzx 0 Rzz
1

Rxx = e− 2 (Hαα +Hββ ) cos α0
1

Ryy = e− 2 (Hββ +Hγγ ) cosh Hβγ
1

1

Rzz = e− 2 (Hαα +Hββ +Hγγ ) (sinh Hβγ sin α0 + e 2 Hββ cos α0 )
1

Rxz = e− 2 (Hαα +Hββ ) sin α0
1

1

Rzx = e− 2 (Hαα +Hββ ) (sinh Hβγ cos α0 − e 2 Hββ sin α0 )
Hence, the final integral over all three dimensional rotations reads:
Iijlm = C̃ijlm efijlm Zijlm RT
ij R̂ijlm Rlm .

A.10 One dimensional n-body toy model
The fictitious n-body interaction model on which we tested our ideas was set
up as a hierarchy of two body interactions defined via the following negative
Gaussian
k(d−a)2
g (d) = −e− 2
where a and k can be thought as a lattice parameter and a spring constant.
This pairwise interaction, depending only on the distance d between two
particles, was then used to generate n-body local energies as
n (ρ) =

X
i1 6=···6=in−1

g (xi1 )g (xi2 − xi1 ) . . . g (xin−2 − xin−1 )

where xi1 , . . . , xin −1 are the positions, relative to the central atom, of n − 1
surrounding particles.

A.11. Mapping the predictive variance

125

A.11 Mapping the predictive variance
We consider again the case of energy prediction using a 2-body kernel and write
down the explicit equation for the predicted variance, obtained substituting
the kernel definition of Eq. (??) in the third expression of Eq. (??):



v̂(ρ) = σn2 +

Ntr X
X
X
2
2
2
2
e−(ri −rk )2 /2`2 −
e−(ri −rj ) /2` (C −1 )tu e−(ri −rl ) /2` 
.

t,u j∈ρt
l∈ρu

i,k∈ρ

(A.2)
We can see how Eq. A.2 can be rewritten considering the part in parenthesis
as a function g(ri , rk ), similarly to what was done for the predicted mean:
v̂(ρ) = σn2 +

X

g(ri , rk ).

(A.3)

i,k∈ρ

The output of the function g(ri , rk ) can be stored in a (3n − 6)2 dimensional
array, where n is again the order of the kernel used. It is fundamental to
point out that the dimensionality of the n-body GP mapping has increased
from 3n − 6 (for the mean prediction) to (3n − 6)2 , due to the presence of the
term k(ρ, ρ) in Eq. (??). To circumvent the cumbersome variance mapping,
it is possible to use the original Gaussian Process to evaluate the variance of
predictions every t time steps during an MD simulation. This would allow
to check whether the MFF is in an interpolative or extrapolative regime, and
to re-train the Gaussian Process, and subsequently map it onto an improved
MFF whenever the accuracy requirement on force predictions is not satisfied.
Alternatively, the mapping of the GP predicted error for n = 3 can be made
computationally affordable by approximating the error contribution from each
n-plet to be independent. MORE HERE

A.12 Quadratic kernel scaling for local updates
Let us assume we have a database configuration xd and a root configuration
xr . Omitting unessential factors, the complete among those two is written
down as
X
X
(1 +
kc (nr , nd ) ∼
δnri+∆ ,ndj+∆ )L−1 .
ij

∆

A.12. Quadratic kernel scaling for local updates

126

Then let us assume that a new configuration nn is a neighbour of the root nr
(i.e. nn ∈ ∂nr ) such that there will be only two indices of nn that change with
respect to those of nr . Let us define by C the set of the two changed indices
and by C¯ the set of L − 2 unchanged ones.
Finally, let us define the Llarge × Lsmall × N array containing the result
of the delta function evaluations along the displacements between the root
configuration and all database entries
Ard
ij =

X

δnri+∆ ,ndj+∆ .

∆

Then, by noticing that the delta function operator will yield identical results on most of the new convifugation
δnni+∆ ,ndj+∆ = δnri+∆ ,ndj+∆

¯
∀∆ | i + ∆ ∈ C,

it is simple to see that the array And
ij for the new configuration can be calculated
in order O(1) as
And
ij =

X

δnni+∆ ,ndj+∆

∆

=

X

δnni+∆ ,ndj+∆ +

∆∈C¯i

= Ard
ij −

X

δnni+∆ ,ndj+∆

∆∈Ci

X
∆∈C¯i

δnri+∆ ,ndj+∆ +

X

δnni+∆ ,ndj+∆

∆∈Ci

¯ respectively.
where Ci (and C¯i ) are the set of indices for which i + ∆ ∈ C(∈ C)
This brings the total cost of calculatind the kernel kc (nr , nd ) to L2 at the
(afforable) cost of storing the large array Ard
ij .

Bibliography
[1]

M Abramowitz and I A Stegun. Handbook of mathematical functions
with formulas, graphs, and mathematical tables. National Bureau of
Standards: Applied Mathematics Series, 1972.

[2]

Alexander Altland and Ben D Simons. Condensed Matter Field Theory.
Cambridge University Press, March 2010.

[3]

M A Alvarez, L Rosasco, and N D Lawrence. Kernels for Vector-Valued
Functions: a Review. June 2011.

[4]

T W Anderson. The Non-Central Wishart Distribution and Certain
Problems of Multivariate Statistics. The Annals of Mathematical Statistics, 17(4):409–431, December 1946.

[5]

S Aubert and C S Lam. Invariant integration over the unitary group.
Journal of Mathematical Physics, 44(12):6112–21, 2003.

[6]

A P Bartók and Gábor Csányi. Gaussian approximation potentials: A
brief tutorial introduction. International Journal of Quantum Chemistry,
115(16):1051–1057, April 2015.

[7]

A P Bartók, Michael J Gillan, Frederick R Manby, and Gábor Csányi.
Machine-learning approach for one- and two-body corrections to density functional theory: Applications to molecular and condensed water.
Physical Review B, 88(5):054104–12, August 2013.

[8]

A P Bartók, Risi Kondor, and Gábor Csányi. On representing chemical
environments. Physical Review B, 87(18):184115–16, May 2013.

[9]

A P Bartók, Mike C Payne, Risi Kondor, and Gábor Csányi. Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics,

BIBLIOGRAPHY
without the Electrons. Physical Review Letters, 104(13):136403–4, April
2010.
[10] T Bayes and M Price. An essay towards solving a problem in the doctrine
of chances. by the late rev. mr. bayes, frs communicated by mr. price, in
a letter to john canton, amfrs. Philosophical Transactions (1683-1775),
1763.
[11] Federico Becca and Sandro Sorella. Quantum Monte Carlo Approaches
for Correlated Systems. Cambridge University Press, 1 edition, November 2017.
[12] J Behler. Atom-centered symmetry functions for constructing highdimensional neural network potentials. The Journal of Chemical Physics,
134(7):074106–14, 2011.
[13] J Behler and M Parrinello. Generalized Neural-Network Representation
of High-Dimensional Potential-Energy Surfaces. Physical Review Letters,
98(14):146401–4, April 2007.
[14] Tristan Bereau, Robert A DiStasio, Alexandre Tkatchenko, and O A
von Lilienfeld. Non-covalent interactions across organic and biological
subsets of chemical space: Physics-based potentials parametrized from
machine learning. Journal of Chemical Physics, 148(24):241706, June
2018.
[15] James O Berger. Statistical Decision Theory and Bayesian Analysis.
Springer Series in Statistics. Springer Science & Business Media, New
York, NY, March 2013.
[16] F Bianchini. Mechanical Properties of Nickel-based Superalloys A Multiscale Atomistic Investigation . PhD thesis, August 2016.
[17] F Bianchini, J R Kermode, and A De Vita. Modelling defects in Ni–Al
with EAM and DFT calculations. Modelling and Simulation in Materials
Science and Engineering, pages 1–15, April 2016.
[18] C M Bishop. Pattern Recognition and Machine Learning. Information
Science and Statistics. Springer, New York, NY, 2006.

128

BIBLIOGRAPHY
[19] Jacob R Boes, Mitchell C Groenenboom, John A Keith, and John R
Kitchin. Neural network and ReaxFF comparison for Au properties.
International Journal of Quantum Chemistry, 116(13):979–987, March
2016.
[20] Rafael Boloix-Tortosa, Juan José Murillo-Fuentes, Francisco Javier
Payan-Somet, and Fernando Perez-Cruz. Complex Gaussian Processes
for Regression. IEEE Transactions on Neural Networks and Learning
Systems, 29(11):5499–5511, October 2018.
[21] V Botu and R Ramprasad. Adaptive machine learning framework to accelerate ab initiomolecular dynamics. International Journal of Quantum
Chemistry, December 2014.
[22] V Botu and R Ramprasad. Learning scheme to predict atomic forces
and accelerate materials simulations. Physical Review B, 92(9):094306–
5, September 2015.
[23] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140,
August 1996.
[24] D W Brenner. The art and science of an analytic potential. physica
status solidi(b), 2000.
[25] M Caccin, Z Li, J R Kermode, and A De Vita. A framework for machinelearning-augmented multiscale atomistic simulations on parallel supercomputers. International Journal of Quantum Chemistry, June 2015.
[26] Michele Casula and Sandro Sorella. Geminal wave functions with Jastrow correlation: A first application to atoms. The Journal of Chemical
Physics, 119(13):6500–6511, October 2003.
[27] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor
Poltavsky, Kristof T Schütt, and K R Müller. Machine learning of
accurate energy-conserving molecular force fields. Science Advances,
3(5):e1603015, May 2017.
[28] Gerardo Andrés Cisneros, Kjartan Thor Wikfeldt, Lars Ojamäe, Jibao
Lu, Yao Xu, Hedieh Torabifard, A P Bartók, Gábor Csányi, Valeria Molinero, and Francesco Paesani. Modeling Molecular Interactions in Wa-

129

BIBLIOGRAPHY
ter: From Pairwise to Many-Body Potential Energy Functions. Chemical
Reviews, 116(13):7501–7528, July 2016.
[29] DH Wolpert Neural computation and 1996. The lack of a priori distinctions between learning algorithms. MIT Press, 8(7):1341–1390, October
1996.
[30] Gábor Csányi, T Albaret, M C Payne, and A De Vita. “Learn on the
Fly”: A Hybrid Classical and Quantum-Mechanical Molecular Dynamics
Simulation. Physical Review Letters, 93(17):175503–4, October 2004.
[31] Sandip De, A P Bartók, G Csányi, and Michele Ceriotti. Comparing
molecules and solids across structural and alchemical space. Physical
Chemistry Chemical Physics, 18:13754–13769, May 2016.
[32] Sandip De, Felix Musil, Teresa Ingram, Carsten Baldauf, and Michele
Ceriotti. Mapping and classifying molecules from a high-throughput
structural database. Journal of Cheminformatics, pages 1–14, January
2017.
[33] Y Deng, E Kozik, and N V Prokof’ev. Emergent BCS regime of the twodimensional fermionic Hubbard model: Ground-state phase diagram.
EPL (Europhysics . . . , 2015.
[34] Volker L Deringer and Gábor Csányi.
Machine learning based
interatomic potential for amorphous carbon. Physical Review B,
95(9):094203, March 2017.
[35] N D Drummond, M D Towler, and R J Needs. Jastrow correlation
factor for atoms, molecules, and solids. Physical Review B, 70(23):12–
11, December 2004.
[36] M Elstner, D Porezag, G Jungnickel, J Elsner, M Haugk, Th Frauenheim,
S Suhai, and G Seifert. Self-consistent-charge density-functional tightbinding method for simulations of complex materials properties. Physical
Review B, 58(11):7260–7268, September 1998.
[37] Fabian H L Essler, Holger Frahm, Frank Göhmann, Andreas Klümper,
and Vladimir E Korepin. The One-Dimensional Hubbard Model. Cambridge University Press, February 2005.

130

BIBLIOGRAPHY
[38] Felix A Faber, Anders S Christensen, Bing Huang, and O A von Lilienfeld. Alchemical and structural distribution based representation for
universal quantum machine learning. The Journal of Chemical Physics,
148(24):241717–13, June 2018.
[39] G Ferré, T Haut, and K Barros. Learning Potential Energy Landscapes
using Graph Kernels. December 2016.
[40] G Ferré, J B Maillet, and G Stoltz. Permutation-invariant distance
between atomic configurations. The Journal of Chemical Physics,
143(10):104114–13, September 2015.
[41] R P Feynman. Forces in Molecules. Physical review, 56(4):340–343,
August 1939.
[42] E J Fuselier Jr. Refined error estimates for matrix-valued radial basis
functions. PhD thesis, Texas A&M University, May 2006.
[43] Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452–459, May 2015.
[44] Luca M Ghiringhelli, Jan Vybiral, Sergey V Levchenko, Claudia Draxl,
and Matthias Scheffler. Big Data of Materials Science: Critical Role of
the Descriptor. Physical Review Letters, 114(10):105503–5, March 2015.
[45] A Glielmo, Peter Sollich, and A De Vita. Accurate interatomic force
fields via machine learning with covariant kernels. Physical Review B,
95(21):214302, June 2017.
[46] A Glielmo, Claudio Zeni, and A De Vita. Efficient nonparametric n-body
force fields from machine learning. Physical Review B, 97(18):1–12, May
2018.
[47] Andrea Grisafi, David M Wilkins, Gábor Csányi, and Michele Ceriotti.
Symmetry-Adapted Machine-Learning for Tensorial Properties of Atomistic Systems. arXiv.org, September 2017.
[48] Bernard Haasdonk and Hans Burkhardt. Invariant kernel functions for
pattern analysis and machine learning. Machine Learning, 68(1):35–61,
May 2007.

131

BIBLIOGRAPHY
[49] K Hansen, Franziska Biegler, Raghunathan Ramakrishnan, Wiktor
Pronobis, O A von Lilienfeld, K R Müller, and Alexandre Tkatchenko.
Machine Learning Predictions of Molecular Properties: Accurate ManyBody Potentials and Nonlocality in Chemical Space. The Journal of
Physical Chemistry Letters, 6(12):2326–2331, June 2015.
[50] K Hansen, Grégoire Montavon, Franziska Biegler, Siamac Fazli, M Rupp,
Matthias Scheffler, O A von Lilienfeld, Alexandre Tkatchenko, and K R
Müller. Assessment and Validation of Machine Learning Methods for
Predicting Molecular Atomization Energies. Journal of Chemical Theory
and Computation, 9(8):3404–3419, August 2013.
[51] W K Hastings. Monte-Carlo Sampling Methods Using Markov Chains
and Their Applications. Biometrika, 57(1):97–&, 1970.
[52] D Haussler. Convolution kernels on discrete structures. 1999.
[53] K Hornik. Some new results on neural network approximation. Neural
networks, 6(8):1069–1072, January 1993.
[54] Bing Huang and O A von Lilienfeld. Communication: Understanding
molecular representations in machine learning: The role of uniqueness
and target similarity. The Journal of Chemical Physics, 145(16):161102–
7, October 2016.
[55] J Hubbard. Electron Correlations in Narrow Energy Bands. Proc. R.
Soc. Lond. A, 276(1364):238–+, 1963.
[56] Haoyan Huo and M Rupp. Unified Representation for Machine Learning
of Molecules and Crystals. arXiv.org, April 2017.
[57] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E
Hinton. Adaptive Mixtures of Local Experts. Neural computation,
3(1):79–87, February 1991.
[58] A T James. A generating function for averages over the orthogonal
group. Proc. R. Soc. Lond. A, 229(1178):367–375, May 1955.
[59] William H Jefferys and James O Berger. Ockham’s Razor and Bayesian
Analysis. American Scientist, 80(1):64–72, January 1992.

132

BIBLIOGRAPHY
[60] Andrew Jones and Ben Leimkuhler. Adaptive stochastic methods for
sampling driven molecular systems. The Journal of Chemical Physics,
135(8):084125–12, 2011.
[61] Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to
Computational Learning Theory. MIT Press, 1994.
[62] W Kohn. Density Functional and Density Matrix Method Scaling Linearly with the Number of Atoms. Physical Review Letters, 76(17):3168–
3171, April 1996.
[63] Neil Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian
process methods: The informative vector machine. In Advances in Neural
Information Processing Systems. University of Sheffield, Sheffield, United
Kingdom, January 2003.
[64] J P F LeBlanc, Andrey E Antipov, Federico Becca, Ireneusz W Bulik,
Garnet Kin-Lic Chan, Chia-Min Chung, Youjin Deng, Michel Ferrero,
Thomas M Henderson, Carlos A Jiménez-Hoyos, E Kozik, Xuan-Wen
Liu, Andrew J Millis, N V Prokof’ev, Mingpu Qin, Gustavo E Scuseria,
Hao Shi, B V Svistunov, Luca F Tocchio, I S Tupitsyn, Steven R White,
Shiwei Zhang, Bo-Xiao Zheng, Zhenyue Zhu, Emanuel Gull, and Simons
Collaboration on the Many-Electron Problem. Solutions of the TwoDimensional Hubbard Model: Benchmarks and Results from a Wide
Range of Numerical Algorithms. Physical Review X, 5(4):517–28, December 2015.
[65] Z Li, J R Kermode, and A De Vita. Molecular Dynamics with On-theFly Machine Learning of Quantum-Mechanical Forces. Physical Review
Letters, 114(9):096405–5, March 2015.
[66] I Macêdo and R Castro. Learning divergence-free and curl-free vector
fields with matrix-valued kernels. Instituto Nacional de Matematica Pura
e Aplicada, 2008.
[67] Juraj Mavračić, Felix C Mocanu, Volker L Deringer, Gábor Csányi,
and Stephen R Elliott. Similarity Between Amorphous and Crystalline Phases: The Case of TiO2. The Journal of Physical Chemistry
Letters, 9(11):2985–2990, May 2018.

133

BIBLIOGRAPHY
[68] M L Mehta. Random Matrices; 3rd ed. Pure and applied mathematics
series. Elsevier, San Diego, CA, 2004.
[69] M I Mendelev, S Han, D J Srolovitz, G J Ackland, D Y Sun, and M Asta.
Development of new interatomic potentials appropriate for crystalline
and liquid iron. Philosophical Magazine, 83(35):3977–3994, December
2003.
[70] J Mercer. Functions of positive and negative type, and their connection
with the theory of integral equations. Proceedings of the Royal Society
of London Series a-Containing Papers of a Mathematical and Physical
Character, 83(559):69–70, November 1909.
[71] C A Micchelli and M Pontil. Kernels for multi-task learning. In Advances
in Neural Information Processing Systems. University at Albany State
University of New York, Albany, United States, January 2005.
[72] C A Micchelli and M Pontil. On learning vector-valued functions. Neural
computation, 17(1):177–204, January 2005.
[73] Y Mishin. Atomistic modeling of the γ and γ’-phases of the Ni–Al system. Acta Materialia, 52(6):1451–1467, April 2004.
[74] Eric Neuscamman, Hitesh Changlani, Jesse Kinder, and Garnet Kin-Lic
Chan. Nonstochastic algorithms for Jastrow-Slater and correlator product state wave functions. Physical Review B, 84(20):205132–9, November
2011.
[75] Roberto Olivares-Amaya, Weifeng Hu, Naoki Nakatani, Sandeep
Sharma, Jun Yang, and Garnet Kin-Lic Chan. The ab-initiodensity matrix renormalization group in practice. The Journal of Chemical Physics,
142(3):034102–14, January 2015.
[76] John P Perdew, K Burke, and Matthias Ernzerhof. Generalized Gradient Approximation Made Simple. Physical Review Letters, 77(18):3865–
3868, October 1996.
[77] Evgeny V Podryabinkin and Alexander V Shapeev. Active learning of
linearly parametrized interatomic potentials. Computational Materials
Science, 140:171–180, December 2017.

134

BIBLIOGRAPHY
[78] E Prodan and W Kohn. Nearsightedness of electronic matter. Proceedings of the National Academy of Sciences, 102(33):11635–11638, August
2005.
[79] C E Rasmussen and Z Ghahramani. Occam’s razor. Nips.
[80] Carl Edward Rasmussen and Zoubin Ghahramani. Infinite mixtures of
Gaussian process experts. In Advances in Neural Information Processing
Systems. UCL, London, United Kingdom, January 2002.
[81] Sandeep K Reddy, Shelby C Straight, Pushp Bajaj, C Huy Pham, Marc
Riera, Daniel R Moberg, Miguel A Morales, Chris Knight, Andreas W
Götz, and Francesco Paesani. On the accuracy of the MB-pol manybody potential for water: Interaction energies, vibrational frequencies,
and classical thermodynamic and dynamical properties from clusters to
liquid water and ice. The Journal of Chemical Physics, 145(19):194504–
14, November 2016.
[82] R Rodrı́guez-Guzmán, Carlos A Jiménez-Hoyos, R Schutski, and Gustavo E Scuseria. Multireference symmetry-projected variational approaches for ground and excited states of the one-dimensional Hubbard
model. Physical Review B, 87(23):96–14, June 2013.
[83] Patrick Rowe, Gábor Csányi, Dario Alfè, and Angelos Michaelides. A
Machine Learning Potential for Graphene. arXiv.org, October 2017.
[84] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv.org, September 2016.
[85] M Rupp. Machine learning for quantum mechanics in a nutshell. International Journal of Quantum Chemistry, 115(16):1058–1073, July 2015.
[86] M Rupp, Raghunathan Ramakrishnan, and O A von Lilienfeld. Machine
Learning for Quantum Mechanical Properties of Atoms in Molecules. The
Journal of Physical Chemistry Letters, 6(16):3309–3313, August 2015.
[87] M Rupp, Alexandre Tkatchenko, K R Müller, and O A von Lilienfeld.
Fast and Accurate Modeling of Molecular Atomization Energies with
Machine Learning. Physical Review Letters, 108(5):058301–5, January
2012.

135

BIBLIOGRAPHY
[88] Omer Sagi and Lior Rokach. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
8(4):e1249–18, February 2018.
[89] Ulrich Schollwöck. The density-matrix renormalization group: a short
introduction. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 369(1946):2643–2661, July
2011.
[90] Lauretta R Schwarz, A Alavi, and George H Booth. Projector Quantum
Monte Carlo Method for Nonlinear Wave Functions. Physical Review
Letters, 118(17):372–6, April 2017.
[91] M Sewell. Ensemble learning. Technical Report RN/11/02. Department
of Computer Science, UCL, London, 2008.
[92] Alexander V Shapeev. Moment Tensor Potentials: A Class of Systematically Improvable Interatomic Potentials. Multiscale Modeling & Simulation, 14(3):1153–1173, January 2016.
[93] A J Skinner and J Q Broughton. Neural networks in computational
materials science: Training algorithms. Modelling and Simulation in
Materials Science and Engineering, 1995.
[94] E Snelson, Z Ghahramani, and 2006. Sparse Gaussian processes using
pseudo-inputs. Nips.
[95] Sandro Sorella. Generalized Lanczos algorithm for variational quantum
Monte Carlo. Physical Review B, 64(2):973–16, June 2001.
[96] Sandro Sorella. Wave function optimization in the variational Monte
Carlo method. Physical Review B, 71(24):241103–4, June 2005.
[97] Frank H Stillinger and Thomas A Weber. Computer simulation of local
order in condensed phases of silicon. Physical review, B31(8):5262–5271,
1985.
[98] Qiming Sun, Timothy C Berkelbach, Nick S Blunt, George H Booth,
Sheng Guo, Zhendong Li, Junzi Liu, James D McClain, Elvira R Sayfutyarova, Sandeep Sharma, Sebastian Wouters, and Garnet Kin-Lic

136

BIBLIOGRAPHY
Chan. P ySCF: the Python-based simulations of chemistry framework. Wiley Interdisciplinary Reviews: Computational Molecular Science, 8(1):e1340–15, September 2017.
[99] Wojciech J Szlachta, A P Bartók, and Gábor Csányi. Accuracy and
transferability of Gaussian approximation potential models for tungsten.
Physical Review B, 90(10):104108–6, September 2014.
[100] Akira Takahashi, Atsuto Seko, and Isao Tanaka. Linearized machinelearning interatomic potentials for non-magnetic elemental metals: Limitation of pairwise descriptors and trend of predictive power. October
2017.
[101] J Tersoff. New empirical approach for the structure and energy of covalent systems. Physical Review B, 37(12):6991–7000, April 1988.
[102] A P Thompson, L P Swiler, C R Trott, S M Foiles, and G J Tucker. Spectral neighbor analysis method for automated generation of quantumaccurate interatomic potentials. Journal of Computational Physics,
285(C):316–330, March 2015.
[103] Michael E Tipping. The relevance vector machine. Nips.
[104] Julien Toulouse and C J Umrigar. Full optimization of Jastrow–Slater
wave functions with application to the first-row atoms and homonuclear
diatomic molecules. The Journal of Chemical Physics, 128(17):174101–
15, May 2008.
[105] Adri C T van Duin, Siddharth Dasgupta, Francois Lorant, and
William A Goddard. ReaxFF: A Reactive Force Field for Hydrocarbons.
The Journal of Physical Chemistry A, 105(41):9396–9409, October 2001.
[106] N G Van Kampen. Stochastic Processes in Physics and Chemistry; 3rd
ed. Elsevier, San Diego, CA, 2007.
[107] O A von Lilienfeld, Raghunathan Ramakrishnan, M Rupp, and Aaron
Knoll. Fourier series of atomic radial distribution functions: A molecular
fingerprint for machine learning models of quantum chemical properties.
International Journal of Quantum Chemistry, 115(16):1084–1093, April
2015.

137

BIBLIOGRAPHY
[108] Johann von Pezold, Liverios Lymperakis, and Jörg Neugebeauer.
Hydrogen-enhanced local plasticity at dilute bulk H concentrations: The
role of H-H interactions and the formation of local hydrides. Acta Materialia, 59(8):2969–2980, May 2011.
[109] S R White. Density-Matrix Formulation for Quantum RenormalizationGroups. Physical Review Letters, 69(19):2863–2866, 1992.
[110] C K I Williams and C E Rasmussen. Gaussian processes for machine
learning. the MIT Press, 2006.
[111] C Zeni, K Rossi, A Glielmo, Á Fekete, and 2018. Building machine
learning force fields for nanoclusters. aip.scitation.org, 148(24):241739,
June 2018.

138

